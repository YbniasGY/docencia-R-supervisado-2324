---
title: "Correlaciones"
subtitle: "Cuadernos pr치cticos de Aprendizaje Supervisado I del Grado en Ciencia de Datos Aplicada (curso 2023-2024)"
author: "Javier 츼lvarez Li칠bana"
format:
  html:
    theme: [default, style.scss]
    toc: true
    toc-title: 칈ndice
    toc-depth: 4
    toc-location: left
    number-sections: true
embed-resources: true
execute: 
  echo: true
---

## Intro a la modelizaci칩n lineal

El objetivo de la asignatura es ser capaces de construir [**modelos predictivos**]{.hl-yellow} para predecir una variable $Y$ num칠rica y continua (conocida como **variable objetivo** o **variable dependiente**), haciendo uso de un conjunto de predictoras $X_1,\ldots,X_p$ (conocidas como **covariables** o **variables independientes**) tal que

$$Y = f\left( X_1,\ldots,X_p \right) + \varepsilon$$

Concretamente, dado que la asignatura gira en torno a [**modelos lineales**]{.hl-yellow}, esa funci칩n $f()$ que relacione las predictoras con la variable objetivo tendr치 que ser una funci칩n lineal. En matem치ticas decimos que una funci칩n $f(x)$ es **lineal** cuando se cumple:

* [**Propiedad aditiva**]{.hl-yellow}: $f(x + y) = f(x) + f(y)$

* [**Propiedad homog칠nea**]{.hl-yellow}: $f(k*x) = k*f(x)$ (donde $k$ es una constante en $\mathbb{R}$).

Ambas se pueden resumir en $f(a*x + b*y) = a*f(x) + b*f(y)$

&nbsp;

- [**Ejemplos lineales**]{.hl-green}: $y = 2*x_1 - 3$ o  $y = 4 - \frac{x_1}{2} + 3*x_2$

- [**Ejemplos no lineales**]{.hl-red}: $y = 2*\frac{1}{x_1}$ o  $y = 4 - x_{1}^{2} - x_2$ o $y = ln(x_1) + cos(x_2)$

&nbsp;

As칤, nuestra [**regresi칩n lineal**]{.hl-yellow} ser치:

$$Y = \beta_0 + \beta_1 X_1 + \beta_p X_p + \varepsilon, \quad E[\varepsilon] = 0$$
lo que traduce en

$$E[Y|X = x] = \beta_0 + \beta_1 x_1 + \beta_p x_p$$


&nbsp;

La idea es encontrar el **춺mejor췉 modelo posible** atendiendo a diferentes aspectos

* 쮺u치nto se **equivoca**? (conocemos el error ya que es **aprendizaje supervisado**: de mi conjunto con el que entreno el modelo, conozco los valores reales de $Y$. 游닄 Ver 춺The elements of Statistical Learning췉 Hastie et al., 2008)

* 쮺칩mo de **complicado** es el modelo? 쮼xiste alguno m치s sencillo (equivoc치ndose similar)?

* Am칠n de lo que se equivoca, 쯖칩mo **var칤an dichas equivocaciones**? 쯈u칠 varianza tienen mis estimaciones?

* 쯈u칠 % de informaci칩n estoy capturando?

&nbsp;

Ese modelo lo podemos reducir a encontrar los 춺mejores췉 estimadores $\left( \hat{\beta}_0, \ldots, \hat{\beta}_p \right)$ 

$$\widehat{E[Y|X = x]} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \ldots + \hat{\beta}_p x_p$$

As칤, nuestro [**error estimado**]{.hl-yellow} se podr치 calcular como

$$\hat{\varepsilon}= Y - \hat{Y}, \quad E[\hat{\varepsilon}] = 0$$

### Caso particular: regresi칩n univariante


El [**caso univariante**]{.hl-yellow} es el m치s sencillo y es aquel en el que solo tenemos una variable predictora $X_1 :=  X$, tal que

$$Y = \beta_0 + \beta_1 X_1 + \varepsilon := \beta_0 + \beta_1 X + \varepsilon$$

&nbsp;

El objetivo en este primera aproximaci칩n es, dado un dataset con una variable objetivo y m칰ltiples predictoras, ser capaces de decidir la [**predictora cuyo efecto lineal en $Y$ sea m치s relevante**]{.hl-yellow}. Y para ello debemos saber un concepto fundamental estad칤stica: la [**covarianza**]{.hl-yellow} y la [**correlaci칩n**]{.hl-yellow}.


## Covarianza

### Repaso te칩rico


Es habitual en estad칤stica hablar de **medidas de centralizaci칩n** (media, moda, mediana) y de **medidas de dispersi칩n**, que cuantifican c칩mo var칤an los datos y en torno a qu칠 valor lo hacen.

&nbsp;

La medida de dispersi칩n m치s famosa es la quiz치s la varianza. [**쯈u칠 es en realidad la varianza?**]{.hl-yellow}


$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 \geq 0$$

&nbsp;

Si atendemos simplemente a la f칩rmula y su definici칩n, la [**varianza es el promedio de las desviaciones al cuadrado**]{.hl-yellow} (respecto a la media), lo que nos permite cuantificar de alguna manera la [**relaci칩n de una variable CONSIGO MISMA**]{.hl-yellow}. Si te fijas en la f칩rmula, podr칤amos separar los cuadrados de la siguiente manera:

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(x_i - \overline{x} \right) = \overline{x*x} - \overline{x}*\overline{x} \geq 0$$

&nbsp;

쯏 si qui칠semos medir la  [**relaci칩n de una variable X respecto a otra variable (en lugar de consigo misma)**]{.hl-yellow}?


&nbsp;

Esta es la base del concepto de la [**covarianza**]{.hl-yellow}: sustituir en la f칩rmula anterior una de las desviaciones de x por las desviaciones respecto a OTRA variable


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{n} \left(x_i - \overline{x} \right)\left(y_j - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

#### Propiedades de la covarianza

Es importante entender algunas [**propiedades de la covarianza**]{.hl-yellow}

&nbsp;

* [**Signo**]{.hl-yellow}: la [**covarianza puede ser tanto positiva como negativa como 0**]{.hl-green}: al eliminar el cuadrado de la varianza, ya no es necesario que sea positiva

* [**쯈u칠 cuantifica?**]{.hl-yellow} La covarianza [**SOLO mide la relaci칩n LINEAL**]{.hl-green} (en torno a una recta) entre dos variables, pero [**no captura relaciones no lineales**]{.hl-red}

&nbsp;

* [**쯈u칠 dice su signo?**]{.hl-yellow} El signo de la covarianza nos indicar치 la [**direcci칩n de la dependencia lineal**]{.hl-yellow}: si es positiva, la relaci칩n ser치 creciente (cuando X crece, Y crece); si es negativa, la relaci칩n ser치 decreciente (cuando X crece, Y decrece)


* [**쯈u칠 dice su magnitud?**]{.hl-yellow}  Al igual que pasa con la varianza, la **covarianza depende de las unidades y magnitudes** de los datos, as칤 que no podemos comparar covarianzas de variables y datasets distintos: si me dicen que la covarianza es de, por ejemplo, 5.5, no sabemos si es mucho o poco.


### Herramientas en R

Para calcular la covarianza entre dos variables en `R` la forma m치s sencilla es usar la funci칩n `cov()`.

::: callout-important
## Importante

Recuerda que los softwares estad칤sticos nos devuelven siempre la [**cuasi covarianza**]{.hl-yellow}, dividido entre $n-1$ y no entre $n$. La cuasivarianza y la cuasicovarianza son los [**mejores estimadores muestrales (insesgados)**]{.hl-yellow} de los respectivos par치metros poblaciones

:::

#### Simulaci칩n: variables sin relaci칩n

Por ejemplo, vamos a simular dos variables $X \sim N(0, 0.5)$ e $Y \sim N(0, 0.5)$ que no tienen ning칰n tipo de relaci칩n (ni lineal ni no lineal entre ellas)

```{r}
set.seed(12345)
x <- rnorm(n = 1e6, mean = 0, sd = 0.5)
y <- rnorm(n = 1e6, mean = 0, sd = 0.5)
cov(x, y)
```

Vemos como efectivamente la **covarianza es pr치cticamente cero**, 쯣ero c칩mo cercano a 0 ser칤a?

#### Simulaci칩n: variables sin relaci칩n lineal

쯏 si simulamos una variable $X$ de manera aleatoria y otra variable $Y$ tal que $Y = X^2$?

```{r}
set.seed(12345)
x <- rnorm(n = 1e6, mean = 0, sd = 0.5)
y <- x^2
cov(x, y)
```

Aunque haya una **relaci칩n de dependencia perfecta** entre X e Y (una funci칩n que las relaciona), [**춰la covarianza puede que sea incluso m치s peque침a que antes (en valor absoluto)!**]{.hl-yellow}.

Esto se debe a que la [**covarianza/correlaci칩n no captura relaciones no lineales**]{.hl-red}, as칤 que para saber si son independientes realmente deber칤amos de pasar un **contraste de independencia**. Para ello podemos hacer uso de `chisq.test()` (contraste no par치metrico).

&nbsp;

Esto lo podemos ilustrar con ejemplo m치s sencillo:

* $X = -1, 0, 1$ cuya media es $\overline{x} = 0$
* $Y = X^2 = 1, 0, 1$ cuya media es $\overline{y} = \frac{2}{3}$
* $XY = X*X^2 = X^3 = -1, 0, 1$ cuya media es $\overline{xy} = 0$

Si hacemos la covarianza

$$s_{xy} = \overline{xy} - \overline{x}{y} = 0 - 0 \frac{2}{3} = 0$$

[**Moraleja**]{.hl-yellow}: covarianza nula [**NO IMPLICA**]{.hl-red} independencia (independencia si implica, en particular, ausencia de relaci칩n lineal)

![](img/escenarios-covarianza.png) 

#### Simulaci칩n: variables con relaci칩n lineal

쯏 si simulamos dos variables con relaci칩n lineal?


```{r}
set.seed(12345)
x <- rnorm(n = 1e6, mean = 0, sd = 0.5)
y <- 2*x
cov(x, y)
```

La covarianza en este caos es `r round(cov(x, y), 5)` y la pregunta es...쯘s mucho o poco? Eso no podemos resolverlo hasta hablar de **correlaci칩n**: de momento solo podemos decir que, en caso de existir, la relaci칩n lineal ser치 **positiva**. Esto cambiar칤a si la hubi칠ramos definido $Y$ de otra manera, por ejemplo `y <- -2*x`

```{r}
y <- -2*x
cov(x, y)
```

### Matriz de covarianzas

Antes de hablar de correlaci칩n, vamos a introducir una **generalizaci칩n** de una simple covarianza, y es la conocida como [**matriz de covarianzas**]{.hl-yellow}, y que jugar치 un papel fundamental en estad칤stica ya que resume un conjunto de datos multivariantes.

&nbsp;

La idea de dicha matriz que llamaremos $\Sigma$ es, que si en lugar de tener una sola predictora tenemos $\left(X_1, \ldots, X_p \right)$ covariables, cada elemento $\Sigma_{i,j}$ se defina como 

$$\Sigma = \left(\Sigma_{i,j} \right)_{i=1,\ldots,p}^{j=1,\ldots,p} = \begin{pmatrix} s_{x_1 x_1} = s_{x_1}^2 & s_{x_1 x_2} & \ldots & s_{x_1 x_p} \\ s_{x_2 x_1} & s_{x_2 x_2} = s_{x_2}^2 & \ldots & s_{x_2 x_p} \\ \vdots & \vdots & \ddots & \vdots \\ s_{x_p x_1} & s_{x_p x_2} & \ldots & s_{x_p x_p} = s_{x_p}^2 \end{pmatrix}$$

En el futuro veremos como los **autovalores y autovectores de esta matriz** ser치n fundamentales. Para calcularla en `R` basta con pasarle `cov()` a un dataset (춰importante, solo num칠ricas!)

```{r}
#| message: false
#| warning: false
library(tidyverse)
starwars |>
  select(where(is.numeric)) |> # importante: solo num칠ricas
  drop_na() |> # importante: covarianza de ausentes --> ausente
  cov()
```

### Datos agrupados

Un peque침o matiz en caso de que tuvi칠semos que hacer el [**c치lculo a mano**]{.hl-yellow}: si tenemos una base de datos agrupados (no tenemos el dato en bruto sino los pares de datos 칰nicos $(x_i, y_j)$ y su frecuencia $n_{ij}$), la matriz anterior se calcula como

$$\Sigma = \left(\Sigma_{i,j} \right)_{i=1,\ldots,p}^{j=1,\ldots,p}, \quad \Sigma_{i,j} = s_{x_i x_j} = \frac{1}{n} \sum_{i=1}^{k_i} \sum_{j=1}^{k_j} n_{ij} \left(x_i - \overline{x} \right)\left(y_j - \overline{y} \right)$$


## Correlaci칩n

### Repaso te칩rico

Como dec칤amos, al igual que pasaba con la varianza, la [**covarianza depende de las unidades y magnitudes**]{.hl-yellow} de los datos, as칤 que lo que haremos ser치 [**estandarizar la covarianza**]{.hl-yellow}.

Definiremos el [**coeficiente correlaci칩n lineal (de Pearson)**]{.hl-yellow} como la covarianza dividida entre el producto de las desviaciones t칤picas (algo adimensional)


$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y} = \frac{\overline{x*y} - \overline{x} * \overline{y}}{\sqrt{\overline{x^2} - \overline{x}^2}\sqrt{\overline{y^2} - \overline{y}^2}}$$

#### Propiedades de la correlaci칩n

Es importante entender algunas [**propiedades de la correlaci칩n**]{.hl-yellow}



* [**Signo**]{.hl-yellow}: mismo signo que la covarianza ya que el denominador es siempre positivo.

* [**쯈u칠 cuantifica?**]{.hl-yellow}: sus [**valores siempre est치n entre -1 y 1**]{.hl-yellow} por lo que nos sirve como una **escala de la fortaleza de una relaci칩n lineal**. Al igual qe con la covarianza [**no captura relaciones no lineales**]{.hl-red}

&nbsp;

* [**쯈u칠 dice su signo?**]{.hl-yellow} El signo de la correlaci칩n nos indicar치 la [**direcci칩n de la dependencia lineal**]{.hl-yellow}: si es positiva, la relaci칩n ser치 creciente (cuando X crece, Y crece); si es negativa, la relaci칩n ser치 decreciente (cuando X crece, Y decrece)


* [**쯈u칠 dice su magnitud?**]{.hl-yellow}  M치s cerca de -1 o 1 implica (a priori...) una relaci칩n m치s fuerte, m치s cerca de 0 ausencia (a priori...) de relaci칩n lineal


![](img/correlaciones.jpg)


### Herramientas en R

Para calcular la correlaci칩n entre dos variables en `R` la forma m치s sencilla es usar la funci칩n `cor()`.

::: callout-important
## Importante

Aunque los softwares estad칤sticos nos devuelven siempre la [**cuasi covarianza**]{.hl-yellow}, dividido entre $n-1$ y no entre $n$, esto no afecta la correlaci칩n (ya que los denominadores se cancelan)

:::

&nbsp;

Como hemos hecho antes con la covarianza, podemos tener una **generalizaci칩n** de una simple correlaci칩n, y es la conocida como [**matriz de correlaciones**]{.hl-yellow}, y que jugar치 tambi칠n un papel fundamental en estad칤stica ya que resume un conjunto de datos multivariantes (estandrizados).

&nbsp;

La idea de dicha matriz que llamaremos $R$ es, que si en lugar de tener una sola predictora tenemos $\left(X_1, \ldots, X_p \right)$ covariables, cada elemento $r_{i,j}$ se defina como 

$$R = \left(r_{i,j} \right)_{i=1,\ldots,p}^{j=1,\ldots,p} = \begin{pmatrix} 1 & r_{x_1 x_2} & \ldots & r_{x_1 x_p} \\ r_{x_2 x_1} & 1 & \ldots & r_{x_2 x_p} \\ \vdots & \vdots & \ddots & \vdots \\ r_{x_p x_1} & r_{x_p x_2} & \ldots & 1 \end{pmatrix}$$

F칤jate que ahora, adem치s de ser sim칠trica como la matriz de covarianzas, la diagonal es siempre 1 (la correlaci칩n entre una variable consigo misma es la m치xima posible). Para calcularla en `R` basta con pasarle `cor()` a un dataset (춰importante, solo num칠ricas!)

```{r}
#| message: false
#| warning: false
library(tidyverse)
starwars |>
  select(where(is.numeric)) |> # importante: solo num칠ricas
  drop_na() |> # importante: correlaci칩n de ausentes --> ausente
  cor()
```

&nbsp;

Una propiedad importante que relaciona ambas matrices. Si tenemos los [**datos estandarizados por media y varianza**]{.hl-yellow}, la [**matriz de covarianzas coincide con la de correlaciones**]{.hl-yellow}

$$R\left(X_1, \ldots, X_p \right) = \Sigma \left(\frac{X_1 - \overline{X_1}}{s_{X_1}}, \ldots, \frac{X_p - \overline{X_p}}{s_{X_p}} \right)$$

La [**matriz de correlaciones no es m치s que la matriz de covarianzas cuando los datos est치n estandarizados**]{.hl-yellow}

```{r}
#| message: false
#| warning: false
library(tidyverse)

# Correlaci칩n
starwars |>
  select(where(is.numeric)) |>
  drop_na() |>
  cor()

# Covarianza con datos estandarizados
starwars |>
  select(where(is.numeric)) |>
  drop_na() |>
  # Estandarizamos
  mutate(across(everything(), function(x) { (x - mean(x))/sd(x) })) |> 
  cov()
```

Efectivamente es la misma matriz.

#### Herramientas extras: paquete corrr y paquete corrplot

Para poder hacer un mejor uso de las correlaciones contamos con el paquete `{corrr}` que nos proporciona la matriz de correlaciones en un tibble propio, 
pudiendo realizar una **profundo an치lisis exploratorio** de la misma. La funci칩n m치s importante es `correlate()` (el equivalente a `cor()`), que vamos a aplicar al conjunto `mtcars` del paquete `{datasets}`

```{r}
library(corrr)
mtcars |> 
  correlate()
```

[**쯈u칠 diferencias hay?**]{.hl-yellow}

1. La salida es a su vez un tibble por lo que podemos [**filtrar de manera muy sencilla**]{.hl-yellow} qued치ndonos solo con la informaci칩n que nos interesa

```{r}
# Filtramos solo las correlaciones respecto a 3 variables
mtcars |> 
  correlate() |> 
  filter(term %in% c("mpg", "disp", "gear"))

# Si queremos predecir mpg, filtramos aquellas variables que tienen una correlaci칩n (en valor absoluto) superior a 0.7
mtcars |> 
  correlate() |> 
  filter(abs(mpg) > 0.7)
```

2. Los [**valores negativos vienen marcados en rojo**]{.hl-yellow}

3. Podemos especificar que valor queremos en la **diagonal**

```{r}
mtcars |> 
  correlate(diag = 1)
```

Adem치s contamos con algunas funciones que nos pueden ayudar:

* `focus()`: realiza un **filtro espec칤fico de las variables de inter칠s** (cuidado: hay otra funci칩n con el mismo nombre del paquete `{skimr}`)

```{r}
mtcars |> 
  correlate() |> 
  corrr::focus(mpg, cyl)
```

Podemos indicar que **variables queremos quitar** con un signo negativo sobre ellas y `mirror = TRUE`

```{r}
mtcars |> 
  correlate() |> 
  corrr::focus(-(mpg:drat), mirror = TRUE)
```

* `rearrange()`: nos permite [**ordenar por correlaci칩n**]{.hl-yellow}, de manera que nos agrupa las variables m치s altamente correladas juntas


```{r}
mtcars |> 
  correlate() |>
  rearrange(absolute = TRUE)
```

* `shave()`: nos muestra solo la **diagonal inferior**

```{r}
mtcars |> 
  correlate() |>
  rearrange(absolute = TRUE) |> 
  shave()
```

* `fashion()`: nos ofrece un **resumen m치s limpio y resumido**

```{r}
mtcars |> 
  correlate() |>
  shave() |> 
  fashion()
```

* `network_plot()`: nos permite [**visualizar las relaciones lineales**]{.hl-yellow} mediante un grafo (podemos decidir el umbral de correlaci칩n con `min_cor = ...`)

```{r}
mtcars |> 
  correlate() |>
  network_plot(min_cor = 0.7)
```

&nbsp;

Adem치s del paquete `{corrr}` podemos hacer uso de `{corrrplot}` para una mejor [**visualizaci칩n**]{.hl-yellow} (importante: necesita la matriz de correlaciones sin formato)

```{r}
library(corrplot)
mtcars |> 
  cor() |> 
  corrplot()
```
Dando valores distintos al argumento `method = ...` podemos obtener visualizaciones diferentes.


```{r}
mtcars |> 
  cor() |> 
  corrplot(method = "color")
```

```{r}
mtcars |> 
  cor() |> 
  corrplot(method = "number")
```

```{r}
mtcars |> 
  cor() |> 
  corrplot(method = "ellipse")
```

Podemos hacer uso tambi칠n del argumento `type = ...` para visualizar solo una mitad de la matriz (sim칠trica)

```{r}
mtcars |> 
  cor() |> 
  corrplot(method = "ellipse")
```


Incluso podemos elegir la paleta de colores con `col = ...` y las opciones del paquete `{RColorBrewer}` para construir paletas



```{r}
library(RColorBrewer)
mtcars |> 
  cor() |> 
  corrplot(type="upper", col = brewer.pal(n = 8, name = "PuOr"))
```

Podemos incluso realizar un [**contraste de hip칩tesis**]{.hl-yellow} cuya **hip칩tesis nula es que la correlaci칩n es nula**

```{r}
cor_test <-
  mtcars |> 
  cor.mtest()
cor_test$p # matriz de p-valores
```

Esa **matriz de p-valores** podemos incluirla en `corrplot()` para que directamente nos [**marque las variables cuya hip칩tesis nula de incorrelaci칩n no puede rechazarse**]{.hl-yellow}

```{r}
mtcars |>
  cor() |> 
  corrplot(method = "color", p.mat = cor_test$p, sig.level = 0.05) 
```


         
#### Simulaci칩n: variables sin relaci칩n

Vamos a volver a simular las dos variables $X \sim N(0, 0.5)$ e $Y \sim N(0, 0.5)$ que no tienen ning칰n tipo de relaci칩n (ni lineal ni no lineal entre ellas)

```{r}
set.seed(12345)
x <- rnorm(n = 1e6, mean = 0, sd = 0.5)
y <- rnorm(n = 1e6, mean = 0, sd = 0.5)
cor(x, y)
```

Ahora s칤 que pdoemos decir que, efectivamente, la **relaci칩n lineal es inexistente** ya que la **correlaci칩n es pr치cticamente nula**



#### Simulaci칩n: variables sin relaci칩n lineal

Simulamos de nuevo una variable $X$ de manera aleatoria y otra variable $Y$ tal que $Y = X^2$

```{r}
set.seed(12345)
x <- rnorm(n = 1e6, mean = 0, sd = 0.5)
y <- x^2
cor(x, y)
```

Aunque haya una **relaci칩n de dependencia perfecta** entre X e Y (una funci칩n que las relaciona), [**la correlaci칩n vuelve a ser pr치cticamente nula**]{.hl-yellow}.


#### Simulaci칩n: variables con relaci칩n lineal

Y vamos a volver a simular dos variables con relaci칩n lineal


```{r}
set.seed(12345)
x <- rnorm(n = 1e6, mean = 0, sd = 0.5)
y <- 2*x
cor(x, y)
```

La covarianza en este caso es `1`, ya que tenemos una **relaci칩n lineal perfecta** dado que ambas variables vienen relacionadas por una f칩rmula

#### Simulaci칩n con ruido

쯏 si a la anterior relaci칩n lineal le incorporamos **ruido**, con una varianza cada vez m치s grande?

```{r}
set.seed(12345)
x <- rnorm(n = 1e5, mean = 0, sd = 0.5)
sigmas <- seq(0.0001, 10, l = 100)
correlations <- tibble("sigmas" = sigmas, "cor" = NA)

for (i in 1:length(sigmas)) {
  
  # vamos a침adiendo ruido y calculamos la correlaci칩n
  y <- 2*x + rnorm(n = 1e5, mean = 0, sd = sigmas[i])
  correlations$cor[i] <- cor(x, y)
  
}

ggplot(correlations, aes(x = sigmas, y = cor)) +
  geom_point(aes(color = sigmas), size = 3, alpha = 0.7) +
  labs(title = "Correlaci칩n vs ruido",
       x = "sigma", y = "correlaci칩n",
       color = "sd ruido") +
  theme_minimal()
```

La **correlaci칩n captura la relaci칩n lineal** pero si entre ambas variables hay dem치s algo aleatorio que no podemos capturar, seg칰n [**aumente la varianza del ruido, la correlaci칩n ser치 m치s d칠bil**]{.hl-yellow} ya que perturbar치 la dependencia entre ellas.

## Correlaci칩n vs causalidad


Por 칰ltimo, vamos a mencionar un matiz importante: la idea de [**causalidad**]{.hl-yellow}


![](img/helados-moreno.jpg)

Imagina que $X$ e $Y$ fuesen variables dependientes (lineales o no) como en la imagen...[**쯜mplicar칤a que el ascenso/descenso de una PROVOCA el ascenso/descenso de la otra?**]{.hl-yellow}

En el caso de la imagen el nivel de bronceado (X) y el consumo de helados (Y). parece que son dependientes? Pero...쯧na causa la otra?

&nbsp;

Diremos que dos variables tienen una [**relaci칩n causal o de causalidad**]{.hl-yellow} cuando una variable es la CAUSA del comportamiento de la otra, algo que no sabremos solo con estad칤stica (sino con conocimiento experto, en este caso de nutricionistas y m칠dicos). Y es que, mal que nos pese a veces a los matem치ticos/estad칤sticos/cient칤ficos de datos, [**correlaci칩n NO IMPLICA causalidad**]{.hl-red} (al menos no tiene por qu칠) 

&nbsp;

**Otro ejemplo** podemos verlo en la siguiente gr치fica que, aparentemente, nos dice que comer chocolate nos har치 ganar el Nobel. Ojal치 :P

![](img/chocolate-correlacion.jpg)


Este fen칩meno es conocido como [**correlaciones esp칰reas**]{.hl-yellow}, y aparece cuando dos variables presentan una [**relaci칩n matem치tica**]{.hl-green} pero sin [**ning칰n tipo de relaci칩n causal o l칩gica**]{.hl-red}. Puedes ver m치s en <https://www.tylervigen.com/spurious-correlations>

&nbsp;

Dicho patr칩n matem치tico puede deberse al mero azar o la existencia de lo que se conoce como [**variables de confusi칩n**]{.hl-yellow} (en el caso del helado, el verano es el que causa ambas).

![](img/confounder.jpg)

La rama que se dedica al [**estudio de la causalidad**]{.hl-yellow}, combinando las ramas de la **estad칤stica, filosof칤a, sociolog칤a y psicolog칤a** se le conoce como [**inferencia causal**]{.hl-yellow} y es fundamental en un an치lisis m치s profundo de las relaciones entre las variables (sobre todo en campos como la econom칤a, la psicolog칤a o la sociolog칤a).

::: callout-warning
## Recuerda

As칤 que recuerda: un buen estad칤stico/a o cient칤fico/a de datos debe ser **humilde** en sus atribuciones. Nuestro trabajo llega hasta encontrar patrones. **Deja a otros expertos** que sean los que, con su conocimiento de la mater칤a, atribuyan causas y consecuencias.

:::
## Caso pr치ctico I: anscombe


En el paquete `{datasets}` se encuentra el dataset conocido como [**cuarteto de Anscombe**]{.hl-yellow}, un dataset que cuenta en realidad con 4 conjuntos de datos.

```{r}
library(tidyverse)
anscombe_tb <- as_tibble(datasets::anscombe)
anscombe_tb
```

Para su correcto manejo vamos a **convertirlo a tidydata**

```{r}
anscombe_x <- 
  anscombe_tb |>
  pivot_longer(cols = x1:x4, names_to = "dataset", values_to = "x") |> 
  select(-contains("y")) |> 
  mutate(dataset = str_remove_all(dataset, "x"))

anscombe_y <- 
  anscombe_tb |>
  pivot_longer(cols = y1:y4, names_to = "dataset", values_to = "y") |> 
  select(-contains("x"), -dataset)

anscombe_tidy <-
  anscombe_x |> mutate("y" = anscombe_y$y)
anscombe_tidy
```

[**쯈u칠 caracter칤sticas muestrales tienen?**]{.hl-yellow} Para conocerlos un poco mejor vamos a **calcular** la media, varianza, desv. t칤pica, covarianza y correlaci칩n en cada dataset

```{r}
anscombe_tidy |> 
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

Si te fijas todos los datasets tienen los [**mismos momentos**]{.hl-yellow} muestrales, y en concreto **tienen la misma correlaci칩n** y **misma covarianza**.

[**쯉er치n el mismo dataset desordenado?**]{.hl-yellow}


```{r}
ggplot(anscombe_tidy) +
  geom_point(aes(x = x, y = y, color = dataset), size = 3, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~dataset) +
  theme_minimal()
```


Por suerte o por desgracia [**no todo son matem치ticas**]{.hl-red}: antes de pensar que modelo es mejor para nuestros datos, es important칤simo realizar un [**an치lisis exploratorio**]{.hl-yellow} de los datos (incluyendo, por supuesto, una correcta visualizaci칩n).

## Caso pr치ctico II: datasauRus

Podemos visualizarlo de manera a칰n m치s extrema con el dataset `datasaurus_dozen` del paquete `{datasauRus}` (ver m치s en <https://www.research.autodesk.com/publications/same-stats-different-graphs/>)

```{r}
library(datasauRus)
datasaurus_dozen |>
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

De nuevo tenemos la **misma correlaci칩n** pero...쯦endr치n la misma relaci칩n lineal de verdad?

```{r}
ggplot(datasaurus_dozen |>
         filter(dataset != "wide_lines"),
       aes(x = x, y = y, color = dataset)) +
  geom_point(size = 2.5, alpha = 0.75) +
  geom_smooth(method = "lm", se = FALSE)  +
  facet_wrap(~dataset, ncol = 3) +
  theme_minimal()
```

::: callout-tip
## Moraleja

Los res칰menes num칠ricos son eso, res칰menes, y como veremos m치s adelante, pueden existir distintos factores que enmascaran la verdadera relaci칩n de las variables. Por eso saber visualizar de una manera correcta es vital.
:::

## Caso pr치ctico III: wine.csv


Vamos a poner en pr치ctica lo aprendido con el dataset `wine.csv` disponible en el campus.

```{r}
datos <- read_csv(file = "./datos/wine.csv")
datos
```

El conjunto de datos est치 formado por 27 observaciones (cosechas de vino rojo Burdeos) y **7 variables**

* `Year`, `Age`: a침o de la cosecha y n칰mero de a침os en barrica.
* `Price`: precio en 1990-1991 de cada cosecha (en **escala log**)
* `WinterRain`: lluvia (en mm) que cay칩 ese a침o en invierno.
* `AGST`: crecimiento medio de la temperatura (en grados Celsius) durante la temporada.
* `HarvestRain`: lluvia (en mm) que cay칩 ese a침o durante la cosecha.
* `FrancePop`: poblaci칩n (miles de habitantes) de Francia.

Puedes ver m치s en detalles en <https://doi.org/10.1080/09332480.1995.10542468>

&nbsp;


Si el objetivo fuese [**predecir el precio**]{.hl-yellow}, $Y = precio$, con una sola predictora $X$, 쯖u치l tendr칤amos que elegir? Si el modelo lineal, parece ya claro que deberemos elegir a priori aquella con [**mayor correlaci칩n lineal (en valor absoluto)**]{.hl-yellow}. Pero como hemos visto con los ejemplos, a veces los res칰menes num칠ricos pueden enga침arnos as칤 que lo primero que haremos ser치 realizar un **an치lisis exploratorio**.

### An치lisis exploratorio

Algunas de las primeras preguntas que podemos hacernos son:

- 쯃as variables son [**num칠ricas (continuas)**]{.hl-yellow}?
- 쯊ienen [**problemas de rango**]{.hl-yellow} (por ejemplo, pesos negativos)? 쯊ienen datos ausentes?
  
&nbsp;

Para responder a dichas preguntas lo m치s sencillos es hacer uso de la funci칩n `skim()` del paquete `{skimr}`

```{r}
library(skimr)
datos |> skim()
```

En este caso [**no tenemos ausentes ni problemas de codificaci칩n**]{.hl-green}


&nbsp;

Profundizando un poco podemos preguntarnos...

- 쮺칩mo se [**distribuyen las variables**]{.hl-yellow}?
- 쮿ay [**datos at칤picos**]{.hl-yellow}?


&nbsp;

Para ello podemos empezar [**visualizando la distribuci칩n de cada variable**]{.hl-yellow} haciendo uso de densidades, histogramas y/o boxplots (por ejemplo), pero si queremos ahorrar tiempo en visualizar convertimos nuestro dataset en [**tidydata**]{.hl-yellow}

```{r}
datos_tidy <-
  datos |> 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "values")

ggplot(datos_tidy, aes(x = values)) +
  geom_density(aes(color = variable, fill = variable),
               alpha = 0.75) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

```{r}
ggplot(datos_tidy, aes(x = values)) +
  geom_histogram(aes(color = variable, fill = variable),
               alpha = 0.75, bins = 10) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```


```{r}
ggplot(datos_tidy, aes(y = values, x = variable)) +
  geom_boxplot(aes(color = variable, fill = variable),
               alpha = 0.75) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

[**No se aprecian valores at칤picos**]{.hl-green} (al menos respecto a los percentiles)

&nbsp;

Podemos incluso gr치ficar todos los scatter plot sin transformar los datos con `facet_matrix()` del paquete `{ggforce}`

```{r}
#| code-fold: true
library(ggforce)
ggplot(datos) +
  geom_point(aes(x = .panel_x, y = .panel_y)) +
  facet_matrix(vars(everything())) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  theme_minimal()
```

Podemos tambi칠n visualizar con un [**scatter plot**]{.hl-yellow} todas las variables y adem치s sus correlaciones, con el paquete `{GGally}` y la funci칩n `ggpairs(), sin necesidad de convertir a tidydata.

```{r}
#| code-fold: true
library(GGally)
ggpairs(datos) +
  theme_minimal()
```
   

### An치lisis de dependencia


Tras una primera visualizaci칩n de los datos, de cada variable por separado, nuestro objetivo real ser칤a responder a

- 쯈u칠 [**predictora est치 m치s correlacionada (linealmente) con la variable objetivo**]{.hl-yellow} a predecir? 쮼xiste otro tipo de dependencia ?
- 쮺칩mo se [**relacionan las predictoras**]{.hl-purple} entre s칤? 쮼st치n correlacionadas? Este 칰ltimo paso ser치 crucial en el [**contexto multivariante**]{.hl-yellow} pero en este caso simplemente vamos a ver como se relacionan linealmente las predictoras entre s칤 (por saber como se har칤a), y cu치l de ellas es la **m치s adecuada para predecir linealmente** `precio`

&nbsp;

El primer paso es la [**matriz de correlaciones**]{.hl-yellow} con la funci칩n `correlate()` del paquete `{corrr}`

```{r}
datos |>
  correlate() |>
  fashion()
```

Podemos **visualizar dichas correlaciones con un grafo** con `network_plot()`

```{r}
#| eval: false
datos |>
  correlate() |>
  network_plot(min_cor = 0.5)
```


En cuando a la [**dependencia entre predictoras**]{.hl-yellow}, las variables `Age`, `Year` y `FrancePop` presentan la misma informaci칩n. Si queremos centrarnos solo en las variables vs la predictora, podemos hacer `focus()` en ella

```{r}
datos |>
  correlate() |>
  corrr::focus(Price)
```

[**Respecto a Y**]{.hl-yellow}, las predictoras con mayor correlaci칩n lineal son `AGST` (m치s calor, menos cosechas, sube el precio) y `HarvestRain` (m치s lluvias, m치s cosechas, baja el precio, 춰el signo importa!)

&nbsp;

Tambi칠n podemos usar `corrplot()` del paquete `{corrplot}`, al que le pasamos una matriz de correlaciones cl치sica y nos la [**visualiza**]{.hl-yellow}.

```{r}
library(corrplot)
datos |>
  cor() |>
  corrplot(method = "ellipse")
```

&nbsp;

Otra opci칩n es [**visualizar con un scatter plots todas las predictoras vs Y**]{.hl-yellow}, pivotando antes nuestro dataset (solo pivotamos las predictoras)

```{r}
datos_tidy <-
  datos |> pivot_longer(cols = -Price, names_to = "variable",
                        values_to = "values")
ggplot(datos_tidy, aes(x = values, y = Price)) + 
  geom_point(aes(color = variable), alpha = 0.7) +
  geom_smooth(method = "lm") +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal()
```


No solo comprobamos que las rectas con m치s pendiente son `AGST` y `HarvestRain`, adem치s los puntos [**parecen poder ajustarse a una recta sin otro patr칩n identificable**]{.hl-green}. Esto es importante hacerlo ya que debemos [**descartar posibles correlaciones esp칰reas**]{.hl-red} (ver ejemplo datasaurus)

&nbsp;

Por lo tanto parece claro: la mejor covariable para predecir linealmente el precio ser칤a `AGST` (en sentido positivo) y `HarvestRain` (en sentido negativo)

