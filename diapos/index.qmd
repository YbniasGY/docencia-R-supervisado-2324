---
title: "Aprendizaje Supervisado I"
subtitle: "Métodos de predicción lineal"
title-slide-attributes:
  data-background-image: img/data-science-2.jpeg
  data-background-size: cover
  data-background-opacity: "0.2"
author: "Grado en Ciencia de Datos Aplicada • curso 2023-2024"
affiliation: Facultad de Estudios Estadísticos (UCM)
lang: es
language: custom_lang.yml
format: 
  revealjs:
    theme: [default, style.scss]
    chalkboard: true
    multiplex: true
    menu:
      side: left
      width: normal
    footer: "[<strong>Javier Álvarez Liébana</strong>](...) • Grado en Ciencia de Datos Aplicada (UCM) • curso 2023-2024"
    slide-number: c/t
execute:
  echo: true
---

# Empieza lo bueno...empieza la modelización

[**Vamos a juntar las piezas del puzzle para hacer «magia»**]{style="color:#444442;"}

```{r}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
```

---

## ¡Buenas!

[**Correo**]{.hl-green}: **<javalv09@ucm.es>**. [**Despacho**]{.hl-green}: 722 (3ª planta). [**Tutorías**]{.hl-green}: lunes (14:30-16:00), martes (13:00-15:00) y viernes (13:00-14:00).

::: columns
::: {.column width="30%"}
![](img/me.jpeg)
:::

::: {.column width="70%"}
::: incremental
-   [**Javier Álvarez Liébana**]{.hl-yellow}, de Carabanchel (Bajo).

-   Licenciado en Matemáticas (UCM). [**Doctorado en estadística**]{.hl-yellow} (UGR).

-   Encargado de la [**visualización y análisis de datos covid**]{.hl-yellow} del Principado de Asturias (2021-2022).

-   Miembro de la [**Sociedad Española de Estadística e IO**]{.hl-yellow} y la [**Real Sociedad Matemática Española**]{.hl-yellow}.
:::
:::
:::

. . .

Actualmente, [**investigador y docente en la Facultad de Estadística de la UCM**]{.hl-yellow}. Divulgando por [**Twitter**](https://twitter.com/dadosdelaplace) e [**Instagram**](https://instagram.com/javieralvarezliebana)


---

## Objetivos

::: columns
::: {.column width="37%"}
![](https://assets-global.website-files.com/6092cb6b4ac959f39728dd26/6188a97fa499b5fbfe410417_target%20(1).png)
:::

::: {.column width="63%"}
::: incremental
- Empezar a [**relacionar asignaturas**]{.hl-yellow} como matemáticas, inferencia y R.

- Aprender los fundamentos del [**aprendizaje estadístico**]{.hl-yellow} (ahora llamado Machine Learning o data science)

- Pasar de los descriptivo a lo predictivo: [**construir nuestros primeros modelos**]{.hl-yellow}

- Entender en profundidad el contexto de la [**predicción lineal**]{.hl-yellow}.


:::
:::
:::

---

## Evaluación

-   [**Asistencia**]{.hl-yellow}. Se [**valorará muy positivamente**]{.hl-purple} la participación. Si se [**restarán puntos si eres expulsado de clase**]{.hl-purple}: -0.3 la 1ª vez, -0.6 la 2ª, -1.2 la 3ª...

. . .

- [**Evaluación continua**]{.hl-yellow}: 1 **examen a papel** (25%), **2 entregas R en clase** (20%-30%) y **entrega grupal último día** (3-6p obligatoriamente, 25%)
  
. . .

- [**Examen final**]{.hl-yellow}:
  - [**Menos de un 6 (sin la grupal)**]{.hl-purple} -> examen de R en lugar de la grupal.
  - [**Más de un 6.5 (con la grupal)**]{.hl-purple} -> podrás decidir **peso del final entre un 20% y un 100%**.
  - [**Entre 5 y 6.5 (con la grupal)**]{.hl-purple} -> podrás decidir **peso del final entre un 40% y un 100%**.
  - [**Por debajo de 5 (con la grupal)**]{.hl-purple} -> podrás decidir **peso del final entre un 70% y un 100%**.
  

---

## Planificación


* [**Entrega I (20%)**]{.hl-yellow}: semana 27-29 de febrero de 2024

* [**Entrega II (30%)**]{.hl-yellow}: semana 2-4 de abril de 2024

* [**Parcial con papel y boli (25%)**]{.hl-yellow}: 16-18 de abril de 2024 

* [**Entrega grupal (25%)**]{.hl-yellow}: 9 de mayo de 2024 (incluyendo lo visto hasta el 30 de abril)

&nbsp;

(se admiten propuestas)

. . . 

[**Clase a recuperar otro día**]{.hl-yellow}: clase del 21 de marzo.

---

## Materiales

* [**Diapositivas**]{.hl-yellow}: las diapositivas que usaremos en el aula a lo largo del curso, estructuradas por clases, estarán disponibles y actualizadas en **<https://javieralvarezliebana.es/docencia-R-supervisado-2324/diapos>** 

En el menú de las diapositivas (abajo a la izquierda) tienes una [**opción para descargarlas en pdf**]{.hl-yellow} en `Tools` (consejo: no lo hagas hasta el final del curso ya que irán modificándose)
  
&nbsp;

* [**Material**]{.hl-yellow}: [**scripts de cada tema**](https://github.com/dadosdelaplace/docencia-R-supervisado-2324/tree/main/material) y materiales extras

* [**Resúmenes de paquetes**]{.hl-yellow}: [**chuletas de los paquetes**](https://github.com/dadosdelaplace/docencia-R-supervisado-2324/tree/main/fichas%20paquetes) en formato .pdf

---


## Requisitos

Para el curso los únicos requisitos serán:

1.  [**Conexión a internet**]{.hl-yellow} (para la descarga de algunos datos y paquetes).

2.  [**Instalar R y RStudio**]{.hl-yellow}: la descarga la haremos (gratuitamente) desde <https://cran.r-project.org/> y  <https://posit.co/download/rstudio-desktop/>

3. Se darán por asumido conocimientos aprendidos de [**R base, tidyverse y ggplot**]{.hl-yellow}

4. Se darán por asumido conocimientos aprendidos de [**Quarto, diapositivas en Quarto y Github**]{.hl-yellow}. Para las entregas [**SOLO SE VALORARÁ**]{.hl-purple} la salida html correspodiente.

5. [**Recomendable**]{.hl-yellow}: saber usar la calculadora en modo estadístico.


---

## Listado de clases

* [**Clase 1**](#clase-1): repaso de descriptiva
* [**Clase 2**](#clase-2): importancia de la visualización
* [**Clase 3**](#clase-3): intro al aprendizaje estadístico. Sesgo vs varianza. Supervisado vs no supervisado. Correlación vs dependencia vs causalidad.


# Clase 1: repaso {#clase-1}

[**Objetivo de la predicción lineal. Concepto de linealidad. Repaso de estadística descriptiva**]{style="color:#444442;"}

---

## ¿Qué es predecir?

Como veremos más adelante, en esta asignatura vamos a tratar principalmente lo que se conoce el aprendizaje estadístico como [**predicción (continua)**]{.hl-yellow}

. . .

Dada una [**variable objetivo (variable dependiente)**]{.hl-yellow}, y con la información aportada por un conjunto de [**variables predictoras (covariables)**]{.hl-yellow}, el objetivo será construir un modelo que consiga dar una estimación/predicción lo «mejor posible»

. . .

Es importante que - de momento - distingamos dos conceptos:

* [**Estimación**]{.hl-yellow}: el modelo aprende de unos datos e intenta estimar dichos valores que ha usado.
* [**Predicción**]{.hl-yellow}: el modelo aprende de unos datos e intenta estimar valores que el **modelo no conoce**.

Más adelante los llamaremos «predicción en train» y «predicción en test»

---

## ¿Qué es la linealidad?

En matemáticas decimos que una función $f(x)$ es [**lineal**]{.hl-yellow} cuando se cumple:

* [**Propiedad aditiva**]{.hl-purple}: $f(x + y) = f(x) + f(y)$

* [**Propiedad homogénea**]{.hl-purple}: $f(k*x) = k*f(x)$ (donde $k$ es una constante en $\mathbb{R}$).

Ambas se pueden resumir en $f(a*x + b*y) = a*f(x) + b*f(y)$

. . .

En estadística llamamos [**modelo de predicción lineal**]{.hl-yellow} a un modelo que usa la información de covariables $X_1, X_2, \ldots, X_p$, de manera que su información siempre [**se relacionen entre sí con sumas y restas**]{.hl-yellow}.

- [**Ejemplos lineales**]{.hl-green}: $y = 2*x_1 - 3$ o  $y = 4 - \frac{x_1}{2} + 3*x_2$

- [**Ejemplos no lineales**]{.hl-red}: $y = 2*\frac{1}{x_1}$ o  $y = 4 - x_{1}^{2} - x_2$ o $y = ln(x_1) + cos(x_2)$

En general son modelos que se pueden representar con rectas (una sola predictora), planos (dos predictoras), etc.

---

## Repaso: continua vs discreta

* **Caracteres**: cada una de las características o cualidades que se podrían medir o analizar para cada individuo de la población (y de los que disponemos el valor para cada individuo de la muestra).

* **Modalidades**: diferentes valores que puede adoptar una característica o variable.

&nbsp;

Como veremos más adelante, en el ámbito del aprendizaje estadístico va a ser fundamental tener clara la diferencia entre una [**variable cualitativa y cuantitativa**]{.hl-purple} y entre [**variable continua y otra discreta**]{.hl-yellow}. ¿Cuál es la diferencia?


---

## Repaso: continua vs discreta

Imagina las siguientes variables:

- ¿Tienes hermanos? (si/no)
- Color de zapatillas
- Nivel de estudios
- Número de hermanos
- Número de pelos en la cabeza
- Resultado de un dado
- Temperatura ºC
- Estatura o peso

&nbsp;

[**¿Cuál es la diferencia entre ellas**]{.hl-yellow}


---

## Repaso: continua vs discreta



* [**Cualitativas**]{.hl-yellow}: **cualidades o categorías**. Ejemplos: sexo, estado civil, estudios, etc.
  * [**Ordinales**]{.hl-purple}: admiten relación jerarquica (suspenso-aprobado-bien, sano-leve-grave).
  * [**Nominales**]{.hl-purple}: no tienen asociada una jerarquía (sexo, religión, color de zapatillas, etc).

. . .

* [**Cuantitativas**]{.hl-yellow}: cuantificación numérica.
  * [**Discretas**]{.hl-purple} (barras): se pueden contar y enumerar (aunque sean infinitos) (granos de arena, nº hermanos, etc) → se **suman**
  * [**Continuas**]{.hl-purple}: toman infinitos valores y entre dos valores cualesquiera hay a su vez infinitas opciones (estatura, peso, etc) → se **integran**

---

## Repaso: continua vs discreta

![](img/vitro-fuego-discretas.jpg)


---

## Repaso: población vs muestra

En estadística llamaremos [**población**]{.hl-yellow} al universo teórico, al colectivo de individuos a estudiar, o de posibles elementos/eventos de los podríamos tener observaciones (ejemplo: 47 millones de españoles).

. . .

Cada uno de esos elementos o eventos se llaman **individuos**.

. . .

Aunque nuestro objetivo será conocer algunas de las propiedades de la población, la [**población suele ser inaccesible**]{.hl-red} en su totalidad → [**SELECCIÓN**]{.hl-green} de un conjunto de individuos

---

## Repaso: población vs muestra

Para ello en estadística usamos lo que se conoce como [**muestra**]{.hl-yellow}: un subconjunto finito, de tamaño $n$, «representativo» de la población (en estudio estadístico realizado sobre la totalidad de una población se denomina **censo**).

. . .

Para ello existe una rama conocida como [**muestreo**]{.hl-yellow}

* [**Probabilístico**]{.hl-purple}: todos los individuos tienen oportunidad de ser seleccionados, y la probabilidad de que suceda puede ser modelizada (al azar, por estratos, etc).

* [**No probabilístico**]{.hl-purple}: algunos elementos de la población no tienen posibilidad de selección (sesgo de exclusión), o su probabilidad no puede ser conocida.

. . .

🤔 ¿Sería adecuado hacer una encuesta sobre el streamer favorito de los jóvenes a través de una encuesta realizada por teléfono fijo?


---

## Sesgos en el muestreo

![](img/sampling-bias.jpg)

. . .

[**Sesgo de selección**]{.hl-yellow}: aparece cuando no se tiene en cuenta la forma en la que se han recogido los datos.

--- 


## Sesgos en el muestreo


![](img/dewey.jpg)

El ejemplo más famoso es el caso [**«Dewey defeats Truman» (Dewer derrota a Truman)**]{.hl-yellow}, el titular con el que abrió el Chicago Tribune en 1948, el mismo día en el que Truman ganó al repúblicano Dewer en las elecciones de 1948: sin esperar a los resultados, se basaron en una encuesta telefónica (sin contar con el sesgo que, en aquella época, solo la clase alta tenía teléfono).

---



## Sesgos en el muestreo

![](img/superviviente.jpg)

¿Dónde reforzarías los aviones?

---

## Sesgos en el muestreo

![](img/superviviente.jpg)



El [**sesgo del superviviente**]{.hl-yellow} (un tipo de sesgo de selección) aparece cuando se toma una muestra de un fenómeno ignorando si los individuos elegidos tienen las mismas opciones respecto al mismo.


---

## Repaso: medidas de centralización

* [**Media**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la media muestral $\overline{x}$ se define como la **suma de todos los valores dividida por el tamaño muestral**

$$\overline{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$

. . .

[**Geométricamente**]{.hl-purple}: es el **valor «más cercano» de todos los datos a la vez** (minimiza las distancias al cuadrado)

---

## Media muestral


[**VENTAJAS**]{.hl-green}

* Fácil de calcular y entender
* Fácil y eficiente de programar
* Siempre existe (para cuantitativas)

. . .

[**DESVENTAJAS**]{.hl-red}

* No es un valor de los datos (la media de {1, 2, 3, 4} es 2.5)
* **Poco robusta** (valores atípicos le afectan mucho)
* Solo se puede definir para variables cuantitativas


---

## Repaso: medidas de centralización

* [**Mediana**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la mediana muestral se define como el **valor que es mayor o igual que al menos el 50%**, y menor igual que al menos el 50% de los datos

$$Me_{x} = \arg \min_{x_i} \left\lbrace F_i > 0.5 \right\rbrace, \quad Me_x = e_{i-1} + \frac{0.5 - F_{i-1}}{F_i - F_{i-1} }a_i$$

La mediana es el [**valor de en medio**]{.hl-purple} si ordenamos los datos (y si se pueden ordenar...)

---

## Mediana muestral


[**VENTAJAS**]{.hl-green}

* Suele ser un valor de la muestra
* Un poco más robusta que la media


. . .

[**DESVENTAJAS**]{.hl-red}

* Muy ineficiente (requiere un algoritmo de ordenación)
* Solo definida para  cuantitativas o cualitativas ordinales

---


## Repaso: medidas de centralización

* [**Moda**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la moda muestral se define como el **valor o valores más repetidos** (en caso de que existan).

$$Mo_x = \arg \max_{x_i} f_i, \quad Mo_x = e_{i-1} + \frac{d_i - d_{i-1}}{\left(d_i - d_{i-1} \right) + \left(d_i - d_{i+1} \right)}a_i$$


. . .

[**Gráficamente**]{.hl-purple}: representa el «pico» de un diagrama de barras o un histograma

---

## Moda muestral


[**VENTAJAS**]{.hl-green}

* Es un valor de la muestra
* Muy robusta
* **Se puede calcular para cualquier** cuanti o cuali



. . .

[**DESVENTAJAS**]{.hl-red}

* No siempre existe (amodal) y pueden existir varias (bimodal, trimodal, etc)
* Poco usada en inferencia


---

## Repaso: medidas de centralización

![](img/ine-salarios-oculto.jpg)

**¿Cuál es la mediana, la media y la moda?**

---

## Repaso: medidas de centralización

![](img/ine-salarios.jpg)

---

## Repaso: medidas de dispersión

![](img/iker-jimenez.jpg)

¿Qué tiene que ver la imagen con la dispersión?


---

## Repaso: medidas de dispersión

![](img/extremos.jpg)

El cambio climático no solo es porque aumente la [**temperatura media (centralización)**]{.hl-yellow} sino por la aparición cada vez más frecuente de fenómenos extremos 


Aumento de la [**variabilidad**]{.hl-yellow} → aumento de la [**DISPERSIÓN**]{.hl-yellow}

---

## Repaso: medidas de dispersión

[**¿Cómo medir lo que se alejan los datos de la media?**]{.hl-yellow}

. . .

![](img/primera-idea-varianza.jpg)

Una **primera idea** podría ser [**medir la distancia de cada dato al centro**]{.hl-yellow}, es decir, restar cada dato de la media, y después realizar su promedio.

---

![](img/wait-for-it.jpg)

---

## Repaso: medidas de dispersión

Imagina que tenemos la siguiente muestra $X = \left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$.

**¿Cuánto vale la media?**

. . .

La media vale 0 y la distancia a ella es...la propia muestra $\left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$. **¿Cuál es el promedio de dichas distancias?**

. . .

Pues...de nuevo vale 0.

. . .

Si la dispersión es 0...[**¿no hay dispersión?**]{.hl-red} ¿No debería de dar 0 solo cuando los datos sean constantes?

---

## Repaso: medidas de dispersión

Para **evitar que se cancelen** los signos lo que haremos será calcular el [**promedio PERO de las distancias al cuadrado**]{.hl-yellow}, la conocida como [**varianza**]{.hl-yellow}

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 $$

. . .


::: callout-warning
# Cuidado

Tomar el valor absoluto (para evitar que se cancelen los signos) suele ser una mala idea en matemáticas (no es derivable como función).
:::

---

## Repaso: medidas de dispersión


[**Problema**]{.hl-red}: si los datos están en metros, la varianza estará en...metros cuadrados


. . .

:::: columns
::: {.column width="50%"}

¿Tiene sentido medir la dispersión de nuestra estatura en baldosas?

:::

::: {.column width="50%"}

![](img/albert-rivera.jpg)
:::

::::

---

## Repaso: medidas de dispersión

Para tener una [**medida de dispersión en las unidades de los datos**]{.hl-yellow} calcularemos la [**desviación típica**]{.hl-yellow}, como la raíz cuadrada de la varianza

$$s_{x} = \sqrt{s_{x}^{2}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2} = \sqrt{\overline{x^2} - \overline{x}^2}$$


---

## Repaso: medidas de dispersión

Todavía tenemos un pequeño problema.

Imagina que queremos **comparar la dispersión de dos conjuntos** de datos, estaturas de personas y diámetros de núcleos de células. Y Supongamos que las medias son 170 cm y 5 micrómetros, y la desviación típica de 1 cm y 1.5 micrómetros.

[**¿Qué conjunto de datos es más disperso?**]{.hl-yellow}

. . .

Para tener una  **medida de dispersión adimensional** definiremos el [**coeficiente de variación**]{.hl-yellow} 

$$CV_{x} = \frac{s_{x}}{\left| \overline{x} \right|}$$

---

## Repaso: medidas de localización

Las [**medidas de posición o localización**]{.hl-yellow} nos localizan los datos, siendo valores que **nos dividen un conjunto ordenado** en subconjuntos del mismo tamaño (ejemplo: mediana es percentil 50).

* **Percentil**: valores $P_{\alpha}$ del conjunto ordenado que dejan por debajo, al menos, el $\alpha$% de datos y $\alpha$% por encima.       
* **Decil**: valores $D_{\alpha}$ que dividen los datos en 10 partes iguales.

* **Cuartil**: valores $C_{\alpha}$ o $q_{\alpha}$ que dividen los datos en 4 partes iguales.

---

## Repaso: covarianza y correlación

[**¿Qué es en realidad la varianza?**]{.hl-yellow}

. . .

La [**varianza es el promedio de las desviaciones al cuadrado**]{.hl-yellow} (respecto a la media), apareciendo dos veces dicha desviación: puede ser entendida como una [**medida que cuantifica la relación de una variable CONSIGO MISMA**]{.hl-yellow}

. . .

[**¿Y si quiésemos medir la relación de una variable X respecto a otra variable Y (en lugar de consigo misma)?**]{.hl-purple}


---

## Repaso: covarianza y correlación

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 \quad \text{(varianza)}$$


La idea detrás de la [**covarianza**]{.hl-yellow} es justo esa: sustituir una de esas desviaciones de la X por la desviación de la Y.


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

---

## Repaso: covarianza y correlación

Es importante entender algunas [**propiedades de la covarianza**]{.hl-yellow}

. . .

* [**Signo**]{.hl-purple}: la covarianza puede ser tanto positiva como negativa como 0: al eliminar el cuadrado de la varianza, ya no es necesario que sea positiva


. . .

* [**¿Qué cuantifica?**]{.hl-purple} La covarianza mide la [**relación LINEAL**]{.hl-red} (en torno a una recta) entre dos variables

. . .

* [**¿Qué dice su signo?**]{.hl-purple} El signo de la covarianza nos indicará la [**dirección de la dependencia lineal**]{.hl-yellow}: si es positiva, la relación será creciente (cuando X crece, Y crece); si es negativa, la relación será decreciente (cuando X crece, Y decrece)


---

## Repaso: covarianza y correlación

Al igual que pasaba con la varianza, la [**covarianza depende de las unidades y magnitudes**]{.hl-yellow} de los datos, así que lo que haremos será [**estandarizar la covarianza**]{.hl-yellow}. Definiremos la [**coeficiente correlación lineal (de Pearson)**]{.hl-yellow} como la covarianza dividida entre el producto de las desviaciones típicas (adimensional)


$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

. . .

Tiene el [**mismo signo que la covarianza**]{.hl-yellow} (el denominador es siempre positivo) y sus [**valores siempre están entre -1 y 1**]{.hl-yellow}

* más cerca de -1 o 1 → relación lineal más fuerte
* más cerca de 0 → ausencia de relación **LINEAL**

---

## Repaso: covarianza y correlación

![](img/correlaciones.jpg)


# Clase 2: importancia del dataviz {#clase-2}

[**¿Basta con calcular la correlación para cuantificar si un ajuste es adecuado?**]{style="color:#444442;"}


---

## Conjunto anscombe

En el paquete `{datasets}` se encuentra el dataset conocido como [**cuarteto de Anscombe**]{.hl-yellow}, un dataset que cuenta con 4 conjuntos de datos.

```{r}
anscombe_tb <- as_tibble(datasets::anscombe)
anscombe_tb
```

. . .

[**Primera tarea: convierte a tidydata**]{.hl-yellow}

---

## Conjunto anscombe

```{r}
anscombe_x <- 
  anscombe_tb |>
  pivot_longer(cols = x1:x4, names_to = "dataset", values_to = "x") |> 
  select(-contains("y")) |> 
  mutate(dataset = str_remove_all(dataset, "x"))

anscombe_y <- 
  anscombe_tb |>
  pivot_longer(cols = y1:y4, names_to = "dataset", values_to = "y") |> 
  select(-contains("x"), -dataset)

anscombe_tidy <-
  anscombe_x |> mutate("y" = anscombe_y$y)
anscombe_tidy
```

---

## Conjunto anscombe

[**¿Qué características muestrales tienen?**]{.hl-yellow} Calcula la media, varianza, desv. típica, covarianza y correlación en cada dataset

. . .

```{r}
anscombe_tidy |> 
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

. . .

Si te fijas [**todos los datasets tienen los mismos momentos**]{.hl-yellow} muestrales, incluso tendrían el mismo ajuste de regresión...¿serán el mismo dataset desordenado?

---

## Conjunto anscombe

[**Visualiza los 4 datasets**]{.hl-yellow}

. . .

```{r}
#| code-fold: true
ggplot(anscombe_tidy) +
  geom_point(aes(x = x, y = y, color = dataset), size = 3, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~dataset) + theme_minimal()
```

---

## Conjunto anscombe

```{r}
#| echo: false
ggplot(anscombe_tidy) +
  geom_point(aes(x = x, y = y, color = dataset), size = 3, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~dataset) + theme_minimal()
```


Por suerte o por desgracia [**no todo son matemáticas**]{.hl-red}: antes de pensar que modelo es mejor para nuestros datos, es importantísimo realizar un [**análisis exploratorio**]{.hl-yellow} de los datos (incluyendo visualización)

---

## Datasaurus

Podemos visualizarlo de manera aún más extrema con el dataset `datasaurus_dozen` del paquete `{datasauRus}` (ver más en <https://www.research.autodesk.com/publications/same-stats-different-graphs/>)

```{r}
library(datasauRus)
datasaurus_dozen |>
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

---

## Datasaurus

```{r}
#| code-fold: true
ggplot(datasaurus_dozen |> filter(dataset != "wide_lines"),
       aes(x = x, y = y, color = dataset)) +
  geom_point(size = 2.5, alpha = 0.75) +
  geom_smooth(method = "lm", se = FALSE)  +
  facet_wrap(~dataset, ncol = 3) +
  theme_minimal()
```

---

# Clase 3: profundizando en la correlación {#clase-3}

[**Matrices de correlación y covarianza. Correlación vs causalidad vs dependencia**]{style="color:#444442;"}


---

## Correlación lineal: sin agrupar


Como decíamos, la idea detrás de la [**covarianza**]{.hl-yellow} es una "varianza" entre dos variales (la varianza es una covarianza de una variable consigo misma), midiendo el **promedio de lo que se desvía cada una respecto a su media**

$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

. . .

Para calcularla en `R` basta con usar la función `cov()`

```{r}
starwars |> 
  drop_na(mass, height) |> 
  summarise(cov(mass, height))
```

---

## Correlación lineal: sin agrupar

Vamos a practicar una vez más como [**hacerlo a mano con el siguiente ejercicio**]{.hl-yellow}.

---

## Cor lineal: sin agrupar

En la tabla inferior se han recopilado (del 2013 al 2022) la **temperatura media en el mes de abril en Madrid (variable X, en ºC)** y el **número de días (variable Y) en el que el nivel de ozono superó las 0.20 ppm (partes por millón)**

* ¿Cuál fue media de días en los que se superó umbral de ozono de 0.20 ppm?
* ¿Cuál fue media de días en los que se superó umbral de ozono en los años que la temperatura media en marzo superó los 17.4ºC?
* ¿Cuál es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)

---


## Correlación lineal: sin agrupar

Repite el ejercicio con pocas líneas de código `R`

* ¿Cuál fue la media de días en los que se superó el umbral de ozono de 0.20 ppm?
* ¿Cuál fue la media de días en los que se superó el umbral de ozono en los años que la temperatura media en marzo superó los 17.4ºC?
* ¿Cuál es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)


---


## Correlación lineal: sin agrupar

Realiza lo que consideres tanto a mano como en `R`

* ¿Existe alguna **relación de dependencia entre las variables**? ¿De qué tipo? ¿Cómo de fuerte o débil es dicha relación? ¿En qué dirección es dicha relación?


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

---

## Correlación lineal: sin agrupar

No sé si te has fijado qué sucede cuando intentamos [**calcular la covarianza/correlación de varias variables**]{.hl-yellow}, por ejemplo vamos a calcular la (cuasi)covarianza de todas las variables numéricas de starwars.

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

. . .

Podemos usar la función `cov()` sin más, fuera de un resumen, obteniendo lo que se conoce como [**matriz de (cuasi)covarianzas**]{.hl-yellow} y que tendrá un papel fundamental en estadística ya que contiene la información (= varianza) del dataset.

---

## Matriz de covarianzas


```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

Además de ser [**simétrica**]{.hl-yellow}...¿qué tenemos en la [**diagonal**]{.hl-yellow}?

. . .

La [**matriz de (cuasi)covarianzas**]{.hl-yellow} se denota como $\Sigma$ y sus elementos se define como $\Sigma_{ii} = s_{x_i}^{2}$ para la diagonal y $\Sigma_{ij} = \Sigma_{ji} = s_{x_i x_j}$ fuera de ella.

. . .

::: callout-important
## Importante

Recuerda que los softwares estadísticos nos devuelven siempre la [**cuasi covarianza**]{.hl-purple}, dividido entre $n-1$ y no entre $n$. La cuasivarianza y la cuasicovarianza son los [**mejores estimadores muestrales (insesgados)**]{.hl-yellow} de los respectivos parámetros poblaciones

:::

---

## Matriz de correlaciones

De la misma manera con `cor()` podemos [**calcular la matriz de correlaciones**]{.hl-yellow} (en este caso sin el `cuasi` ya que se cancelan denominadores)

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cor()
```

. . .

La [**matriz de correlaciones**]{.hl-yellow} se denota como $R$ y sus elementos se define como $r_{ii} = 1$ para la diagonal y $r_{ij} = r_{x_ix_j}$ fuera de ella, y nos proporciona la dependencia lineal entre variables ya de manera **estandarizada**.

---

## Matriz de correlaciones

[**¿Se te ocurre alguna manera de calcular la matriz de correlaciones a partir de la de covarianzas?**]{.hl-yellow}

---

## Correlación lineal: datos agrupados

Imagina ahora que queremos calcular la covarianza/correlación de los siguientes datos que representan 24 tiradas de dados

![](img/dados.png)

¿Cómo calcular la covarianza/correlación agrupando los datos?

---

## Correlación lineal: datos agrupados

![](img/tabla-bi-freq.png)
---

## Correlación lineal: datos agrupados

---

## Correlación vs dependencia


Podemos tener [**variables incorreladas**]{.hl-red}, con correlación nula, pero que [**exista dependencia entre ellas**]{.hl-green}: la covarianza/correlación [**SOLO CAPTURA relaciones lineales**]{.hl-yellow}, nada más.

. . .

Veamos un ejemplo sencillo con $X = \left\lbrace -1, 0, 1 \right\rbrace$ y $Y = X^2 =  \left\lbrace 1, 0, 1 \right\rbrace$. 

* La media de ambas es nula
* La media del producto es la media de $XY = \left\lbrace -1, 0, 1 \right\rbrace$, que es de nuevo nula
* Así la covarianza $\overline{x*y} - \overline{x}*\overline{y}$ es nula a pesar de tener la mayor dependencia posible (dependencia funcional)

---

## Correlación vs dependencia

![](img/covarianza-no-lineal.png)

En relaciones no lineales como la de la imagen, la **correlación estará cercana a cero** (ya que no hay relación lineal) pero existe una [**dependencia**]{.hl-yellow}. Diremos que [**dos variables son dependientes entre sí**]{.hl-yellow} cuando existe un **patrón numérico que las relaciona**

. . .

* [**Independencia implica incorrelación**]{.hl-green}
* [**Incorrelación NO implica independencia**]{.hl-red}

---

## Correlación vs dependencia

![](img/escenarios-covarianza.png)

---

## Dependencia vs causalidad

![](img/helados-moreno.jpg)

Si $X$ e $Y$ fuesen variables dependientes (lineales o no)...[**¿implicaría que el ascenso/descenso de una provoca el ascenso/descenso de la otra?**]{.hl-yellow}

. . .

Imagina por ejemplo dos variables: nivel de bronceado (X) y consumo de helados (Y). ¿Son dependientes? Aparentemente sí ya que su comportamiento es similar. **¿Una causa la otra?**

---

## Dependencia vs causalidad

![](img/helados-moreno.jpg)

Diremos que dos variables tienen una [**relación causal o de causalidad**]{.hl-yellow} cuando una variable es la CAUSA del comportamiento de la otra, algo que no sabremos solo con estadística (sino con conocimiento experto, en este caso de nutricionistas y médicos)

. . .

[**Correlación NO IMPLICA causalidad**]{.hl-red} (al menos no tiene por qué)

---

## Dependencia vs causalidad


![](img/chocolate-correlacion.jpg)


Este fenómeno es conocido como [**correlaciones espúreas**]{.hl-yellow}, y aparece cuando dos variables presentan una [**relación matemática**]{.hl-green} pero sin [**ningún tipo de relación causal o lógica**]{.hl-red}. Puedes ver más en <https://www.tylervigen.com/spurious-correlations>

---

## Dependencia vs causalidad

![](img/confounder.jpg)

Dicho patrón matemático puede deberse al mero azar o la existencia de lo que se conoce como [**variables de confusión**]{.hl-yellow} (en el caso del helado, el verano es el que causa ambas).

La rama que se dedica al [**estudio de la causalidad**]{.hl-yellow}, combinando las ramas de la **estadística, filosofía, sociología y psicología** se le conoce como [**inferencia causal**]{.hl-yellow} y es fundamental en un análisis más profunde de las relaciones entre las variables (sobre todo en campos como la economía o la sociología)


# Clase 4: primeras regresiones {#clase-4}

[**Historia regresión. Aprendizaje supervisado. Regresión lineal univariante**]{style="color:#444442;"}

---

## Historia de la reg

> Hay una regla universal: cualquier pariente tuyo es probablemente más mediocre que tú»

La [**historia de regresión**]{.hl-yellow} se remonta a **Francis Galton**, primo de Charles Darwin, que además de estadístico fue psicólogo, geógrafo y, por desgracia, el primer eugenésico (de hecho acuñó el termino)

. . .

También fue el primero en proponer métodos de clasificación de huellas en medicina forense e incluso se le atribuye el primer mapa meteorológico de la historia

---

## Regresión y Darwin

Galton mostró fascinación por «El origen de la especies» de su primo. Sin embargo, [**Galton no se centraba en los mejor adaptados sino en los que él llama mediocres**]{.hl-yellow}

. . .

Según Galton, las sociedades estaban fomentando la mediocridad, interfiriendo en la selección natural, así que empezó a **estudiar si el talento era o no hereditario**.


. . .

¿Su conclusión? El [**talento se disipaba**]{.hl-yellow} a lo largo de las generaciones

---

## Regresión a la mediocridad

En 1886 publicó «Regression towards mediocrity in hereditary stature», un artículo que cambiaría la estadística: fue el [**primer uso documentado de lo que hoy conocemos como recta de regresión**]{.hl-yellow} 

. . .

:::: columns
::: {.column width="38%"}

![](img/reg_dalton.jpg)

:::

::: {.column width="62%"}

Galton analizó la estatura de 205 hijos y padres, observando que, de nuevo, los valores extremos se disipaban: a lo largo de las generaciones había una [**regresión (un retroceso) a la mediocridad (entendida como la media)**]{.hl-yellow} 


:::
::::

Hijos de altos eran algo más bajitos, e hijos de bajitos eran algo más altos.

---

## Regresión a la mediocridad

:::: columns
::: {.column width="60%"}

![](img/reg-galton-coef.jpg)

:::

::: {.column width="40%"}

Galton no solo observó que las [**estaturas «regresaban» a un valor medio sino que lo hacían con un patrón**]{.hl-yellow}, con un factor constante de $2/3$: si los padres se desviaban $+3$ por encima de la media, los hijos se desviaban solo $(2/3)*3 = +2$ por encima de la media.

:::
::::

---

## Aprendizaje estadístico

La regresión lineal es el modelo más simple de lo que se conoce como [**aprendizaje estadístico (Machine Learning)**]{.hl-yellow}, en concreto del conocido como [**aprendizaje supervisado**]{.hl-yellow}

![](img/ml.jpg)


---

## Ciencia de Datos


La [**ciencia de datos**]{.hl-yellow} es precisamente la rama que integra las matemáticas, la estadística, la probabilidad, el Machine Learning e incluso el Big Data

![](img/stats-ml.jpg)

---

## Aprendizaje ¿supervisado?

![](img/unsupervised-learning.jpg)

---

## Aprendizaje ¿supervisado?

En el campo del Machine Learning hay principalmente dos tipos de metodologías:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje supervisado**]{.hl-purple}: tendremos dos tipos de variables, la [**variable dependiente (output/target)**]{.hl-yellow} que se quiere predecir/clasificar, normalmente denotada como $Y$, y las [**variables independientes (inputs) o explicativas o predictoras**]{.hl-yellow}, que contienen la información disponible. Ejemplos: regresión, knn, árboles, etc.
 

:::

::: {.column width="50%"}

![](img/supervised.jpg)
:::
::::

---

## Aprendizaje ¿supervisado?

En el campo del Machine Learning hay principalmente dos tipos de metodologías:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje no supervisado**]{.hl-purple}: no existe la distinción entre target y variables explicativas ya que [**no tenemos etiquetados los datos**]{.hl-yellow}, no sabemos a priori la respuesta correcta. El aprendizaje no supervisado [**buscará patrones**]{.hl-yellow} basados en similitudes/diferencias. Ejemplos: PCA, clustering, redes neuronales, etc.

:::


::: {.column width="50%"}

![](img/unsupervised.jpg)

:::
::::

---

## Clasificación vs predicción

Como hemos comentado, la [**regresión lineal**]{.hl-yellow} se enmarca dentro del [**predicción supervisada**]{.hl-yellow}

* [**Predicción**]{.hl-purple}: la [**variable objetivo es una variable cuantitativa continua**]{.hl-yellow} (por ejemplo, precio, glucosa, peso, etc).

* [**Clasificación**]{.hl-purple}: la [**variable objetivo es una variable cualitativa**]{.hl-yellow} (por ejemplo, especie de flor, ausencia/presencia de enfermedad, si/no, etc) o **cuantitativa discreta** (por ejemplo, número de accidentes). La etiqueta tomará un valor dentro del conjunto de modalidades permitidas, pudiendo ser **binaria** (si/no) o **multiclase** (A, B, C, D).

&nbsp;

📚 Ver «The elements of Statistical Learning» (Hastie et al., 2008)

# Clase 5: ajuste de regresión {#clase-5}

[**Interpretación de coeficientes. Método mínimos cuadrados**]{style="color:#444442;"}

---

## Modelo predictivo

Dentro del marco de la [**predicción supervisada**]{.hl-yellow} un modelo tendrá siempre la siguiente forma:

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$

* $X$ serán los [**datos**]{.hl-yellow}

* $f(\cdot)$ será nuestro [**modelo**]{.hl-yellow}, es decir, el [**valor esperado de $Y$**]{.hl-yellow} con la información que tenemos.

* $\mathbf{X} = \left(X_1, \ldots, X_p \right)$ serán nuestras [**predictoras o variables independientes**]{.hl-yellow}

* $\varepsilon$ será el [**error o ruido**]{.hl-yellow}, una [**variable aleatoria de media 0**]{.hl-yellow} $E \left[\varepsilon | \boldsymbol{X} = x \right] = 0$ (el error debería ser reducido a **algo aleatorio (irreducible)**, aunque en estadística SIEMPRE nos vamos a equivocar).

---

## Modelo predictivo

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$


El objetivo es intentar [**estimar dicha variable objetivo**]{.hl-yellow} minimizando al máximo el error cometido (la parte que podemos evitar).

. . .

El [**modelo estimado**]{.hl-yellow} se definirá como

$$\widehat{Y} := \widehat{E \left[Y | \boldsymbol{X} = x \right]}  = \widehat{f}\left(X_1, \ldots, X_p \right), \quad \widehat{\varepsilon} = \widehat{Y} - Y$$

* $\widehat{Y}$ serán las [**estimaciones**]{.hl-yellow}, definidas como la estimación del [**valor esperado de $Y$**]{.hl-yellow} con la información que tenemos.

* $\widehat{f}$ será el [**modelo estimado**]{.hl-yellow}

* $\widehat{\varepsilon}$ los [**errores estimados**]{.hl-yellow} (con media muestral igual a cero).


---

## Reg. lineal univariante

En el caso concreto de la [**regresión lineal**]{.hl-yellow} nuestro modelo será un **hiperplano lineal** (en el caso de una variable, una simple recta):

$$E \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p$$

. . .

Su estimación será por tanto

$$\widehat{Y} := \widehat{E \left[Y | \boldsymbol{X} = x \right]} = \widehat{f}(\mathbf{X}) = \widehat{\beta_0} + \widehat{\beta_1} X_1 + \ldots + \widehat{\beta_p} X_p $$

* El modelo $\widehat{f}(\cdot)$ será una [**recta/hiperplano lineal**]{.hl-yellow}

* En el caso de la regresión lineal [**univariante**]{.hl-yellow} tendremos por tanto $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1X$ ($p = 1$)

* El objetivo será obtener la estimación de los $\widehat{\beta}$ tal que **minimicemos el error**

---

## Varianza residual

Como ya hemos comentado, una manera de cuantificar lo que nuestro modelo se equivoca es lo que llamamos [**varianza residual o error cuadrático medio**]{.hl-yellow} (varianza del error), que podemos estimar como sigue (ya que la [**media muestral de los errores será cero**]{.hl-purple})

$$s_{r}^{2} = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \left( \widehat{\varepsilon}_{i} - \overline{\widehat{\varepsilon}}_{i}\right)^2 = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} $$
¿Cómo quedaría la fórmula si **desarrollo lo que vale los errores estimados** $\widehat{\varepsilon}$?

. . .


$$s_{r}^{2} := s_{r}^{2}\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)= \frac{1}{n} \sum_{i=1}^{n} \left(Y_i - \widehat{Y}_i \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\widehat{\beta}_0 + \widehat{\beta}_1X_i \right)\right]^2$$

---

## Método mínimos cuadrados

El [**método de los mínimos cuadrados**]{.hl-yellow}, originariamente planteado a la vez por Legendre y Gauss, consiste en establecer que los [**parámetros óptimos**]{.hl-yellow} serán aquellos $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$ que minimicen dicha suma de cuadrados (que minimicen la varianza residual)

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} s_{r}^{2}\left(\beta_0, \beta_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} n*s_{r}^{2}\left(\beta_0, \beta_1 \right)$$

Vamos a desarrollar ese cuadrado...

--- 


## Método mínimos cuadrados

$$s_{r}^{2}\left(\beta_0, \beta_1 \right) = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\beta_0 + \beta_1 X_i \right)\right]^2$$ 
Vamos a desarrollar ese cuadrado...

. . .

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$




---

## Método mínimos cuadrados

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$

¿Cómo encontrar el mínimo de una función?

. . .

Efectivamente, derivando.

$$\frac{\partial s_{r}^{2}}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{0} + 2 \beta_1 X_i - 2Y_i \right] = 2 \left( \beta_{0} + \beta_{1} \overline{x} - \overline{y} \right)$$ 

$$\frac{\partial s_{r}^{2}}{\partial \beta_1} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{1}X_{i}^{2} + 2 \beta_0 X_i - 2Y_iX_i \right] = 2 \left( \beta_{1} \overline{x^2}+\beta_{0}\overline{x} - \overline{xy} \right)$$ 

---

## Método mínimos cuadrados


Si lo agrupamos en un sistema y lo [**igualamos a cero**]{.hl-yellow} para encontrar el óptimo tenemos

$$\left\{\beta_{0} + \beta_{1} \overline{x}   = \overline{y} \atop \overline{x} \beta_{0} + \overline{x^2} \beta_{1} = \overline{xy} \right.$$ 

¿Cómo resolver un sistema?

. . .

Haciendo la regla de Cramer tenemos que

$$\widehat{\beta}_1 = \frac{\overline{xy} - \overline{x}*\overline{y}}{\overline{x^2} - \overline{x}^2} = \frac{s_{xy}}{s_{x}^{2}}$$

$$\widehat{\beta}_0 = \frac{\overline{y}*\overline{x^2} - \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2}  = \overline{y} + \frac{\overline{y}*\overline{x}^2- \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2} = \overline{y} - \widehat{\beta}_1 \overline{x}$$


---

## Estimación reg. univariante


Los [**estimadores de los parámetros de una regresión lineal univariante**]{.hl-yellow} serán por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

. . .

Es importante [**distinguir lo población de lo muestral**]{.hl-yellow}

* Los parámetros $\left( \beta_0, \beta_1 \right)$ son desconocidos, los [**parámetros poblacionales**]{.hl-yellow}

* Los parámetros $\left( \widehat{\beta}_0, \widehat{\beta}_1 \right)$ son estimados a partir de los datos, son [**variables aleatorias**]{.hl-yellow} ya que han sido calculados en función de una muestra aleatoria $\left\lbrace \left(x_i, y_i \right) \right\rbrace_{i=1}^{n}$ de la población $\left(X, Y \right) $

---

## Estimación reg. univariante

Los [**estimadores de los parámetros de una regresión lineal univariante**]{.hl-yellow} serán por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

¿Cuál es su [**interpretación**]{.hl-yellow}?

. . .

* [**Ordenada en el origen**]{.hl-yellow}: también llamado **intercepto**, y denotado como $\beta_0$ su valor real, es el valor de $Y$ cuando $X=0$. Es decir, $\widehat{\beta}_0$ se puede interpretar como la estimación $\widehat{Y}$ cuando $X = 0$ (cuando dicha estimación tenga sentido).


. . .

* [**Pendiente**]{.hl-yellow}: denotado com $\beta_1$ su valor real, cuantifica el incremento de $Y$ cuando $X$ aumenta una unidad. Es decir, $\widehat{\beta}_1$ se puede interpretar como la variación de la **estimación $\widehat{Y}$ cuando $X$ tiene un incremento unitario**.

---

## Regresión en R

Para hacer un [**ajuste de regresión lineal univarainte en R**]{.hl-yellow} solo necesitamos utilizar la función `lm()` (de linear model) con dos argumentos:

* `data = ...`: los datos de los que queremos sacar el ajuste (sin ausentes de momento)
* `formula = ...`: la fórmula del ajuste (y frente a x, expresado como `y ~ x`)

```{r}
datos <- 
  starwars |>
  drop_na(mass, height) 
ajuste_lineal <- lm(data = datos, formula = mass ~ height)
```

---

## Regresión en R

Una vez realizado el ajuste podemos [**resumirlo**]{.hl-yellow} con `summary()`

```{r}
ajuste_lineal |> summary()
```

. . .

¿Qué representa cada bloque de la salida?

---

## Regresión en R

```{r}
ajuste_lineal |> summary()
```

* `Call: ...`: la orden que hemos ejecutado

* `Residuals: ...`: un resumen en forma de [**cuartiles de los residuales**]{.hl-yellow}, es decir, de los [**errores**]{.hl-yellow} (fíjate que aunque la media de los errores siempre es 0, no tiene porque serlo la mediana, de hecho que no lo sea ya nos indica que normales no van a ser seguramente).


---

## Regresión en R

```{r}
ajuste_lineal |> summary()
```


* `Coefficients: ...`: de momento solo nos interesa la columna [**Estimate**]{.hl-yellow} que nos da las estimaciones de los parámetros. En la fila `Intercept` siempre irá $\widehat{\beta}_0$, y el resto de filas tendrá el nombre de la variable predictora a la que multiplica el parámetro (en este caso la fila `height` corresponde a la estimación $\widehat{\beta}_1$).

---


## Regresión en R

```{r}
ajuste_lineal |> summary()
```

En nuestro caso particular, el [**ajuste de regresión**]{.hl-yellow} viene dado por $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1 X = -11.4868 + 0.6240*X$, donde $X$ es la estatura: por cada cm extra que mida un personaje, el modelo estima que el peso aumenta 0.6240 kg, y la estimación de peso para un persona que mida 0 cm es de -11.4868 kg (obviamente esta estimación mucho sentido no tiene)

---

## Regresión en R

```{r}
ajuste_lineal |> summary()
```

* `Multiple R-squared: ...` (de momento lo demás lo ignoramos): es el conocido como $R^2$ o bondad de ajuste. Hablaremos de él en las próximas clases, pero de momento, nos basta saber que es una [**métrica de calidad del ajuste**]{.hl-yellow} que va de 0 (peor) a 1 (mejor).

---

## Predicción en R

Una vez estimado el modelo (sus coeficientes) podemos usarlo, bien para  [**estimar valores que el modelo conoce**]{.hl-yellow} (de los que aprendió) o para [**predecir nuevos valores**]{.hl-yellow} de los que el modelo no ha aprendido

. . .

Simplemente necesitamos la función `predict(modelo, nuevos_datos)`, usando como argumentos el **modelo entrenado** y un [**nuevo dataset que contenga una/s columna/s llamadas igual que nuestras variables predictores**]{.hl-yellow} con los valores de la predictora para los que queremos predecir

```{r}
predicciones <- 
  tibble("height" = c(15, 81.3, 153.1, 189.3, 210.3),
         "mass_predict" = predict(ajuste_lineal, tibble("height" = height)))
predicciones
```

---

## Rango de fiabilidad

Aunque hablaremos en profundidad de la bondad de ajuste, supongamos que tenemos un **modelo extremadamente bueno**, con una bondad de ajuste $R^2 = 0.99$. ¿Sería fiable nuestro modelo anterior para predecir el peso de un personaje con $X = 15$ cm?

. . .

```{r}
predicciones
```

Si te fijas la [**¡predicción del peso es negativa!**]{.hl-red}: por muy bebé que sea, algo pesará. ¿Por qué sucede esto?

---

## Rango de fiabilidad


![](img/rango_1.png)
![](img/rango_2.png)

Nuestro [**modelo solo puede aprender de los datos que conoce**]{.hl-yellow}: solo podremos predecir para valores nuevos de las predictoras [**dentro del rango de las predictoras con las que entrenamos**]{.hl-yellow} el modelo

Da igual lo bueno que sea el modelo: las [**predicciones fuera del rango de las predictoras de entrenamiento**]{.hl-red} no serán fiables ya que el modelo no sabe lo que sucede fuera.

# Clase 6: diagnosis  {#clase-6}

[**Hipótesis e inferencia del modelo**]{style="color:#444442;"}

---

## Repaso


* [**Modelo lineal**]{.hl-yellow}: descrito como $Y = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p X_p + \varepsilon$, buscamos estimar el **valor esperado**  $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p x_p$, donde $\boldsymbol{X}$ es la población y $x$ la muestra.

. . .

* [**Caso univariante**]{.hl-yellow}: $p = 1$ tal que $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 x$.

. . .

* [**Estimación**]{.hl-yellow}: $\widehat{Y}$ en función de $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$. De todas las rectas posibles, queremos la [**recta de regresión mínimos-cuadrados**]{.hl-yellow}: **minimiza la varianza residual** (calculamos sus derivadas parciales, igualamos a cero y resolvemos el sistema). Esos estimadores serán $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$  y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$.

. . .

* [**Predicción**]{.hl-yellow}: podemos estimar o predecir valores con la recta $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_0 X$, siempre y cuando $X = x$ esté dentro del rango de valores de entrenamiento del modelo.

---

## Diagnosis

Hasta ahora no hemos pedido nada a la muestra $\left\lbrace \left( X_i, Y_i \right) \right\rbrace_{i=1}^{n}$, ¿para qué necesitaríamos [**hipótesis**]{.hl-yellow} entonces?

. . .

La razón es que, hasta ahora, lo único que hemos podido realizar es una [**estimación puntual**]{.hl-yellow} de los parámetros, pero dado que dichos estimadores serán variables aleatorias, necesitaremos realizar [**inferencia estadística**]{.hl-yellow} sobre ellos (recuerda: los parámetros son simpleme estimaciones para esa muestra de la población, de forma que dada otra muestra, la recta será distinta).

. . .

Para poder cuantificar la [**variabilidad y precisión de nuestras estimaciones**]{.hl-yellow} necesitaremos que los datos cumplan ciertas [**hipótesis probabilísticas**]{.hl-purple}: lo interesante no es la estimación puntual de los parámetros a partir de la muestra sino lo que [**podamos inferir de ellos a la población**]{.hl-green}

---

## Diagnosis

En el caso de la regresión lineal univariante pediremos [**4 hipótesis**]{.hl-yellow}

. . .

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1 x$

. . .

2. [**Homocedasticidad**]{.hl-green}: el modelo no podrá explicar toda la información (a veces se equivocará por arriba y otras por abajo), pero necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow}, tal que $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \boldsymbol{X} = x \right] = cte < \infty$, que no varíe según aumenta o disminuye la $X$.

. . .

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$

. . .

4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser independientes entre sí (el error en una observación no depende de otras). En particular, serán **incorrelados**

$${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] - E \left[\varepsilon_i \right] E \left[\varepsilon_j \right] = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$$

---


## Diagnosis


Las 4 hipótesis se pueden [**resumir de manera teórica**]{.hl-yellow} en

$$Y | \boldsymbol{X} = x \sim N \left(\beta_0 + \beta_1x, \sigma_{\varepsilon}^2 \right)$$

. . .

Su [**versión muestral**]{.hl-purple} sería simplemente

$$y_i | \boldsymbol{X} = x_i \sim N \left(\beta_0 + \beta_1x_i, \sigma_{\varepsilon}^2 \right), \quad i=1,\ldots,n$$

---

## Inferencia de los parámetros

Las hipótesis nos permiten decir (lo demostraremos más adelante) que los [**parámetros estimados siguen una distribución (condicionada) normal**]{.hl-yellow} de [**media el parámetro**]{.hl-purple}  a estimar y de [**varianza el conocido como standard error**]{.hl-purple}

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

. . .

Por las propiedades de la normal y el teorema central del límite 

$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)}| \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$

---

## Inferencia de los parámetros


$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

Veremos más adelante porqué pero de momento...estos son los $SE$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

¿Qué **propiedades** tienen estos estimadores?


---

## Inferencia de los parámetros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

* [**Insesgados**]{.hl-yellow}: ambos son [**estimadores insesgados**]{.hl-green} ya que la esperanza de la estimación es el valor a estimar. $E \left[ \widehat{\beta}_j \right] = \beta_j$

. . .
 
* [**Precisión vs tamaño muestral**]{.hl-yellow}: cuando $n \to \infty$, sus varianzas tienden a cero, es decir, $SE \left(\widehat{\beta}_j\right)^2 \to^{n \to \infty} 0$ ya que $n$ está dividiendo. Traducción: [**a más datos, mayor precision**]{.hl-green} (menos varianza tendrán los estimadores si repetimos la toma de muestras)


---

## Inferencia de los parámetros


$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$

. . .

* [**Precisión vs var residual**]{.hl-yellow}: cuanto más grande sea la varianza del error $\sigma_{\varepsilon}^{2}$, $SE \left(\widehat{\beta}_j\right)^2$ crecerá (es decir, [**más ruido implicará más imprecisión**]{.hl-red})

. . .

* [**Precisión vs varianza de X**]{.hl-yellow}: cuanto más grande sea la varianza de la predictora $s_{x}^2$, $SE \left(\widehat{\beta}_j\right)^2$ decrecerá, tal que $SE \left(\widehat{\beta}_j\right)^2 \to^{s_{x}^2 \to \infty} 0$. Dicho de otra forma: cuánta [**más información (varianza) contenga nuestra tabla, mayor precisión**]{.hl-green}.

. . .

* [**Precisión vs media X**]{.hl-yellow}: solo afecta a la estimación de $\beta_0$, cuya [**precisión decrece cuando la media aumenta**]{.hl-red}. Esto tiene sentido ya que cuánto más grande en media sean los datos, menos fiable será la predicción para $X=0$.

---

## Inferencia de los parámetros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


Pero tenemos un problema: la [**varianza residual poblacional**]{.hl-yellow} (varianza población del error) $\sigma_{\varepsilon}^{2}$ es [**desconocida**]{.hl-yellow}. Como suele pasar, la [**sustituiremos por un estimador (insesgado)**]{.hl-yellow} de la misma (ya demostaremos...), donde $p$ es el número de variables (lo que R llama `std error` en la salida)

. . .

$$\widehat{\sigma_{\varepsilon}^{2}} = \frac{n}{n-p-1} s_{r}^{2} = \frac{1}{n-p-1} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 =_{p=1} \frac{1}{n-2} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2$$
---

## Inferencia de los parámetros



$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$


¿Qué suele suceder cuando en una normal **desconocemos la varianza real y la sustituimos por su estimador insesgado**?

. . .

Si [**sustituimos el error standard por su estimador**]{.hl-yellow} (sustituyendo la varianza residual por su estimador insesgado), obtenemos que sigue una **t-Student**

$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$
donde $\widehat{SE} \left(\widehat{\beta}_0\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right]$ y $\widehat{SE} \left(\widehat{\beta}_1\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n s_{x}^2}$

---

## Inferencia de los parámetros


```{r}
ajuste_lineal |> summary()
```

* `std error`: es lo que hemos denotado como $\widehat{SE} \left(\widehat{\beta}_j\right)^2$

. . .

* `t value`: es el estadístico $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ que hemos dicho que sigue una t-Student (fíjate que hemos puesto $\beta_j = 0$)

## Inferencia de los parámetros


```{r}
ajuste_lineal |> summary()
```

* `Pr(>|t|)`: representa un p-valor. ¿De qué contraste se te ocurre?

. . .

* `Residual standard error`: es el estimador insesgado de la varianza residual, que hemos denotado como $\widehat{\sigma_{\varepsilon}^{2}}$

---

## Inferencia de los parámetros

:::: columns
::: {.column width="55%"}

$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$

:::

::: {.column width="45%"}

![](img/tablas-intervalos.png)

:::
::::

Si te acuerdas de inferencia, esto implica que los [**intervalos de confianza de nuestros parámetros**]{.hl-yellow} son

$$IC \left(\alpha, \widehat{\beta}_j \right) = \left[\widehat{\beta}_j - t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right), \widehat{\beta}_j + t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right) \right]$$
donde $t_{n-p-1; \alpha/2}$ es el cuantil $1-\alpha/2$ (deja a la izquierda una probabilidad de $1-\alpha/2$).

---


## Inferencia de los parámetros

Podemos calcular los intervalos de confianza de los parámetros en `R` con `confint()`

```{r}
confint(ajuste_lineal, level = 0.99)
confint(ajuste_lineal, level = 0.95)
confint(ajuste_lineal, level = 0.9)
```

---

## Inferencia de los parámetros

[**¿Qué significa realmente un intervalo de confianza?**]{.hl-yellow}

. . .

La **probabilidad de que el parámetro real** caiga dentro [**NO EXISTE**]{.hl-red}: el parámetro es desconocido pero fijo, no aleatorio, así que no sentido calcular su probabilidad.

. . .

Si obtenemos para un parámetro un intervalo $[-1, 1]$ al 95%, no significa que $P(parametro \in [-1, 1]) = 0.95$: significa que [**si tomamos 1000 muestras distintas de la población**]{.hl-yellow} y calculamos para cada una el intervalo de confianza, [**aproximadamente 950 intervalos de confianza contendrán dentro el parámetro real**]{.hl-yellow}

. . .

Un [**intervalo al 95% implica que habrá una frecuencia esperada de 0.95**]{.hl-yellow} de que intervalos que no conocemos (porque se derivan de muestras que no hemos tomado) contengan al parámetro real, pero [**no es la probabilidad de que tu intervalo calculado contenga a dicho parámetro**]{.hl-red}: nos habla la precisión de nuestra metodología de estimación, no del parámetro.


---

## Inferencia de los parámetros

[**Deberes**]{.hl-yellow}. Dada una población normal $\mu = 3$ y $\sigma = 1.2$, crea un código que genere 500 muestras distintas (tamaño $n = 100$ cada una), de manera que para cada una apliques un `t.test()` para calcular un IC para $\mu$. Tras ello, gráfica los intervalos como se muestra en la imagen (haz uso de `geom_segment()`)

```{r}
#| echo: false
mu <- 3
sigma <- 1.2
m <- 500
conf_int <- tibble("id" = 1:m, "low" = NA, "high" = NA)

for (i in 1:m) {
  
  sample <- rnorm(n = 100, mean = mu, sd = sigma)
  hyp_test <- t.test(x = sample)
  conf_int[i, 2] <- hyp_test$conf.int[1]
  conf_int[i, 3] <- hyp_test$conf.int[2]
}
conf_int <-
  conf_int |> 
  mutate(true_param = mu >= low & mu <= high)

ggplot(conf_int) +
  geom_segment(aes(y = id, yend = id, x = low, xend = high,
                   color = true_param, linewidth = true_param)) +
  geom_vline(xintercept = mu) +
  scale_linewidth_manual(values = c(1.1, 0.2)) +
  theme_minimal() +
  guides(linewidth = "none") +
  labs(x = "Valores del intervalo",
       y = "id intervalo",
       color = "¿Contiene mu real?")
```

---


## Inferencia de los parámetros

Y si tenemos inferencia, tenemos contrastes: ¿te acuerdas de los p-valores que devuelve la tabla para cada parámetro?

. . .

Para cada parámetro se realiza un [**contraste de significancia**]{.hl-yellow}: ¿cuánta evidencia hay en mis datos para poder decir que el [**valor estimado de mi parámetro es distinto de 0**]{.hl-yellow}?

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

. . .

El estadístico usado es el [**t value**]{.hl-yellow} que devuelve `R` definido como $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ y que, [**BAJO EL SUPUESTO DE QUE LA HIPÓTESIS NULA SEA CIERTA**]{.hl-purple}, sigue una t-Student $t_{n-p-1}$

---

## Inferencia de los parámetros

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

Este contraste significancia en realidad **contesta a la siguiente pregunta**

¿Tiene mi variable $X_j$ un [**efecto lineal SIGNIFICATIVO**]{.hl-yellow} en mi variable $Y$?

. . .

&nbsp;

Esa pregunta puede ser respondida desde un punto de vista **frecuentista** haciendo uso de los [**p-valores**]{.hl-yellow}: si $p-value < \alpha$, se suele rechazar la nula (rechazamos que la variable no tenga efecto lineal significativo, es decir, aceptamos que sí lo tiene); en caso contrario no se suele rechazar (que **no es lo mismo que aceptarla**). Pero...

---

## Paréntesis: p-valor

[**¿Qué representa un p-valor?**]{.hl-yellow}

. . .

Hablaremos más adelante de ellos pero este es un [**pequeño resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>

. . .

* [**Habla sobre los datos**]{.hl-yellow}. Los p-valores nos pueden servir como indicadores de **cómo de incompatible son los datos respecto a un modelo/hipotesis/explicación asumida**: habla sobre los datos, no sobre la veracidad de la hipotesis nula per se o la probabilidad de que los datos hayan salido tan extremos por azar. 

---

## Paréntesis: p-valor

[**¿Qué representa un p-valor?**]{.hl-yellow}

Hablaremos más adelante de ellos pero este es un [**pequeño resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>



* [**Realidades complejas, decisiones complejas**]{.hl-yellow}. Cuidado con reducir a decisiones binarias realidades complejas. La regla del p-valor es una herramienta más, pero no debe ser la única en la que nos basemos para decidir. [**Otros aspectos a considerar**]{.hl-yellow}: calidad de las medidas, diseño del estudio, evidencia externa en la literatura respecto a la causalidad subyacente, validez de las hipótesis planteadas, etc

---

## Paréntesis: p-valor

[**¿Qué representa un p-valor?**]{.hl-yellow}

Hablaremos más adelante de ellos pero este es un [**pequeño resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>


* [**No hagas cherry picking**]{.hl-red}: muestra de manera transparentes que has probado, que ha salido y que no, y no te quedes solo con lo que sale bien o los p-valores que te convenga (**p-hacking**).

---

## Paréntesis: p-valor

[**¿Qué representa un p-valor?**]{.hl-yellow}

Hablaremos más adelante de ellos pero este es un [**pequeño resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>


* [**Significancia estadística**]{.hl-yellow} no implica significacia real (científica, humana, económica, etc). Obtener [**p-valores muy pequeños no implica un mayor efecto**]{.hl-red} que otros p-valores no tan pequeños (y viceversa). [**Cualquier efecto, por pequeño que sea, puede derivar en p-valores pequeños si el tamaño uestral o la precisión de las medidas es suficiente alto**]{.hl-yellow}, y al contrario (efectos evidentes pueden derivar en p-valores altos si $n$ es pequeño)

---

## Paréntesis: p-valor

[**¿Qué representa un p-valor?**]{.hl-yellow}

Hablaremos más adelante de ellos pero este es un [**pequeño resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>


* [**Alternativas**]{.hl-purple}: intervalos de confianza, de credibilidad, métodos bayesianos, false discovery rates.

&nbsp;

📚 Ver más en <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>, <https://anabelforte.com/2020/11/15/contraste/> y <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877414/>

---

## Inferencia de los parámetros



```{r}
ajuste_lineal |> summary()
```

Si te fijas ambos p-valores están por encima de $\alpha = 0.05$ (umbral adoptado habitualmente) los que nos dice que [**no hay evidencias de los datos sean compatibles con afirmar que haya efectos lineales significativos**]{.hl-yellow}

. . .

¿Y si probamos a quitar $\beta_0$ (es decir, la respuesta está centrada)?

---

## Inferencia de los parámetros

Para ello basta añadir un `-1` antes de las variables predictoras.

```{r}
lm(data = datos, formula = mass ~ -1 + height) |> summary()
```

Fíjate que ahora, amén que [**hay un efecto lineal significativo de la altura**]{.hl-yellow} y de tener $R^2$ mayor, $\beta_1$ tiene una precisión mayor. En este caso [**solo podía quitar uno**]{.hl-yellow} (perderíamos $X$), pero veremos más adelante cómo decidir cuál quitar si tuviésemos varias variables.

# Clases 7 y 8: caso práctico  {#clase-7-8}

[**¿De qué depende el precio del vino?**]{style="color:#444442;"}

---

## Caso práctico

Vamos a poner en práctica lo aprendido con el dataset `wine.csv` disponible en el campus.

```{r}
datos <- read_csv(file = "./datos/wine.csv")
datos
```

---

## Caso práctico


El conjunto de datos está formado por 27 observaciones (cosechas de vino rojo Burdeos) y 7 variables

* `Year`, `Age`: año de la cosecha y número de años en barrica.
* `Price`: precio en 1990-1991 de cada cosecha (en **escala log**)
* `WinterRain`: lluvia (en mm) que cayó ese año en invierno.
* `AGST`: crecimiento medio de la temperatura (en grados Celsius) durante la temporada.
* `HarvestRain`: lluvia (en mm) que cayó ese año durante la cosecha.
* `FrancePop`: población (miles de habitantes) de Francia.

Ver más en detalles en <https://doi.org/10.1080/09332480.1995.10542468>

. . .

[**El objetivo: predecir el precio**]{.hl-yellow}


---

## Caso práctico

```{r}
datos
```

Para predecir el precio vamos a usar (de momento) una [**regresión lineal univariante**]{.hl-yellow}, donde $Y = precio$ y deberemos elegir la predictora $X$ más apropiada.

---

## Pasos a seguir

1. [**Análisis exploratorio inicial**]{.hl-yellow}:
  - ¿Las variables son [**numéricas (continuas)**]{.hl-purple}?
  - ¿Tienen [**problemas de rango**]{.hl-purple} (por ejemplo, pesos negativos)? ¿Tienen datos ausentes?
  - ¿Cómo se [**distribuyen las variables**]{.hl-purple}? Ideas: resúmen numérico, histogramas/densidades, boxplots, gráficos de violín, etc
  - ¿Hay [**datos atípicos**]{.hl-purple}?

---

## Pasos a seguir

2. [**Análisis de dependencia**]{.hl-yellow}:
  - ¿Qué [**predictora está más correlacionada (linealmente) con la variable objetivo**]{.hl-purple} a predecir? ¿Existe otro tipo de dependencia (pendiente implementar en `R`)?
  - ¿Cómo se [**relacionan las predictoras**]{.hl-purple} entre sí? ¿Están correlacionadas? Ideas: matriz de correlaciones, diagramas de dispersión vs Y, corrplots, etc

. . .

3. [**Formulación**]{.hl-yellow} del modelo

. . .

4. [**Fase de estimación**]{.hl-yellow}:
  - ¿Cuánto valen los [**parámetros estimados**]{.hl-purple}? ¿Cómo queda el ajuste?
  - ¿Qué [**interpretación**]{.hl-purple} tienen?


---

## Pasos a seguir

5. [**Fase de diagnosis**]{.hl-yellow} (paquetes `{performance}` y `{olsrr}`):
  - ¿Cumplen los datos las [**hipótesis parámetricas**]{.hl-purple} requeridas para poder hacer inferencia? 
  - ¿Cómo modificar los datos para que se cumplan?
  - Análisis de residuales
  
. . .

6. [**Fase de inferencia**]{.hl-yellow}:
  - ¿Qué [**variabilidad**]{.hl-purple} tienen las estimaciones de nuestro parámetros?
  - ¿Las predictoras/intercepto tienen un [**efecto lineal significativo**]{.hl-purple}?
  - ¿Debemos [**re-entrenar el modelo**]{.hl-purple} sin alguno de ellos?

---

## Pasos a seguir

7. [**Fase de evaluación**]{.hl-yellow}:
  - ¿Es significativo el modelo? [**ANOVA: análisis de la varianza**]{.hl-purple}
  - ¿Qué información de la predictora explica el modelo? [**Parámetros de bondad de ajuste**]{.hl-purple} ($R^2$ por ejemplo)

. . .

8. [**Fase de predicción**]{.hl-yellow}


---

## Análisis exploratorio inicial


1. [**Análisis exploratorio inicial**]{.hl-yellow}:
  - ¿Las variables son [**numéricas (continuas)**]{.hl-purple}?
  - ¿Tienen [**problemas de rango**]{.hl-purple} (por ejemplo, pesos negativos)? ¿Tienen datos ausentes?
  
&nbsp;

Para responder a dichas preguntas lo más sencillos es hacer uso de la función `skim()` del paquete `{skimr}`

```{r}
#| eval: false
library(skimr)
datos |> skim()
```

. . .

En este caso [**no tenemos ausentes ni problemas de codificación**]{.hl-green}


---

## Análisis exploratorio inicial

  - ¿Cómo se [**distribuyen las variables**]{.hl-purple}?
  - ¿Hay [**datos atípicos**]{.hl-purple}?


&nbsp;

Para ello podemos [**visualizar la distribución de cada variable**]{.hl-yellow} haciendo uso de densidades, histogramas y/o boxplots (por ejemplo), pero si queremos ahorrar tiempo en visualizar convertimos nuestro dataset en [**tidydata**]{.hl-yellow}

```{r}
datos_tidy <-
  datos |> 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "values")
datos_tidy
```

---

## Análisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(x = values)) +
  geom_density(aes(color = variable, fill = variable),
               alpha = 0.75) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

---

## Análisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(x = values)) +
  geom_histogram(aes(color = variable, fill = variable),
               alpha = 0.75, bins = 10) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

---

## Análisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(x = values)) +
  geom_histogram(aes(color = variable, fill = variable),
               alpha = 0.75, bins = 10) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

---

## Análisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(y = values, x = variable)) +
  geom_boxplot(aes(color = variable, fill = variable),
               alpha = 0.75) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

[**No hay valores atípicos (respecto a los percentiles)**]{.hl-green}

---

## Análisis exploratorio inicial

Podemos incluso gráficar todos los scatter plot sin transformar los datos con `facet_matrix()` del paquete `{ggforce}`

```{r}
#| code-fold: true
library(ggforce)
ggplot(datos) +
  geom_point(aes(x = .panel_x, y = .panel_y)) +
  facet_matrix(vars(everything())) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  theme_minimal()
  
```

---

## Análisis exploratorio inicial

Podemos también visualizar con un [**scatter plot**]{.hl-yellow} todas las variables y además sus correlaciones, con el paquete `{GGally}` y la función `ggpairs(), sin necesidad de convertir a tidydata.

```{r}
#| code-fold: true
library(GGally)
ggpairs(datos) +
  theme_minimal()
```
   
---


## Análisis de dependencia

2. [**Análisis de dependencia**]{.hl-yellow}:
  - ¿Qué [**predictora está más correlacionada (linealmente) con la variable objetivo**]{.hl-purple} a predecir? ¿Existe otro tipo de dependencia (pendiente implementar en `R`)?
  - ¿Cómo se [**relacionan las predictoras**]{.hl-purple} entre sí? ¿Están correlacionadas? Ideas: matriz de correlaciones, diagramas de dispersión vs Y, corrplots, etc

. . .

Este paso será crucial en el [**contexto multivariante**]{.hl-yellow} pero en este caso simplemente vamos a ver como se relacionan linealmente las predictoras entre sí, y cuál de ellas es la **más adecuada para predecir linealmente** `precio`

---

## Análisis de dependencia

El primer paso es la [**matriz de correlaciones**]{.hl-yellow} con la función `cor()` o con la función `correlate()` del paquete `{corrr}` (importa en tibble más visual)

```{r}
library(corrr)
datos |> correlate()
```

. . .

* [**Respecto a Y**]{.hl-yellow}: predictoras con mayor cor lineal son `AGST` (más calor, menos cosechas, sube el precio) y `HarvestRain` (más lluvias, más cosechas, baja el precio, ¡el signo importa!)

* [**Dependencia entre predictoras**]{.hl-yellow}: las variables `Age`, `Year` y `FrancePop` presentan la misma información.


---

## Análisis de dependencia

También podemos usar `corrplot()` del paquete `{corrplot}`, al que le pasamos una matriz de correlaciones clásica y nos la [**visualiza**]{.hl-yellow}.

```{r}
library(corrplot)
datos |> cor() |> corrplot(method = "ellipse")
```

Puedes ver distintas opciones de visualización en <https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html>

---

## Análisis de dependencia

Otra opción es [**visualizar con un scatter plots todas las predictoras vs Y**]{.hl-yellow}, pivotando antes nuestro dataset (solo pivotamos las predictoras)

:::: columns
::: {.column width="65%"}

```{r}
#| code-fold: true
datos_tidy <-
  datos |> pivot_longer(cols = -Price, names_to = "variable",
                        values_to = "values")
ggplot(datos_tidy, aes(x = values, y = Price)) + 
  geom_point(aes(color = variable), alpha = 0.7) +
  geom_smooth(method = "lm") +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal()
```

:::

::: {.column width="35%"}

No solo comprobamos que las rectas con más pendiente son `AGST` y `HarvestRain`, además los puntos parecen poder ajustarse a una recta sin otro patrón identificable.

:::
::::

Esto es importante hacerlo ya que debemos [**descartar posibles correlaciones espúreas**]{.hl-red} (ver ejemplo datasaurus)

---

## Formulación del modelo

Una vez que hemos decidido que dos predictoras usaemos, vamos por tanto a plantear [**dos modelos univariantes**]{.hl-yellow}

$$Price = \beta_0 + \beta_1*AGST + \varepsilon$$
$$Price = \beta_0 + \beta_1*HarvestRain + \varepsilon$$

---

## Fase de estimación


4. [**Fase de estimación**]{.hl-yellow}:
  - ¿Cuánto valen los [**parámetros estimados**]{.hl-purple}? ¿Cómo queda el ajuste?
  - ¿Qué [**interpretación**]{.hl-purple} tienen?


&nbsp;

Para ello ejecutaremos ambos modelos con `lm()`

```{r}
ajuste_AGST <- lm(data = datos, formula = Price ~ AGST)
ajuste_harvest <- lm(data = datos, formula = Price ~ HarvestRain)
```


---

## Fase de estimación

[**Ajuste con AGST**]{.hl-yellow}

```{r}
ajuste_AGST |> summary()
```

* $\beta_0=$ `r round(ajuste_AGST$coefficients[1], 3)`: predicción del precio cuando $AGST = 0$ es de -3 (recuerda que está en escala logartímica)

* $\beta_1=$ `r round(ajuste_AGST$coefficients[2], 3)`: por cada grado de aumento, el precio sube `r round(ajuste_AGST$coefficients[2], 3)` (escala log).

---

## Fase de estimación

[**Ajuste con AGST**]{.hl-yellow}

```{r}
#| echo: false
ajuste_AGST |> summary()
```

* [**Residuales**]{.hl-yellow}: además de media cero, parecen presentar una distribución simétrica con la mediana en torno al cero. Además se tiene que $\widehat{\sigma}_{\varepsilon}^{2}=\frac{1}{n-2}\sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = 0.4819$ (estimador insesgado de la varianza residual) y $R^2 = 0.4456$ (bondad de ajuste)


---

## Fase de estimación

[**Ajuste con harvestRain**]{.hl-yellow}


```{r}
ajuste_harvest |> summary()
```

* $\beta_0=$ `r round(ajuste_harvest$coefficients[1], 3)`: predicción del precio cuando la lluvia fue nula es de 7.679 (recuerda que está en escala logartímica)

* $\beta_1=$ `r round(ajuste_harvest$coefficients[2], 3)`: por cada litro de lluvia, precio baja `r round(1000*ajuste_AGST$coefficients[2], 3)` (escala log).

---

## Fase de estimación

[**Ajuste con harvestRain**]{.hl-yellow}

```{r}
#| echo: false
ajuste_harvest |> summary()
```

* [**Residuales**]{.hl-yellow}: además de media cero, parecen presentar una distribución simétrica con la mediana en torno al cero. Además se tiene que $\widehat{\sigma}_{\varepsilon}^{2}=\frac{1}{n-2}\sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = 0.5577$ (algo más grande que el otro ajuste) y $R^2 = 0.2572$ (algo más pequeño que el otro ajuste) -> de momento es mejor el primer modelo.

---

## Fase de diagnosis

5. [**Fase de diagnosis**]{.hl-yellow} (paquetes `{performance}` y `{olsrr}`):
  - ¿Cumplen los datos las [**hipótesis parámetricas**]{.hl-purple} requeridas para poder hacer inferencia? 
  - ¿Cómo modificar los datos para que se cumplan?
  - Análisis de residuales
  
&nbsp;

Recuerda que [**necesitamos verificar antes las hipótesis**]{.hl-yellow} para poder hacer inferencia con los parámetros, así que vamos a ello con el paquete `{performance}` y el paquete `{olsrr}`

---

## Fase de diagnosis

```{r}
library(performance)
check_model(ajuste_AGST)
```

---

## Diagnosis: linealidad

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1 x$


```{r}
#| code-fold: true
check_model(ajuste_AGST)
```

Si te fijas el gráfico que se refiere a ello está [**visualizando residuales vs valores estimados**]{.hl-yellow}: está volviendo a plantear un segundo modelo de regresión donde ahora $\widehat{\varepsilon}_i = \gamma_0 + \gamma_1 \widehat{y}_i$

---

## Diagnosis: linealidad

```{r}
linealidad <- lm(data = tibble("fitted" = ajuste_AGST$fitted.values,
                               "residuals" = ajuste_AGST$residuals),
                 formula = residuals ~ fitted)
linealidad |> summary()
```

Si te fijas [**ambos parámetros no son significativamente distintos de 0**]{.hl-green}: no presentan una tendencia (lineal al menos)

---

## Diagnosis: linealidad

Más adelante probaremos alguna otra cosa pero de momento nos basta con eso. También podemos [**visualizar nosotros ese scatter plot residuales vs estimaciones**]{.hl-yellow}

```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "residuals" = ajuste_AGST$residuals),
       aes(x = fitted, y = residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm") +
  theme_minimal()
```

---

## Diagnosis: homocedasticidad

2. [**Homocedasticidad**]{.hl-green}: necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow}, tal que $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \boldsymbol{X} = x \right] = cte < \infty$.

```{r}
check_heteroscedasticity(ajuste_AGST)
```

. . .

El gráfico titulado `Homogeneity of variance` nos visualiza la raíz cuadrada del valor absoluto de los residuos estandarizados frente a las predicciones (se conoce como [**gráfico de escala-localización**]{.hl-yellow})

---

## Diagnosis: homocedasticidad

Si visualizamos los [**residuales**]{.hl-yellow} deberían estar en torno a 0, dentro de una banda constante (varianza constante)


```{r}
#| code-fold: true
ggplot(tibble("id" = 1:length(ajuste_AGST$residuals),
              "residuals" = ajuste_AGST$residuals),
       aes(x = id, y = residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

---


## Diagnosis: homocedasticidad

Si visualizamos el gráfico de [**escala-localización**]{.hl-yellow} deberíamos obtener un diagrama de dispersión cuya recta de regresión saliese casi plana en torno al 1.


```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "sqrt_std_residuals" = sqrt(abs((ajuste_AGST$residuals - mean(ajuste_AGST$residuals)) / sd(ajuste_AGST$residuals)))),
       aes(x = fitted, y = sqrt_std_residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

Según el gráfico no deberíamos asumir homocedasticidad. **¿Por qué el contraste no la rechaza?**

---

## Diagnosis: homocedasticidad


```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "sqrt_std_residuals" = sqrt(abs((ajuste_AGST$residuals - mean(ajuste_AGST$residuals)) / sd(ajuste_AGST$residuals)))),
       aes(x = fitted, y = sqrt_std_residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

Con el poco tamaño muestral que tenemos, es complicado tener evidencias que refuten la hipótesis nula (y el gráfico puede estar parcialmente diseñado). Por eso es la [**hipótesis más difícil de cumplir**]{.hl-yellow}. Lo importante es que en [**la recta de regresión al dibujar los residuos no se aprecia una banda cuya anchura se modifique groseramente**]{.hl-green}, más o menos constante


---

## Diagnosis: normalidad

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$

Con la función `ols_test_normality()` del paquete `{olsrr}` podemos obtener diferentes contrastes de normalidad

```{r}
library(olsrr)
ols_test_normality(ajuste_AGST)
```

Nos centraremos en los contrastes de `Shapiro-Wilk`, `Kolmogorov-Smirnov` y `Anderson-Darling`: [**no se rechaza normalidad**]{.hl-yellow}

---

## Diagnosis: normalidad

Además del contraste podemos visualizar con `stat_qq()` y `stat_qq_line()` el conocido como [**Q-Q plot**]{.hl-yellow}: enfrenta los cuantiles de una muestra con los cuantiles de una normal teórica, teniendo que **obtener los puntos en torno a una recta** (especilamente en el centro).


```{r}
#| code-fold: true
ggplot(tibble("residuals" = ajuste_AGST$residuals)) +
  stat_qq(aes(sample = residuals)) +
  stat_qq_line(aes(sample = residuals))
```


---

## Diagnosis: independencia

4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser independientes entre sí (el error en una observación no depende de otras). En particular, serán **incorrelados**

$${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] - E \left[\varepsilon_i \right] E \left[\varepsilon_j \right] = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$$

```{r}
check_autocorrelation(ajuste_AGST)
```

Por último, `check_autocorrelation()` comprueba como efectivamente los [**residuales/errores son independientes**]{.hl-yellow}, haciendo un test de autocorrelación (nos tiene que salir lo contrario a una serie temporal, que el error i no depende del i-1).

---

## Diagnosis: independencia

Otra forma de verlo es [**visualizando los residuos respecto a su versión con retardo**]{.hl-yellow} (por ejemplo, $\left(\widehat{\varepsilon}_1, \widehat{\varepsilon}_2, \widehat{\varepsilon}_3, \ldots, \widehat{\varepsilon}_{n-1} \right)$ vs $\left(\widehat{\varepsilon}_2, \widehat{\varepsilon}_3, \ldots, \widehat{\varepsilon}_n \right)$ 

```{r}
#| code-fold: true
ggplot(tibble("lag1" = ajuste_AGST$residuals[-length(ajuste_AGST$residuals)],
              "residuals" = ajuste_AGST$residuals[-1]),
       aes(x = residuals, y = lag1)) +
  geom_point(size = 3, alpha = 0.7) + 
  geom_smooth(method = "lm") + 
  theme_minimal()
```

---

## Fase de diagnosis

```{r}
check_model(ajuste_AGST)
```

En nuestro caso se cumplen todas las hipótesis (algunas más fuertemente que otras).

[**Repite el proceso con el otro modelo**]{.hl-yellow}

---

## Fase de diagnosis

```{r}
check_predictions(ajuste_AGST)
```

Nos faltan dos gráficas por comentar:

* `Posterior Predictive Checks`: [**simula distintas variables respuesta**]{.hl-yellow} suponiendo que el modelo fuese cierto (añadiendo ruido aleatorio) y lo compara con la muestra. [**Si lo observado se distancia mucho de las simulaciones**]{.hl-red} es que el modelo planteado no ajusta bien a la muestra.

---

## Fase de diagnosis

```{r}
check_outliers(ajuste_AGST)
```

* `Influential Observations`: nos permite identificar [**observaciones influyentes**]{.hl-yellow}, marcando aquellas (con su id de fila) que se salgan fuera de la banda definida por la conocida como **distancia de Cook** denotada como $D_i$ (realiza, para cada observación, la suma de todos los cambios de la regresión cuando la observación $i$ es retirada: si hay muchos cambios al cambiar una observación, es que era muy influyente)

. . .

Diferencia dos tipos: 

* [**outliers**]{.hl-yellow}: valor atípico de la **respuesta** pudiendo perturbar la varianza residual
* [**high-leverage points**]{.hl-yellow}: valor atípico en alguna de las **predictoras**

---

## Fase de inferencia

6. [**Fase de inferencia**]{.hl-yellow}:
  - ¿Qué [**variabilidad**]{.hl-purple} tienen las estimaciones de nuestro parámetros?
  - ¿Las predictoras/intercepto tienen un [**efecto lineal significativo**]{.hl-purple}?
  - ¿Debemos [**re-entrenar el modelo**]{.hl-purple} sin alguno de ellos?

&nbsp;

Una vez verificadas las hipótesis lo que haremos será [**inferir conclusiones de la población en función de la muestra**]{.hl-yellow}

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Variabilidad**]{.hl-yellow} de las estimaciones de nuestro parámetros

  - $\widehat{SE} \left( \widehat{\beta}_0 \right)$ igual a 2.344 por lo que (aprox) $\widehat{\beta_0} \sim N(-3.547, 2.344)$
  - $\widehat{SE} \left( \widehat{\beta}_1 \right)$ igual a 0.143  por lo que (aprox) que $\widehat{\beta_1} \sim N(0.643, 0.143)$
  

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Estadístico**]{.hl-yellow} del contraste

  - $\frac{\widehat{\beta}_0 - 0}{\widehat{SE} \left( \widehat{\beta}_0 \right)}$ igual a -1.5 (valor que tendrías que buscar en las tablas a mano)
  - $\frac{\widehat{\beta}_1 - 0}{\widehat{SE} \left( \widehat{\beta}_1 \right)}$ igual a 4.483 (valor que tendrías que buscar en las tablas a mano)
  
  
  
---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Efecto (lineal)**]{.hl-yellow}: si nos fijamos en la tabla, el p-valor de $\beta_0$ es 0.146052. Si adoptamos $\alpha = 0.05$ como suele ser habitual, el contraste $H_0:~\beta_0 = 0$ vs $H_1:~\beta_0 \neq 0$ nos dice que [**no podemos rechazar de forma significativa la hipótesis nula**]{.hl-red} (no sucede con $\beta_1$, si sucediese no habría modelo)

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

[**¿Y si quitamos dicho parámetro?**]{.hl-yellow}

. . .

Para quitarlo añadimos un -1 al modelo

```{r}
ajuste_AGST_sin_beta0 <- lm(data = datos, formula = Price ~ -1 + AGST)
```
 
---

## Re-aprendiendo

```{r}
ajuste_AGST_sin_beta0 |> summary()
```

* La [**bondad de ajuste**]{.hl-yellow} ha pasado de $R^2 = 0.446$ a $R^2 = 0.9953$

* La [**variabilidad de la estimación**]{.hl-yellow} $\widehat{SE} \left( \widehat{\beta}_1 \right)$  ha pasado de 0.143 a 0.005757.

---

## Comparar modelos

Aunque no hemos hablado en profundidad de las **métricas de evaluación** podemos [**comparar los modelos**]{.hl-yellow} con `compare_performance()` del paquete `{performance}`

```{r}
#| echo: false
ajuste_harvest_sin_beta0 <- lm(data = datos, formula = Price ~ -1 + HarvestRain)
```
```{r}
compare_performance(ajuste_AGST, ajuste_AGST_sin_beta0,
                    ajuste_harvest, ajuste_harvest_sin_beta0)
```

# Clases 9: evaluación y predicción {#clase-9}

[**¿Cómo se puede evaluar un modelo? ¿Qué métricas existen? ¿Cómo predecir?**]{style="color:#444442;"}

---

## Repitamos el proceso


Para interiorizar lo aprendido, vamos a repetir todo el proceso con el conjunto `datos_linearreg.csv` (las variables predictoras representan diferentes variables meteorológicas y la variable objetivo `y` la temperatura media en primavera, para distintas ciudades).

```{r}
datos <- read_csv(file = "./datos/datos_linearreg.csv")
datos
```

---

## Regresión lineal

Debes seguir los siguientes pasos de la manera más detallada posible

1. [**Análisis exploratorio inicial**]{.hl-yellow} tanto numérico como visualizando. ¿Son numéricas sin problemas de codificación? ¿Cómo se distribuyen? ¿Hay datos atípicos?

2. [**Análisis de dependencia**]{.hl-yellow}. ¿Qué predictora correlaciona más con la objetivo? ¿Cómo se relacionan las predictoras entre sí?

3. [**Formulación**]{.hl-yellow} del modelo

4. [**Fase de estimación**]{.hl-yellow}. ¿Cuáles son los parámetros? ¿Cómo se interpretan?

5. [**Fase de diagnosis**]{.hl-yellow}

6. [**Fase de inferencia**]{.hl-yellow}. ¿Qué variabilidad tiene la estimación? ¿Hay efecto significativo? 


---

## Fase de evaluación

```{r}
ajuste_lineal <- lm(data = datos, formula = y ~ x1)
ajuste_lineal |> summary()
```

Teníamos pendiente la fase final: [**fase de evaluación**]{.hl-yellow}

--- 

## Fase de evaluación

7. [**Fase de evaluación**]{.hl-yellow}:
  - ¿Es significativo el modelo? [**ANOVA: análisis de la varianza**]{.hl-purple}
  - ¿Qué información de la predictora explica el modelo? [**Parámetros de bondad de ajuste**]{.hl-purple} ($R^2$ por ejemplo)
  - ¿Qué otras métricas o herramientas podemos usar para [**cuantificar la calidad predictora de nuestro ajuste**]{.hl-purple}
  
. . . 

Una de las herramientas más útiles para evaluar nuestro modelo es [**enfrentar los valores ajustados con los valores reales**]{.hl-yellow} (dado que los conocemos al ser aprendizaje supervisado)

---

## Fase de evaluación


```{r}
#| code-fold: true
ggplot(tibble("y" = datos$y, "y_est" = ajuste_lineal$fitted.values),
       aes(x = y, y = y_est)) +
  geom_point(size = 1.2, alpha = 0.75) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  labs(x = "Valores reales", y = "Valores estimados")
```

En el gráfica podemos ver como los [**valores reales vs estimados**]{.hl-green} están muy cercanos a la diagonal: el error cometido es muy pequeño.

---

## Fase de evaluación

Podemos considerar algunas [**métricas para cuantificar el acierto del modelo**]{.hl-yellow}

. . .

* [**SSE**]{.hl-red} (sum of squared errors): definido como la suma de errores al cuadrado

$$SSE = \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 = \sum_{i=1}^{n} \left( Y_i - \widehat{Y}_i \right)^2$$
Fíjate que dado que $\widehat{\sigma}_{\varepsilon} = \widehat{\sigma}_{r} = \frac{n}{n-p-1} s_{r}^{2} =  \frac{1}{n-p-1} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2$ (el que `R` llama `Residual standard error`), tenemos que $SSE = (n-p-1)\widehat{\sigma}_{\varepsilon}$

. . .

* [**MSE**]{.hl-red}: media de lo anterior $MSE = s_{r}^{2} = \frac{1}{n} SSE  = \frac{1}{n} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 = \frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}$

. . .

Ambas métricas nos hablan de la [**varianza del error**]{.hl-red}, es decir, de lo que el [**modelo no es capaz de explicar**]{.hl-red}

---


## Fase de evaluación



* [**SSR**]{.hl-green} (regressions sum of squares): definido como la suma de las desviaciones de cada predicción a su media (al tener estimadores insesgados, la media de las estimaciones $\overline{\widehat{Y}}$ es la misma que la de la variable a estimar $\overline{Y}$)

$$SSR = \sum_{i=1}^{n} \left( \overline{Y} - \widehat{Y}_i \right)^2 = \sum_{i=1}^{n} \left( \overline{\widehat{Y}}_i - \widehat{Y}_i \right)^2$$

. . .


* [**MSR**]{.hl-green}: media de lo anterior $MSR = s_{\widehat{y}}^2 = \frac{1}{n} SSR  = \frac{1}{n} \sum_{i=1}^{n} \left( \overline{Y} - \widehat{Y}_i \right)^2$

. . .

Ambas métricas nos hablan de la [**variación de Y en torno a la regresión**]{.hl-green}, es decir, la variación de $\overline{Y}$ que es explicada por la media condicional estimada $(Y_i|X=x_i) \sim \widehat{\beta}_0 + \widehat{\beta}_1 X_i$, cuantifica la [**información de Y explicada por el modelo**]{.hl-green}

---

## Fase de evaluación


* [**SST**]{.hl-yellow} (total sum of squares): definido como la suma de las desviaciones de la variable objetivo $Y$ a su media.

$$SST =  \sum_{i=1}^{n} \left( Y_i - \overline{Y} \right)^2 $$

. . .

* [**Varianza muestral de Y**]{.hl-yellow}: la media de lo anterior (la varianza de $Y$)

$$s_{y}^2= \frac{1}{n} SST  = \frac{1}{n}\sum_{i=1}^{n} \left( Y_i - \overline{Y} \right)^2$$



. . .


Ambas métricas nos hablan de la [**variación total de Y**]{.hl-yellow}, es decir, la [**información total de nuestra variable objetivo**]{.hl-yellow}

---

## ANOVA

Así tenemos 3 tipos de métricas:

* `SST` y $s_{y}^2$: el [**total de info a explicar**]{.hl-yellow}

* `SSR` y $MSR$: el [**total de info explicada por el modelo**]{.hl-green}

* `SSE` y $MSE$: el [**total de info NO explicada por el modelo**]{.hl-red} (a veces se usa la raíz cuadrada del MSE, conocido como $RMSE$, o el $MAE$, tomando valor absoluto en los errores).

. . .

Se pueden demostrar que, [**SOLO EN EL CASO LINEAL**]{.hl-purple}, desarrollando el sumatorio $SST =  \sum_{i=1}^{n} \left( Y_i - \overline{Y} \right)^2 = \sum_{i=1}^{n} \left( \left(Y_i - \widehat{Y}_i \right) +  \left(  \widehat{Y}_i - \overline{Y} \right) \right)^2$ se llega a que

[**SST**]{.hl-yellow} =  [**SSR**]{.hl-green} + [**SSE**]{.hl-red}

---


## ANOVA

[**SST**]{.hl-yellow} =  [**SSR**]{.hl-green} + [**SSE**]{.hl-red}


. . .

[$s_{y^2}$]{.hl-yellow} = [$s_{\widehat{y}}^2$]{.hl-green} + [$s_{r}^2$]{.hl-red} (equivalente, [$s_{y^2}$]{.hl-yellow} = [$s_{\widehat{y}}^2$]{.hl-green} + [$\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}$]{.hl-red})

. . .

Esta decomposición (solo se cumple en el caso lineal) se conoce como [**ANOVA o análisis de la varianza**]{.hl-yellow} y podemos hacerlo en `R` con `aov()` o `anova()`

:::: columns
::: {.column width="45%"}

```{r}
ajuste_lineal |> aov()
```

:::

::: {.column width="55%"}

```{r}
ajuste_lineal |> anova()
```

:::
::::

---

## ANOVA

```{r}
ajuste_lineal |> aov()
```

| Terms | x1 (predictora) | Residuals |
|:---------:|:-----:|:------:|
| Sum of Squares  | SSR   |    SSE | 
| Deg. of Freedom  (grados libertad)  | p  |  n - p - 1 |

&nbsp;

`Residual standard error`: $\widehat{\sigma}_{\varepsilon}^{2}= \frac{n}{n-p-1} s_{r}^{2} = \frac{1}{n-p-1} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2}$

---

## ANOVA

```{r}
ajuste_lineal |> anova()
```

|  | Df (grados) | Sum Sq | Mean Sq | F value | Pr(>F) |
|:---------:|:-----:|:------:|:------:|:------:|:------:|
| x1  | p  | SSR | $\frac{SSR}{p}$ | $F-value = \frac{\frac{SSR}{p}}{\frac{SSE}{n-p-1}}$ | p-valor F test
| Residuals  | n - p - 1 | SSE | $\widehat{\sigma}_{\varepsilon}^{2} = \frac{SSE}{n-p-1}$ | | |

El F-value es el estadítico  $F = \frac{\frac{SSR}{p}}{\frac{SSE}{n-p-1}} \sim F_{p, n-p-1}$ asociado al [**contraste de significación global**]{.hl-yellow}

$$H_0:~\beta_1 = \ldots = \beta_p = 0 \quad vs \quad H_1:~\text{existe al menos un} \quad \beta_j \neq 0$$

---

## ANOVA

```{r}
ajuste_lineal |> anova()
```


$$H_0:~\beta_1 = \ldots = \beta_p = 0 \quad vs \quad H_1:~\text{existe al menos un} \quad \beta_j \neq 0$$

El contraste pretende responder a: [**¿existe una dependencia lineal entre $Y$ y el CONJUNTO de predictoras?**]{.hl-yellow} (global, no parámetro a parámetro).

. . .

Si se [**rechaza**]{.hl-yellow} significa que [**existe al menos un predictor cuyo efecto LINEAL sobre Y es significativo**]{.hl-yellow}.

. . .

[**Importante**]{.hl-red}: en el caso de la reg. lineal univariante, $F-value$ y $p-value$ del ANOVA es equivalente al $t-value$ y $p-value$ del contraste de significación para $\beta_1$ (ya que...no hay más).

---

## Bondad de ajuste

El conocido como $R^2$ o [**bondad de ajuste**]{.hl-yellow} o el [**coeficiente de determinación**]{.hl-yellow} es una forma sencilla, y relacionada con el ANOVA, para cuantificar la [**cantidad de información explicada por el modelo**]{.hl-yellow}

. . .

De hecho es, literal, un [**ratio de información explicada**]{.hl-yellow} (lo que `R` llama `Multiple R-squared`, ya hablaremos de su versión ajustada en regresión multivariante)

$$R^2 = \text{Ratio info explicada} = \frac{SSR}{SST}$$

. . .

En el caso lineal, por lo visto en el ANOVA

$$R^2= \frac{SSR}{SST} = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST} = 1 - \frac{s_{r}^{2}}{s_{y}^{2}} =  1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^{2}}{s_{y}^{2}}$$

. . .

Por definición, la [**bondad de ajuste está entre 0 y 1**]{.hl-yellow}.

---

## Bondad de ajuste

[**Deberes: demuestra que $R^2 = r_{y \widehat{y}}^2$ y que en el caso de $p=1$ coincide además con $R^2 = r_{x,y}^2$**]{.hl-yellow}




$$R^2 = r_{y \widehat{y}}^2 =_{p=1} r_{x,y}^2$$

. . .

La bondad de ajuste tiene un [**problema importante**]{.hl-red}: no solo depende del modelo sino que también de los datos. ¿De qué depende? ¿Por qué es [**peligroso usar ciegamente $R^2$ para decidir**]{.hl-red}?



---

## Bondad de ajuste

$$R^2= \frac{SSR}{SST} = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST} = 1 - \frac{s_{r}^{2}}{s_{y}^{2}} =  1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^{2}}{s_{y}^{2}}$$

¿De qué depende?

. . .

* [**Más predictoras**]{.hl-yellow} implica que [**$R^2$ crece**]{.hl-yellow}, ¡incluso aunque dichas [**predictoras no sean significativas**]{.hl-red}! (esto lo arreglaremos con el $R^2$ ajustado en el futuro)

* [**Más ruido, menor $R^2$**]{.hl-yellow}

* [**Ignora si el modelo cumple las hipótesis**]{.hl-yellow}: un modelo con un alto $R^2$ puede dar predicciones nefastas.
  
  
---

## Predictoras vs R2

Vamos a repetir pero [**añadiendo 5 y 22 variables más**]{.hl-yellow}, sin efecto lineal con $Y$.

```{r}
datos_extras <-
  tibble("y" = datos$y, "x1" = datos$x1, "x2" = rnorm(1e3), "x3" = rnorm(1e3),
         "x4" = rnorm(1e3), "x5" = rnorm(1e3), "x6" = rnorm(1e3), "x7" = rnorm(1e3),
         "x8" = rnorm(1e3), "x9" = rnorm(1e3), "x10" = rnorm(1e3), "x11" = rnorm(1e3),
         "x12" = rnorm(1e3), "x13" = rnorm(1e3), "x14" = rnorm(1e3), "x15" = rnorm(1e3),
         "x16" = rnorm(1e3), "x17" = rnorm(1e3), "x18" = rnorm(1e3), "x19" = rnorm(1e3),
         "x20" = rnorm(1e3), "x21" = rnorm(1e3), "x22" = rnorm(1e3), "x23" = rnorm(1e3))
ajuste_6pred <- lm(data = datos_extras, formula = y ~ x1 + x2 + x3 + x4 + x5 + x6)
ajuste_23pred <- lm(data = datos_extras, formula = y ~ .)
compare_performance(ajuste_lineal, ajuste_6pred, ajuste_23pred)
```

Con `compare_performance()` podemos comparar métricas de modelos.

---

## Ruido vs R2

Hemos dicho que el ruido afecta...¿qué crees que **pasará si fijamos la parte determinística y solo modificamos el ruido**? Vamos a [**simular 6 modelos**]{.hl-yellow}, con exactamente los mismos $\beta_0$ y $\beta_1$ (es decir, **mismo ajuste**) pero con diferente varianza en el ruido (supongamos que $X \sim N(3, 1.5)$).

$$\text{Modelo 1: } Y = -1.2 + 3.2X + N(0, 0.25)$$
$$\text{Modelo 2: } Y = -1.2 + 3.2X + N(0, 1)$$

$$\text{Modelo 3: } Y = -1.2 + 3.2X + N(0, 1.5)$$

$$\text{Modelo 4: } Y = -1.2 + 3.2X + N(0, 2)$$

$$\text{Modelo 5: } Y = -1.2 + 3.2X + N(0, 4)$$


$$\text{Modelo 6: } Y = -1.2 + 3.2X + N(0, 8)$$

---

## Ruido vs R2


```{r}
#| code-fold: true
x <- rnorm(n = 1000, mean = 3, sd = 1.5)
eps1 <- rnorm(n = 1000, mean = 0, sd = 0.25)
eps2 <- rnorm(n = 1000, mean = 0, sd = 1)
eps3 <- rnorm(n = 1000, mean = 0, sd = 1.5)
eps4 <- rnorm(n = 1000, mean = 0, sd = 2)
eps5 <- rnorm(n = 1000, mean = 0, sd = 4)
eps6 <- rnorm(n = 1000, mean = 0, sd = 8)
datos <- tibble("x" = x, "y1" = -1.2 + 3.2*x + eps1,
                "y2" = -1.2 + 3.2*x + eps2, "y3" = -1.2 + 3.2*x + eps3,
                "y4" = -1.2 + 3.2*x + eps4, "y5" = -1.2 + 3.2*x + eps5,
                "y6" = -1.2 + 3.2*x + eps6)
ajuste_1 <- lm(data = datos, formula = y1 ~ x)
ajuste_2 <- lm(data = datos, formula = y2 ~ x)
ajuste_3 <- lm(data = datos, formula = y3 ~ x)
ajuste_4 <- lm(data = datos, formula = y4 ~ x)
ajuste_5 <- lm(data = datos, formula = y5 ~ x)
ajuste_6 <- lm(data = datos, formula = y6 ~ x)
```

```{r}
compare_performance(ajuste_1, ajuste_2, ajuste_3, ajuste_4, ajuste_5, ajuste_6)
```

---

## Ruido vs R2

En todos los casos el [**ajuste debe ser (aprox) el mismo**]{.hl-yellow} ya que el modelo de regresión busca [**modelizar los efectos lineales no aleatorios**]{.hl-yellow} entre la variable objetivo y los predictores.

. . .

[**Moraleja**]{.hl-green}: tener un $R^2$ no implica que el modelo sea malo, ya que la [**cantidad de información no modelizable**]{.hl-yellow} puede deberse a una [**cantidad alta de ruido (algo aleatorio no predecible)**]{.hl-red}. Por ello es importante usar más herramientas que un mero coeficiente para valorar un ajuste (por ejemplo, en campos como la sociología o la economía la bondad de ajuste será generalmente bajo)

. . .

&nbsp;

[**Deberes**]{.hl-yellow}: ¿cómo ilustrar gráficamente que a mayor varianza del ruido, menor es $R^2$? Diseña un estudio de simulación para ello con distintos modelos y gráfica la caída de $R^2$.

---

## Diagnosis vs R2

Entonces, si tenemos un modelo con un alto $R^2$, [**¿no hace falta que cumpla las hipótesis?**]{.hl-yellow}

. . .

Vamos a simular un modelo que [**incumple**]{.hl-red}

* Linealidad

* Homocedasticidad

. . .

La variable predictora $x_i = 0.01 + 0.01*(i-1)$ ($i = 1,\ldots, n = 200$) será la siguiente

```{r}
x <- seq(0.01, 2, l = 200)
```


[**¿Cómo podríamos crear una $Y$ cuya relación con $X$ sea no lineal?**]{.hl-yellow}

---

## Diagnosis vs R2

[**¿Cómo podríamos crear una $Y$ cuya relación con $X$ sea no lineal?**]{.hl-yellow}

Tenemos muchas maneras, por ejemplo:

$$Y = X + X^2 + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

$$Y = \log(X^2) - cos(X) + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

$$Y = \frac{1}{X + 1} + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

$$Y = 1 - 2X(1 + 0.25 \sin(4 \pi X)) + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

. . .

Simula este último modelo (con $\sigma_{\varepsilon} = 0.5$) y realiza el ajuste

---

## Diagnosis vs R2

```{r}
x <- seq(0.15, 2, l = 200)
eps <- rnorm(n = 200, sd = 0.5)
y <- 1 - 2 * x * (1 + 0.25*sin(4 * pi * x)) + eps
datos <- tibble("y" = y, "x" = x)
ajuste_lineal <- lm(data = datos, formula = y ~ x)
ajuste_lineal |> summary()
```

. . .

[**¿Cómo podríamos hacer que la hipótesis de homocedasticidad no se cumpla**]{.hl-yellow}


---

## Diagnosis vs R2

Vamos a considerar que la [**varianza del error no es cte, crece según aumenta x**]{.hl-yellow}

$$y_i = f(x_i) + \varepsilon_i, \quad \varepsilon_i \sim N(0, \sigma_{\varepsilon,i}), \quad \sigma_{\varepsilon,i} = 0.25 * x_{i}^2 \quad i=1, \ldots, 200$$

```{r}
#| code-fold: true
x <- seq(0.01, 2, l = 200)
eps <- rnorm(n = 200, sd = 0.25 * x^2)
y <- 1 - 2 * x * (1 + 0.25*sin(4 * pi * x)) + eps
datos <- tibble("y" = y, "x" = x, "sigma" =  0.25 * x^2)
ajuste_lineal <- lm(data = datos, formula = y ~ x)
ajuste_lineal |> summary()
```

---

## Diagnosis vs R2

```{r}
#| code-fold: true
ggplot(datos, aes(x = x, y = y)) +
  geom_point(aes(color = sigma), size = 3, alpha = 0.75) +
  geom_smooth(method = "lm") +
  theme_minimal()
```

Aunque el $R^2$ es bastante alto, el modelo no tiene sentido, dando [**estimaciones cada vez más erradas**]{.hl-red} según nos movemos hacia la derecha en el eje X.


---

## Cosicas a examen

Algunas cosas que podrían caer en la primera entrega:

. . .

* Realizar un [**análisis exploratorio inicial**]{.hl-yellow} lo más completo posible. Elegir de manera justificada [**la mejor predictora**]{.hl-yellow}

. . .

* Saber [**ajustar e INTERPRETAR**]{.hl-yellow} un modelo de regresión.

. . .

* Realizar e interpretar la [**fase de diagnosis**]{.hl-yellow} lo más completa posible para luego interpretar (si procede) la [**inferencia**]{.hl-yellow} del modelo.

. . .

* Saber hacer una [**correcta fase de evaluación**]{.hl-yellow} dle modelo.

. . .

* Saber [**simular modelos de regresión**]{.hl-yellow} (tanto que cumplan las hipótesis como que no las cumplan).


# Clase 10: predicción y resumen {#clase-10}

[**Fase de predicción. Resumen**]{style="color:#444442;"}


---

## Resumen

Un breve resumen de lo aprendido sobre reg. lineal univariante

* [**Modelo supervisado de predicción**]{.hl-yellow}: hay una variable objetivo $Y$ continua (numérica) cuyo valor real conocemos

. . .

* [**La visualización importa**]{.hl-yellow}: no fies tu análisis solo a los parámetros matemáticos, la visualización ayuda a entender los datos.

. . .

* [**Relación entre variables**]{.hl-yellow}: buscamos predictoras muy correladas (linealmente) con $Y$ y lo más incorreladas/independientes entre ellas.

. . .

* [**Modelo**]{.hl-yellow}: el modelo parámetrico se resume en $Y = \beta_0 + \beta_1 X + \varepsilon$, donde $\varepsilon$ es una variable aleatoria (ruido).


. . .

* [**Estimación**]{.hl-yellow}: la estimación viene modelizada bajo la hipótesis de linealidad

$$E[Y|X=x] = \beta_0 + \beta_1 x$$




# Clase 11: entrega I {#clase-11}


[**Entrega I**]{style="color:#444442;"}

# Clase 12: incumpliendo hipótesis {#clase-12}

[**¿Cómo simular datos que incumplan las hipótesis?**]{style="color:#444442;"}

---

## Incumpliendo hipótesis

Los contenidos vistos en esta clase se subirán resumidos en formato notebook.


# Clase 13: reg. multivariante {#clase-13}

[**Introducción a la regresión multivariante. Formulación del modelo y estimación**]{style="color:#444442;"}

## Formulación multivariante

De aquí en adelante llamaremos [**modelo multivariante**]{.hl-yellow} a todo modelo en el que $p > 1$ (es decir, tenemos más de una variable predictora).


$$Y = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right)$$
tal que $E \left[ \varepsilon | \left( X_1 = x_1, \ldots, X_p = x_p \right) \right] = 0$.

. . .

En el caso del [**modelo lineal multivariante**]{.hl-yellow} se traducirá por tanto en


$$E \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p, \quad \widehat{Y} = \widehat{\beta_0} + \displaystyle \sum_{j=1}^{p} \widehat{\beta_j} X_j $$

El objetivo seguirá siendo obtener la estimación de los $\widehat{\beta}$ tal que [**minimicemos el error**]{.hl-yellow}

---

## Formulación matricial


Su formulación muestral la podemos expresar mediante la [**matriz de diseño**]{.hl-yellow}

$$\mathbf{X} =\begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}$$

tal que $Y_i = \beta_0 + \sum_{j=1}^{p} \beta_j X_j + \varepsilon_i$, para todo elemento de la muestra $i=1,\ldots, n$


$$ \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix}_{(p+1)\times1} + \begin{pmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{pmatrix}_{n\times1}$$


---

## Formulación matricial

La estimación será por tanto


$$\widehat{\mathbf{Y}} = \begin{pmatrix} \widehat{Y}_1 \\ \widehat{Y}_2 \\ \vdots \\ \widehat{Y}_n \end{pmatrix}_{n\times1} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \widehat{\beta}_1 \\ \widehat{\beta}_2 \\ \vdots \\ \widehat{\beta}_p \end{pmatrix}_{(p+1)\times1}$$


---

## Varianza residual

Como sucedía antes, el objetivo será minimizar la [**varianza residual o error cuadrático medio**]{.hl-yellow} (varianza del error), o lo que hemos llamado $SSR$

$$SSR \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \displaystyle \sum_{i=1}^{n} \left(Y_i - \beta_0 - \beta_1 X_{i1} - \ldots - \beta_p X_{ip} \right)^2 $$
¿Cómo quedaría **matricialmente**?

. . .

$$SSR \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = \varepsilon^{T} \varepsilon$$

Así, aplicando el [**método de los mínimos cuadrados**]{.hl-yellow}, los [**parámetros óptimos**]{.hl-yellow} serán aquellos $\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p\right)$ que minimicen dicha suma

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p \right) = \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)$$


Y repetimos la idea: [**calcularemos la derivada respecto a $\widehat{\boldsymbol{\beta}}$ e igualamos a cero**]{.hl-yellow}

--- 


## Método mínimos cuadrados

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p \right) = \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)$$

Derivando matricialmente tenemos (recuerda: $\left(A B \right)^{T} = B^{T} A^{T}$)

$$\begin{eqnarray}\frac{\partial SSR \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=& \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} \nonumber \\  &=& \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} + \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}}{\partial \boldsymbol{\beta}} \nonumber \\  &=& -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X} - \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \mathbf{X}^{T} \nonumber \\  &=& -\mathbf{X}^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)  - \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \mathbf{X}^{T} \nonumber \\ &=& -2\mathbf{X}^{T} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = 0 \end{eqnarray}$$

---

## Método mínimos cuadrados


$$\begin{eqnarray}\frac{\partial SSR \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=&  -2\mathbf{X}^{T} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = 0 \end{eqnarray}$$

Si descartamos el $-2$ como constante, tenemos que

$$\mathbf{X}^{T} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = 0 \longrightarrow \mathbf{X}^{T}  \mathbf{Y} = \mathbf{X}^{T} \mathbf{X} \boldsymbol{\beta}$$

Si ahora despejamos $\beta$  multiplicando en ambos lados por $\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}$ tenemos finalmente

$$\widehat{\boldsymbol{\beta}}=\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$$

---

## Estimación

Fíjate que cuando $p = 1$ tenemos que

$$\widehat{\mathbf{Y}} = \begin{pmatrix} \widehat{Y}_1 \\ \vdots \\ \widehat{Y}_n \end{pmatrix}_{n\times1} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \begin{pmatrix} 1 & X_{11} \\\vdots & \vdots \\ 1 & X_{n1} \end{pmatrix}_{n \times 2}\begin{pmatrix} \widehat{\beta}_0 \\ \widehat{\beta}_1 \end{pmatrix}_{2\times1}$$

y que por tanto $\widehat{\boldsymbol{\beta}}= \left(\begin{pmatrix} 1 & \ldots & 1 \\ X_{11}  & \ldots & X_{n1} \end{pmatrix} \begin{pmatrix} 1 & X_{11} \\ 1 & X_{21}  \\ \vdots & \vdots \\ 1 & X_{n1} \end{pmatrix} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$

[**Deberes**]{.hl-red}: sería interesante que para el examen supieses demostrar que cuando $p=1$ esa expresión acaba en 

$$\widehat{\beta}_1  = \frac{s_{xy}}{s_{x}^{2}}, \quad \widehat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$$

---

## Estimación

$$\widehat{\boldsymbol{\beta}}=\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$$

* La [**estimación**]{.hl-yellow} será por tanto

$$\widehat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\beta}}=\mathbf{X}\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y} = \mathbf{H}\mathbf{Y}$$

* La matriz $\mathbf{H} = \mathbf{X}\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}$ se conoce como [**hat matrix o matriz de proyección**]{.hl-yellow} (ya que hace que las estimaciones $\hat{y}$ sean en realidad los valores $y$ proyectados verticalmente sobre el plano de regresión ajustado).



---

## Interpretación

Al igual que pasaba antes será importante [**distinguir lo población de lo muestral**]{.hl-yellow}

* Los parámetros $\left( \beta_0, \beta_1, \ldots, \beta_p \right)$ son desconocidos, los [**parámetros poblacionales**]{.hl-yellow}

* Los parámetros $\left( \widehat{\beta}_0, \widehat{\beta}_1 , \ldots, \widehat{\beta}_p\right)$ son estimados a partir de los datos, son [**variables aleatorias**]{.hl-yellow} ya que han sido calculados en función de una muestra aleatoria.

&nbsp;

[**¿Cómo se interpretan ahora los parámetros?**]{.hl-yellow}

---

## Interpretación

$$\widehat{\mathbf{Y}} = \mathbf{X}\widehat{\boldsymbol{\beta}}= \widehat{\beta}_0 + \sum_{j=1}^{p} \widehat{\beta}_j X_j$$

* [**Ordenada en el origen**]{.hl-yellow}: también llamado **intercepto**, y denotado como $\beta_0$ su valor real, es el valor de $Y$ cuando $X_1 = \ldots = X_p = 0$. Es decir, $\widehat{\beta}_0$ se puede interpretar como la estimación $\widehat{Y}$ cuando [**TODAS las predictoras son nulas**]{.hl-purple}


. . .

* [**Pendientes**]{.hl-yellow}: denotadas como $\beta_j$, para $j=1,\ldots, p$, su valor real, cuantifica el incremento de $Y$ cuando solo $X_j$ aumenta una unidad. Es decir, $\widehat{\beta}_j$ se puede interpretar como la variación de la estimación $\widehat{Y}$ cuando [**$X_j$ tiene un incremento unitario y el RESTO DE PREDICTORAS permanecen fijas**]{.hl-purple}.

---

## Diagnosis

En el caso multivariante las [**4 hipótesis**]{.hl-yellow} se convierten en

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es 

$$E \left[Y | \left( \boldsymbol{X}_1 = x_1, \ldots, \boldsymbol{X}_p = x_p \right) \right] = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$$

2. [**Homocedasticidad**]{.hl-green}: necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow} $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \left( \boldsymbol{X}_1 = x_1, \ldots, \boldsymbol{X}_p = x_p \right) \right] = cte < \infty$

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$


4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser en independientes, y en particular **incorrelados** ${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$

$$Y | \left( \boldsymbol{X}_1 = x_1, \ldots, \boldsymbol{X}_p = x_p \right) \sim N \left(\beta_0 + \beta_1x_1 + \ldots + \beta_p x_p, \sigma_{\varepsilon}^2 \right)$$


---

## Inferencia 

Ahora las hipótesis nos permiten decir  que los [**parámetros estimados siguen una distribución normal multivariante**]{.hl-yellow} de [**media el vector de parámetros**]{.hl-purple}  a estimar y de [**matriz de covarianzas la varianza residual por $\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}$**]{.hl-purple}

$$\widehat{\boldsymbol{\beta}} \sim N_{p+1}\left(\boldsymbol{\beta}, \sigma_{\varepsilon}^2 \left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}\right)$$

. . .

Esa normal multivariante, componente a componente, deriva en

$$\widehat{\beta}_j\sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad \frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)} \sim N \left(0, 1 \right), \quad \frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} \sim t_{n-p-1}$$

donde $SE \left(\widehat{\beta}_j\right)^2 = \sigma_{\varepsilon}^2 v_j$ y $\widehat{SE} \left(\widehat{\beta}_j\right)^2 = \widehat{\sigma}_{\varepsilon}^2 v_j$, con $v_j$ el elemento $j$-ésimo de la diagonal de $\left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}$ (matriz que representa la **variabilidad de predictores**)


---

## Caso práctico: wine.csv

Vamos a volver a usar nuestros datos `wine.csv`

```{r}
datos <- read_csv(file = "./datos/wine.csv")
datos
```

Pero esta vez no vamos a seleccionar ninguna variable previamente. Para [**ajustar un modelo multivariante**]{.hl-yellow} basta con añadir variables `+`

```{r}
ajuste_uni <- lm(data = datos, formula = Price ~ AGST)
ajuste_2 <- lm(data = datos, formula = Price ~ AGST + HarvestRain)
ajuste_full <- lm(data = datos, formula = Price ~ .)
```


---

## Caso práctico: wine.csv

Fíjate que ahora la tabla `coefficients` tiene una [**línea por covariable**]{.hl-yellow} (más $\beta_0$) y además en este caso dice `(1 not defined because of singularities)`: la [**matriz de covarianzas no es invertible**]{.hl-red} ya que el [**determinante es 0**]{.hl-red} (en este caso `Year` es "igual" que `Age`)

```{r}
ajuste_full |> summary()
```

---

## Caso práctico: wine.csv

Una vez eliminada, en la última línea `F-statistic: ... p-value: 2.232e-07`: se está [**rechazando la hipótesis nula**]{.hl-green} del contraste de significación global (existe [**alguna predictora cuyo efecto lineal es significativo**]{.hl-green}). Recuerda que la nula es $H_0:~\beta_1 = \ldots = \beta_5 = 0$


```{r}
ajuste_full <- lm(data = datos |> select(-Age), formula = Price ~ .)
ajuste_full |> summary()
```

---

## Caso práctico: wine.csv

Si nos fijamos en la tabla de coeficientes el [**modelo ajustado**]{.hl-yellow} es

$$\begin{eqnarray}\widehat{Precio} &=& 2.496 - 0.0137 * Year + 0.0012 * WinterRain + 0.614 * AGST \nonumber \\ &-& 0.0038 * HarvestRain - 0.00002 * FrancePop \nonumber \end{eqnarray}$$

```{r}
ajuste_full |> summary()
```

---

## Caso práctico: wine.csv

$$\begin{eqnarray}\widehat{Precio} &=& 2.496 - 0.0137 * Year + 0.0012 * WinterRain + 0.614 * AGST \nonumber \\ &-& 0.0038 * HarvestRain - 0.00002 * FrancePop \nonumber \end{eqnarray}$$

La [**interpretación**]{.hl_yellow} es

* La estimación del modelo cuando todas las covariables son 0 (sin población, sin lluvia ni temperatura, año 0) es de 2.496 (escala log)

* Por ejemplo, si el resto de variables permaneciesen fijas, por cada litro de lluvia que caiga de más durante la cosecha (agosto-septiembre), el modelo estima que el precio baja 3.8 (escala log)

---

## Caso práctico: wine.csv

Si nos fijamos en la tabla de coeficientes tenemos **2 predictoras cuyo efecto lineal no se acepta que sea significativo**: `Year`, `FrancePop` (además de $\beta_0$) ya que el contraste de significación $H_0:~\beta_j = 0$ vs $H_1:~\beta_j \neq 0$ nos devuelve un $p-valor > \alpha$

```{r}
ajuste_full |> summary()
```

---

## Caso práctico: wine.csv

La pregunta que intentaremos resolver en futuras clases es:

[**¿Cómo decidir que predictoras seleccionamos?**]{.hl-yellow} El problema si quitamos todas las no significativas de manera simultánea es que no sabemos qué **efectos puede haber entre las propias predictoras**

```{r}
ajuste_full |> summary()
```

---

## Caso práctico: wine.csv

Fíjate que ahora en la [**fase de diagnosis**]{.hl-yellow} tenemos una **sexta gráfica a chequear**: una gráfica que nos calcula la conocida como `VIF` (Variance Inflaction), que nos [**cuantifica la colinealidad (efectos lineales) entre las predictoras**]{.hl-yellow}

```{r}
check_model(ajuste_full)
```

---

## ANOVA

Ahora `anova()` en realidad nos hace una [**ANOVA secuencial**]{.hl-yellow} (variable a variable)


```{r}
ajuste_full |> anova()
```

---

## ANOVA

Ahora `anova()` en realidad nos hace una [**ANOVA secuencial**]{.hl-yellow} (variable a variable)


|  | Df (grados) | Sum Sq | Mean Sq | F value | Pr(>F) |
|:---------:|:-----:|:------:|:------:|:------:|:------:|
| $x_1$  | 1  | $SSR_1$ | $\frac{SSR_1}{1}$ | $F-value = \frac{\frac{SSR_1}{1}}{\frac{SSE}{n-p-1}}$ | $p_1$
| ...  | ...  | ... | ... | $F-value = \frac{\frac{SSR}{p}}{\frac{SSE}{n-p-1}}$ | p-valor F test
| $x_p$  | 1  | $SSR_p$ | $\frac{SSR_p}{1}$ | $F-value = \frac{\frac{SSR_p}{1}}{\frac{SSE}{n-p-1}}$ | $p_p$
| Residuals  | n - p - 1 | SSE | $\widehat{\sigma}_{\varepsilon}^{2} = \frac{SSE}{n-p-1}$ | | |


---

## ANOVA


```{r}
ajuste_full |> anova()
```


Ahora $SSR_j$, con $j=1,\ldots,p$, representa la **suma de residuos al cuadrado** $SSR$ cuando incluimos la predictora j-ésima $X_j$, frente a cuando no lo hacemos, tal que

$$SSR_j = SSR \left(X_1, \ldots, X_j \right) - SSR \left(X_1, \ldots, X_{j-1} \right)$$

Así los p-valores $p_j$ son individuales (los mismos que los de la tabla de coeficientes).



# Clase 14: selección de modelos {#clase-14}

[**Selección secuencial de modelos**]{style="color:#444442;"}

---

## Bondad de ajuste

Como ya hemos hablado, el conocido como $R^2$ o [**bondad de ajuste**]{.hl-yellow} o el [**coeficiente de determinación**]{.hl-yellow} es una forma sencilla, y relacionada con el ANOVA, para cuantificar la [**cantidad de información explicada por el modelo**]{.hl-yellow} definida como

$$R^2 = \text{Ratio info explicada} = \frac{SSR}{SST} =_{lineal} 1 - \frac{SSE}{SST} = 1 - \frac{s_{r}^{2}}{s_{y}^{2}} =  1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^{2}}{s_{y}^{2}}$$


Por definición, la [**bondad de ajuste está entre 0 y 1**]{.hl-yellow} pero además [**presenta un problema en el caso multivariante**]{.hl_red}


* [**Más predictoras**]{.hl-yellow} implica que [**$R^2$ crece**]{.hl-yellow} (al aumentar $p$), ¡incluso aunque dichas [**predictoras no sean significativas**]{.hl-red}!

---
  
## R2 ajustado

Para evitar dicho problema vamos a definir la conocida como [**bondad de ajuste ajustada**]{.hl-yellow} 

$$R_{adj}^2 = 1 - \frac{\frac{SSE}{n-p-1}}{\frac{SST}{n-1}} = 1 - \frac{SSE}{SST}\frac{n-1}{n-p-1}  = 1 - \frac{\widehat{\sigma}_{\varepsilon}^{2}}{SST}(n-1)$$


. . .

Así el $R_{adj}^2$ [**no depende del número de predictoras de manera directa**]{.hl-green} (si sigue dependiendo del ruido, de manera que descenderá solo cuando incrementar $p$ implica reducir el error, es decir, variables con un efecto significativo). Se cumple además que que si $p=1$

$$\lim_{n\to \infty} R_{adj}^2 = \lim_{n\to \infty} \left(1 - \frac{\widehat{\sigma}_{\varepsilon}^{2}}{SST}(n-1) \right) = \lim_{n\to \infty} \left(1 - \frac{\widehat{\sigma}_{\varepsilon}^{2}}{SST}(n-p-1) \right) = R^2$$


---

## R2 ajustado

Para ver mejor su diferencia debes realizar el siguiente [**estudio de simulación**]{.hl-yellow} (si quieres fija semilla `set.seed(12345)`):

. . .

1. Considera dos predictoras $X_1 \sim N(0, 1)$ y $X_2 \sim N(0, 1)$ de tamaño muestral $n = 200$.  Considera el ruido como $\varepsilon \sim N(0, \sigma = 12)$

. . .

2. Simula el modelo $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \varepsilon_i$ con $\beta_0 = 0.01$, $\beta_1 = 1.5$ y $\beta_2 = -1.5$

. . .

3. Realiza el ajuste con todas las predictoras y calcula el $R^2$ y el $R_{adj}^2$ (lo necesitamos guardar, así usa la fórmula no la salida del `lm()`)

. . .

4. Repite este proceso $M = 300$ veces (vuelve a simular las variables, vuelve a construir el modelo, y obtén de nuevo las bondades de ajuste, de manera que tengamos $300$ de cada uno)

---

## R2 ajustado


Lo que debería salirte si fijas `set.seed(12345)`

```{r}
#| echo: false
M <- 300
n <- 200
R2 <- R2_adj <- rep(NA, M)
p <- 2

for (i in 1:M) {
  
  x_1 <- rnorm(n)
  x_2 <- rnorm(n)
  eps <- rnorm(n, mean = 0, sd = 12)
  
  y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
  datos <- tibble("y" = y, "x_1" = x_1, "x_2" = x_2)
  ajuste <- lm(data = datos, y ~ .)
  
  R2[i] <- 1 - var(ajuste$residuals) / var(datos$y)
  R2_adj[i] <-
    1 - (var(ajuste$residuals) / var(datos$y)) * ((n - 1) / (n - p - 1))
}

library(ggridges)
ggplot(tibble("id" = 1:M, "R2" = R2, "R2_adj" = R2_adj) |> 
         pivot_longer(cols = -id, names_to = "R2_type",
                      values_to = "values")) +
  geom_density_ridges(aes(x = values, y = R2_type,
                          color = R2_type, fill = R2_type),
                      alpha = 0.5) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  coord_flip() +
  theme_minimal()
```


---

## R2 ajustado

Repite el estudio de simulación  añadiendo 

1. Una nueva predictora "basura" $X_3 \sim N(0, 2)$ tal que $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + 10^{-10} X_{i3} + \varepsilon_i$.

2. Dos nuevas predictora "basura" $X_3 \sim N(0, 2)$ y $X_4 \sim N(0, 2)$ tal que $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + 10^{-10} X_{i3} + 10^{-10} X_{i4} + \varepsilon_i$.

...

3. 195 nuevas predictora "basura" $X_j \sim N(0, 2)$  tal que $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \displaystyle \sum_{j=3}^{195} 10^{-10} X_{ij} + \varepsilon_i$.


Ahora debes obtener $300*195$ valores de $R^2$ y $R_{adj}^2$ (300 simulaciones por cada nueva predictora que añadimos)

---

## R2 ajustado

![](img/predictoras-R2.png)


```{r}
#| echo: false
#| eval: false
M <- 300
n <- 200
p <- 195
R2 <- R2_adj <- matrix(NA, M, p)

set.seed(12345)
for (i in 1:M) {
  
  x_1 <- rnorm(n)
  x_2 <- rnorm(n)
  eps <- rnorm(n, mean = 0, sd = 12)
  y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
  
  X <- matrix(NA, n, p)
  for (j in 1:p) {
    
    X[, j] <- rnorm(n, mean = 0, sd = 2)
    y <- y + 1e-10 * X[, j]
    datos <- tibble("y" = y, "x_1" = x_1, "x_2" = x_2, X[, 1:j])
    ajuste <- lm(data = datos, y ~ .)
    
    R2[i, j] <- 1 - var(ajuste$residuals) / var(datos$y)
    R2_adj[i, j] <-
      1 - (var(ajuste$residuals) / var(datos$y)) *
      ((n - 1) / (n - (j + 2) - 1))
  }
}

mean_R2 <- R2 |> colMeans()
mean_R2_adj <- R2_adj |> colMeans()

gg <- ggplot()
for (i in 1:M) {
  gg <-
    gg +
    geom_line(data =
                 tibble("p" = 1:p, "R2" = R2[i, ],
                        "R2_adj" = R2_adj[i, ]) |>
                 pivot_longer(cols = -p, names_to = "R2_type",
                              values_to = "values"),
               aes(x = p, y = values, color = R2_type),
              linewidth = 0.01, alpha = 0.1) +
    geom_line(data = tibble("p" = 1:p, "R2" = mean_R2, "R2_adj" = mean_R2_adj) |>
                 pivot_longer(cols = -p, names_to = "R2_type",
                              values_to = "values"),
              aes(x = p, y = values, color = R2_type), linewidth = 1.5)
}
gg1 <- 
  gg +
  theme_minimal() +
  scale_y_continuous(limits = c(-0.5, 1.1)) +
  labs(x = "Número de predictoras basura")
```

---

## Multicolinealidad

Uno de los mayores problemas de los [**modelos lineales multivariantes**]{.hl-yellow} es el conocido como [**problema de colinealidad**]{.hl-red}

. . .

Imagina un modelo $Y = \beta_0 + \beta_1 X_1 + \beta_2X_2 + \beta_3 X_3$, donde $X_3$ es definida como (literal) la suma de $X_1$ y $X_2$.

```{r}
n <- 200
x_1 <- rnorm(n, sd = 2)
x_2 <- rnorm(n, sd = 2)
x_3 <- x_1 + x_2
eps <- rnorm(n)

y <- 0.5 - 2*x_1 + 3*x_2 -2.5*x_3
datos <- tibble(y, x_1, x_2, x_3)
ajuste <- lm(data = datos, formula = y ~ .)
```

---

## Multicolinealidad

Como ya vimos en el ejemplo de `wine.csv`, cuando tenemos [**dos o más predictoras que dependen linealmente entre sí**]{.hl-yellow} la matriz $\left(X^{T} X \right)^{-1}$ no se puede invertir ya que es singular (determinante igual a 0), así que debe eliminar una de las ecuaciones para que el problema sea de rango completo

Esto se conoce como [**colinealidad exacta**]{.hl-yellow}

```{r}
ajuste |> summary()
```

---

## Multicolinealidad

La colinealidad exacta es solo el ejemplo más extremo de lo que se conoce como [**problema de colinealidad**]{.hl-red}: un problema que aparece cuando varias predictoras están **altamente correladas**. 
Un [**problema de colinealidad**]{.hl-yellow} tiene principalmente dos consecuencias:

. . .

* [**Reduce la precisión de los estimadores**]{.hl-red} ya que la matriz $X^{T} X$, según aumenta la dependencia, tiene un determinante cada vez más cercano a 0, por lo que $\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}$, que siempre depende de $1/det\left( \mathbf{X}^{T}\mathbf{X} \right)$, tendrá valores cada vez más grandes. Dado que $\widehat{\beta}_j\sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right)$, con $SE \left(\widehat{\beta}_j\right)^2 = \sigma_{\varepsilon}^2 v_j$ siendo $v_j$ el elemento $j$-ésimo de la diagonal de $\left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}$, implica que [**a mayor colinealidad, mayor varianza de las estimaciones**]{.hl_yellow}

. . .


* [**Distorsiona los efectos**]{.hl-red} de los predictores sobre la respuesta

---

## Multicolinealidad

¿Pero qué sucede cuando esa [**colinealidad no es tan obvia**]{.hl-yellow}? Veamos un ejemplo sencillo: simula el siguiente modelo (para $n = 100$)

$$Y = 1 + 0.5 X_1 + 2 X_2 - 3 X_3 - X_4 + \varepsilon, \quad \varepsilon \sim N(0, 1)$$


* $X_1 \sim N(0, 1)$, $X_2 = 0.5*X_1 + N(0, 1)$ y $X_3 = 0.5*X_2 + N(0, 1)$

* $X_4 = -X_1 + X_2 + N(0, 0.5)$

```{r}
#| code-fold: true
set.seed(12345)
n <- 100

x1 <- rnorm(n)
x2 <- 0.5 * x1 + rnorm(n)
x3 <- 0.5 * x2 + rnorm(n)
x4 <- -x1 + x2 + rnorm(n, sd = 0.5)
eps <- rnorm(100)

y <- 1 + 0.5 * x1 + 2 * x2 - 3 * x3 - x4 + eps
datos <- tibble(y, x1, x2, x3, x4)
```

¿Cómo serán sus **correlaciones (lineales)**?

---

## Multicolinealidad

A priori [**no se observa un problema obvio de colinealidad**]{.hl-yellow} ya que no hay dos predictoras altamente correladas.

```{r}
datos |> 
  correlate()
```

---

## Multicolinealidad

Por la definición realizada es obvio que hay una [**dependencia lineal entre todas las predictoras**]{.hl-yellow}, pero al ser una relación lineal más compleja (con ruido de por medio y distintas predictoras interactuando a la vez), una simple [**revisión de las correlaciones es necesaria pero no suficiente**]{.hl-yellow} ya que puede ocultar problemas de colinealidad que sí existen.

```{r}
datos |> 
  cor() |> 
  corrplot(method = "color", addCoef.col = "#121212")
```


---

## Multicolinealidad

Necesitamos por tanto una forma de cuantificar dicha multicolinealidad, y para ello usaremos el conocido como [**factor de inflación de la varianza (VIF)**]{.hl-yellow}

. . .

Para cada coeficiente $\beta_j$, y su estimador $\widehat{\beta}_j$, se define el VIF como

$$VIF\left(\widehat {\beta_j} \right) = \frac{1}{1 - R^2_{X_j | X_{-j}}}, \quad R_{j}^2:= R^2_{X_j | X_{-j}}$$

donde $R_{j}^2$ representa la [**bondad de ajuste $R^2$ cuando predecimos con una regresión lineal multivariante la predictora $X_j$**]{.hl-yellow} en función del resto de predictoras (sin la variable objetivo)

$$X_j = \gamma_0 + \gamma_1 X_1 + \ldots + \gamma_{j-1} X_{j-1} + \gamma_{j+1} X_{j+1} + \ldots + \gamma_j X_j$$

---

## Multicolinealidad


$$VIF\left(\widehat {\beta_j} \right) = \frac{1}{1 - R^2_{X_j | X_{-j}}}, \quad R_{j}^2:= R^2_{X_j | X_{-j}}$$

La idea es [**cuantificar como de relacionado está cada predictor**]{.hl-yellow} respecto al resto (de manera conjunta, no dos a dos). Nota: **siempre es mayor que 1**

. . .


* Si $VIF\left(\widehat {\beta_j} \right)$ [**cercano a 1**]{.hl-yellow}, entonces $R_{j}^{2}$ cercano a 0 --> variabilidad del predictor $X_j$ no se puede explicar con la combinación lineal de otras

. . .


* Si $VIF\left(\widehat {\beta_j} \right)$ [**mayor que 5**]{.hl-yellow}, entonces $1 - R_{j}^{2} < 0.2$, ergo $R_{j}^{2} > 0.8$ --> problema moderado de colinealidad

. . .


* Si además $VIF\left(\widehat {\beta_j} \right)$ [**mayor que 10**]{.hl-yellow}, entonces $R_{j}^{2} > 0.9$ --> problema grave de colinealidad 

---

## Multicolinealidad

Para calcular el $VIF$ podemos o bien hacer regresión con cada parámetro, de manera manual, o bien hacer uso de `check_collinearity()` del paquete `{performance}`

```{r}
ajuste <- lm(data = datos, y ~ .)
performance::check_collinearity(ajuste)
```

---

## Multicolinealidad

Si te fijas $VIF \left(\widehat{\beta}_2 \right) = 10.41$, que es justamente el $1/(1-R^2)$ si hacemos la regresión $X_2$ vs el resto de predictoras

```{r}
check_collinearity(ajuste)$VIF
ajuste_X2 <- lm(data = datos |> select(-y), x2 ~ .)
ajuste_X2 |> summary()
```

---

## Multicolinealidad

El VIF podemos además [**visualizarlo en una sexta gráfica**]{.hl-model} cuando hacemos `check_model()`

```{r}
check_model(ajuste)
```

---

## Multicolinealidad

El VIF podemos además [**visualizarlo de manera manual**]{.hl-model}

```{r}
#| code-fold: true
VIF <- tibble("variable" = c("x1", "x2", "x3", "x4"),
              "VIF"= check_collinearity(ajuste)$VIF)
ggplot(VIF) +
  geom_col(aes(x = variable, y = VIF, fill = VIF),
           alpha = 0.75) +
  scale_fill_gradient2(low = "#1B8A54", mid = "#F7E865",
                       high = "#CE2424", midpoint = 5) +
  theme_minimal()
```


---

## Selección de modelos

Por lo tanto parece que obvio que en un **modelo de regresión multivariante** vamos a tener que que [**seleccionar distintas variables**]{.hl-yellow} para solventar esos problemas de colinealidad

. . .

Llamaremos [**modelo saturado**]{.hl-yellow} al modelo con todas las $p$ predictoras, sin seleccionar.

. . .

Y como hemos visto, [**no podremos simplemente comparar 2 a 2 correlaciones o p-valores**]{.hl-red} ya que el efecto de una variable nos puede afectar en otras. ¿Qué hacer?


---

## Selección de modelos

Un protocolo de actuación habitual podría ser el siguiente:

1. Hacemos el [**ajuste del modelo saturado**]{.hl-yellow}

2. Calculamos el [**VIF de cada estimador**]{.hl-yellow}

3. Adoptamos un umbral (por ejemplo, $VIF > 10$), de manera que [**toda predictora que lo supere se elimina ya de antemano**]{.hl-yellow}

4. Volvemos a chequear el VIF. Si hay que eliminar, volver al paso 3.

5. Del resto de predictoras se hace una [**selección más fina**]{.hl-yellow}. Algunas opciones: BIC/AIC, regresión penalizada (LASSO, ridge, elastic net), PCA.

---

## Selección de modelos

Lo que nos dice el VIF y lo visto hasta ahora es que, incluso aunque tengan efecto, [**añadir predictoras complejizando el modelo no es gratis**]{.hl-yellow}, ya que lo hacemos a costa de [**sobreajustar el modelo**]{.hl-yellow} (la varianza de los estimadores se incrementa considerablemente)

¿Cuál es el [**número mínimo/máximo de predictoras**]{.hl-yellow} que podremos incluir?

. . .

* El número mínimo de predictoras será $p = 1$
* El número máximo será $p = n-2$, o dicho de otra forma, necesito al menos $n \geq p+2$ observaciones. Piensa en $p=1$: si quiero una recta, necesito al menos 2 puntos para que la recta existe y al menos 3 para poder calcular la varianza residual estimada (acuérdate que se divide entre $n-p-1$)

. . .

De manera resumida: dado un modelo con $p$ predictores, necesitaremos estimar [**$p+2$ incógnitas**]{.hl-yellow} ($p+1$ coeficientes y la varianza residual).

---

## BIC y AIC

[**¿Cómo seleccionar los predictores más adecuados para el CONJUNTO del ajuste?**]{.hl-yellow}

. . .

Los métodos más conocidos son los conocidos como [**selección de modelos stepwise (paso-a-paso)**]{.hl-yellow}, que de manera **iterativa**, va incluyendo y descartando distintos predictores, y comparando su calidad, para decidir que **combinación de parámetros** es la más óptima.

. . .

La idea es [**seleccionar el modelo más óptimo**]{.hl-yellow} en función de un [**criterio de información**]{.hl-yellow} que combina la calidad del modelo con el número de predictoras empleadas: vamos a [**penalizar el uso de variables que no mejore suficiente el modelo**]{.hl-yellow}

---


## BIC y AIC

Los dos criterios de información más famosos son el [**Bayesian Information Criterion (BIC)**]{.hl-yellow} y el [**Akaike Information Criterion (AIC)**]{.hl-yellow} definidos ambos como

$$AIC/BIC_{modelo} = -Calidad + \underbrace{\text{npar(modelo)} * \text{penalización}}_{\text{Complejidad}}$$

* $\text{npar(modelo)}$ es el [**número de parámetros a estimar del modelo**]{.hl-yellow}, que en el caso que nos ocupa es $\text{npar(modelo)} = p+2$ (coeficientes + var residual)

* $\text{Calidad}$ una medida que nos diga como de bueno es nuestro ajuste: al tener signo negativo, buscamos el [**menor valor de AIC/BIC**]{.hl-yellow}

* $\text{penalización}$: cuando sube $p$, entonces suba el $AIC/BIC$

El objetivo será probar [**distintos modelos y quedarnos con el que tenga AIC/BIC más pequeño**]{.hl-yellow}

---

## BIC y AIC

Es habitual que la [**calidad del modelo**]{.hl-yellow} venga cuantificada por $2\ell(\text{modelo})$, donde $\ell(\text{modelo})$ es lo que se conoce como [**log-verosimilitud del modelo**]{.hl-yellow} 

$$\ell(\text{modelo}) = \log \left(P \left(\text{datos} |  \left(\text{parámetros}, \text{modelo} \right) \right) \right)$$

es decir, suponiendo que el **modelo fuese correcto** y los **parámetros valiesen la estimación obtenida**, ¿cómo de [**probable es que mis datos hayan sido los que han sido**]{.hl-yellow}?

. . .

Así nuestros criterios quedan como

$$AIC/BIC_{modelo} = -2\ell(\text{modelo}) + \underbrace{(p+2) * \text{penalización}}_{\text{Complejidad}}$$

---

## BIC y AIC

La diferencia entre ambos está en la **penalización usada**


$$BIC(modelo_p) = -2\ell(\text{modelo}) + \underbrace{(p+2) * \log(n)}_{\text{Complejidad}}$$

$$AIC (modelo_p) = -2\ell(\text{modelo}) + \underbrace{(p+2) * 2}_{\text{Complejidad}}$$

El [**criterio BIC es más agresivo**]{.hl-yellow} seleccionando variables ya que, si $n$ crece, $\log(n) >> 2$ (penaliza más el sobreajuste) y, además, [**depende del tamaño muestral**]{.hl-yellow}

---

## BIC y AIC

Vamos a ver un **pequeño ejemplo en R** con `wine.csv`, aplicando un modelo saturado (habiendo hecho un filtro inicial por VIF), un modelo solo con `AGST`,  otro con `AGST + HarvestRain` y otro con `AGST + HarvestRain + Year`, y calcular su BIC/AIC con las funciones `BIC()` y `AIC()`

```{r}
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, formula = Price ~ .)
ajuste_1 <- lm(data = datos, formula = Price ~ AGST)
ajuste_2 <- lm(data = datos, formula = Price ~ AGST + HarvestRain)
ajuste_3 <- lm(data = datos, formula = Price ~ AGST + HarvestRain + Age)
```

:::: columns
::: {.column width="50%"}
```{r}
BIC(ajuste_saturado)
BIC(ajuste_1)
```
:::

::: {.column width="50%"}
```{r}
BIC(ajuste_2)
BIC(ajuste_3)
```
:::
::::

---

## BIC y AIC

Vamos a ver un **pequeño ejemplo en R** con `wine.csv`, aplicando un modelo saturado (habiendo hecho un filtro inicial por VIF), un modelo solo con `AGST`,  otro con `AGST + HarvestRain` y otro con `AGST + HarvestRain + Year`, y calcular su BIC/AIC con las funciones `BIC()` y `AIC()`

:::: columns
::: {.column width="50%"}
```{r}
AIC(ajuste_saturado)
AIC(ajuste_1)
```
:::

::: {.column width="50%"}
```{r}
AIC(ajuste_2)
AIC(ajuste_3)
```
:::
::::

---

## BIC y AIC

Podemos usar `compare_performance()`  ($AICc = AIC +\frac{2(p+2)^{2} + 2(p+2)}{n-(p+2)-1}$ es una versión corregida para tamaños muestrales pequeños)

```{r}
performance::compare_performance(ajuste_saturado, ajuste_1, ajuste_2, ajuste_3)
```

. . .

En ambos criterios la conclusión es que de los 4 modelos, el [**"mejor" es el 3º ya que AIC/BIC más bajos**]{.hl-yellow} aunque tenga un $R_{adj}^2$ menor: la mejora que produce meter todas las variables no es suficiente para lo que se complica el modelo.

---

## BIC y AIC

En realidad lo correcto sería antes chequear si [**podemos eliminar alguna variable**]{.hl-yellow} ya de manera preliminar usando el VIF

```{r}
check_collinearity(ajuste_saturado)
```

---

## BIC y AIC

Según el VIF debemos ya eliminar de antemano `Year` y `FrancePop` así que lo hacemos y volvemos calcularlo para las restantes

```{r}
datos_VIF <- datos |> select(-Year, -FrancePop)
ajuste_saturado <- lm(data = datos_VIF, formula = Price ~ .)
check_collinearity(ajuste_saturado)
```

---

## BIC y AIC

Tras este primer **filtro grosero** volvemos a implementar los 3 modelos

```{r}
ajuste_saturado <- lm(data = datos_VIF, formula = Price ~ .)
ajuste_1 <- lm(data = datos_VIF, formula = Price ~ AGST)
ajuste_2 <- lm(data = datos_VIF, formula = Price ~ AGST + HarvestRain)
ajuste_3 <- lm(data = datos_VIF, formula = Price ~ AGST + HarvestRain + Age)
compare_performance(ajuste_saturado, ajuste_1, ajuste_2, ajuste_3)
```

Ahora **nos indica que el mejor modelo es el saturado** (pero que ya no tiene 6 variables sino 4): la mejora de bondad de ajuste compensa por incrementar una sola variable (entre ajuste3 y saturado)

---

## BIC y AIC

Fíjate que además al haber quitado dos variables con alta dependenica lineal del resto las [**hipótesis se cumplen**]{.hl-yellow} (antes no)

```{r}
check_model(ajuste_saturado)
```

---

## BIC y AIC

Vamos a repetirlo con un ejemplo simulado $Y = 0.01 + 1.5*X_1 -1.5X_2 + \varepsilon$, donde $X_1,X_2 \sim N(0,1)$ y $\varepsilon \sim N(0, 2)$, y al que vamos añadiendo predictoras basura $X_{2+j} \sim N(0, 2)$ con $\beta_{2+j} = 10^{-8}$, probando cuando $j=1$, $j = 5$, $j = 10$, $j=25$, $j=50$ y $j = 100$. El mejor modelo es el que solo tiene las **dos predictoras con un efecto real** sobre y.

```{r}
#| code-fold: true
n <- 200
p <- c(1, 5, 10, 25, 50, 100)

set.seed(12345)
x_1 <- rnorm(n)
x_2 <- rnorm(n)
eps <- rnorm(n, mean = 0, sd = 2)
y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
ajuste_0 <- lm(data = tibble("y" = y, "x_1" = x_1, "x_2" = x_2), y ~ .)

X <- matrix(rnorm(n * max(p), mean = 0, sd = 2), nrow = n)
ajuste <- list()
for (j in 1:length(p)) {
  
  ynew <- y + 1e-8 * sum(X[, 1:p[j]])
  datos <- tibble("y" = ynew, "x_1" = x_1, "x_2" = x_2, X[, 1:p[j]])
  ajuste[[j]] <- lm(data = datos, y ~ .)
}
compare_performance(ajuste_0, ajuste[[1]], ajuste[[2]],
                    ajuste[[3]], ajuste[[4]], ajuste[[5]])
```



---


## Sobreajuste

Algo importante a tener en cuenta es que, aunque ambos criterios nos ayudan a seleccionar modelos, ambos [**funcionan de manera aceptable bajo la hipótesis**]{.hl-yellow} de que $n >> p +2$: en caso contrario, si $n$ se acerca a $p +2$, el sobreajuste seguirá produciéndose

. . .

Dado que la penalización es más grande en el BIC, el [**criterio BIC nos garantiza una más temprana detección del sobreajuste**]{.hl-yellow}

---

## Sobreajuste

Lo anterior se puede ilustrar calculando BIC e AIC del anterior estudio de simulación de $R^2$ vs $R_{adj}^2$: fíjate como el [**BIC tarda más en bajar**]{.hl-yellow}, tal que $BIC(p = 195) > BIC(p=2)$ (nos haría quedarnos con el modelo sin sobreajustar) mientras que $AIC(p = 156) < AIC(p=2)$ (y para todos los que van detrás), por lo que acabaríamos eligiendo el modelo más sobreajustado.

![](img/BIC-AIC.png)

```{r}
#| echo: false
#| eval: false
M <- 300
n <- 200
p <- 195
BIC_values <- AIC_values <- matrix(NA, M, p)

set.seed(12345)
for (i in 1:M) {
  
  x_1 <- rnorm(n)
  x_2 <- rnorm(n)
  eps <- rnorm(n, mean = 0, sd = 12)
  y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
  
  X <- matrix(NA, n, p)
  for (j in 1:p) {
    
    X[, j] <- rnorm(n, mean = 0, sd = 2)
    y <- y + 1e-10 * X[, j]
    datos <- tibble("y" = y, "x_1" = x_1, "x_2" = x_2, X[, 1:j])
    ajuste <- lm(data = datos, y ~ .)
    
    BIC_values[i, j] <- BIC(ajuste)
    AIC_values[i, j] <- AIC(ajuste)
  }
}

mean_BIC <- BIC_values |> colMeans()
mean_AIC <- AIC_values |> colMeans()

gg <- ggplot()
for (i in 1:M) {
  gg <-
    gg +
    geom_line(data =
                 tibble("p" = 1:p, "BIC" = BIC_values[i, ],
                        "AIC" = AIC_values[i, ]) |>
                 pivot_longer(cols = -p, names_to = "criterio",
                              values_to = "values"),
               aes(x = p, y = values, color = criterio),
              linewidth = 0.01, alpha = 0.1) +
    geom_line(data = tibble("p" = 1:p, "BIC" = mean_BIC,
                            "AIC" = mean_AIC) |>
                 pivot_longer(cols = -p, names_to = "criterio",
                              values_to = "values"),
              aes(x = p, y = values, color = criterio), linewidth = 1.5)
}
gg1 <- 
  gg +
  theme_minimal() +
  labs(x = "Número de predictoras basura")
```

---

## Consistencia

Otra [**enorme ventaja del BIC**]{.hl-yellow} es que se ha demostrado matemáticamente que es [**consistente**]{.hl-yellow}: si el tamaño muestral fuese lo suficientemente grande, el [**BIC garantiza elegir el modelo correcto que genera los datos**]{.hl-green}. Matemáticamente se cumple que, si tenemos una serie de modelos $M_1, \ldots, M_m$, y el modelo real que genera los datos $M_0$ (que pretendemos estimar), entonces

$$P\left[\arg\min_{k=0,\ldots,m}\text{BIC}(\widehat{M}_k)=0\right]\to 1, \quad  n \to \infty$$

Esto solo suponiendo que el modelo subyacente sea lineal claro... Esto [**no sucede con el AIC**]{.hl-red} (para vuestro yo del futuro: en <https://doi.org/10.2307/2290328.> se prueba que el AIC es equivalente a usar validación cruzada leave-one-out, el cual es inconsistente)

---

## stepAIC

Y aquí nos puede nacer una duda: si tengo muchos predictores, [**¿tengo que calcular el BIC/AIC de todas las combinaciones posibles?**]{.hl-yellow}

. . .

Sí, pero lo hará por nosotros `MASS::stepAIC()`, donde en el parámetro `k = ...` le indicamos la penalización (si `k = 2` es AIC y si `k = log(n)` es el BIC)

```{r}
#| eval: false
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, Price ~ .)
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```

---

## stepAIC

```{r}
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, Price ~ .)
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```


---

## stepAIC

Lo que obtendremos será una [**selección secuencial de modelos**]{.hl-yellow}, de manera que el irá probando las combinaciones, nos **muestra el ajuste y el valor del BIC/AIC** y se parará cuando el AIC/BIC no mejore.

```{r}
#| eval: false
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, Price ~ .)
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```


---

## stepAIC

```{r}
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "forward")
```

El argumento `direction = ...` puede tomar los valores `"both"` (por defecto), `"forward"` y `"backward"` que nos determina la [**dirección de búsqueda**]{.hl-yellow}:


* `direction = "forward"`: empieza con el modelo proporcionado y va [**añadiendo**]{.hl-yellow} predictoras haciendo modelos cada vez más complejos.

---

## stepAIC

```{r}
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "forward")
```


Fíjate que ahora  como le hemos dado el modelo saturado y `direction = "forward"`, entonces no hace nada. Para controlar esto podemos incluir la variable `scope = list(lower = mod_easy, upper = mod_complex)`, donde **podemos pasarle dos modelos**p: los modelos más complejos y más sencillo **pentre los que queremos que se mueva en la búsqueda**.

---

## stepAIC

```{r}
#| eval: false
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "backward")
```

* `direction = "backward"`: empieza con el modelo proporcionado y va [**eliminando**]{.hl-yellow} predictoras haciendo modelos cada vez más sencillos.


```{r}
#| eval: false
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```

* `direction = "both"`: empieza con el modelo proporcionado y va [**añadiendo y eliminando**]{.hl-yellow} predictoras según sea más conveniente (pudiendo añadir/eliminar una variable que previamente e había eliminado/añadido)




---

## stepAIC

```{r}
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "forward")
```

[**Aviso**]{.hl-red}: en realidad `MASS::stepAIC()` no calcula exactamente el mismo BIC/AIC que las funciones `BIC()` e `AIC()`, sino que les suma una constante $n(\log(2\pi) + 1) + \log(n)$ para BIC y $n(\log(2\pi) + 1) + 2$ para AIC

Dado que es una constante nos da igual para comparar modelos, pero hace que [**NO podamos comparar salidas**]{.hl-red} de `BIC()` y `AIC()` con salidas de `MASS::stepAIC()`.



# Clase 15: casos prácticos {#clase-15}

[**Casos prácticos: datos de viviendas de Boston y seatpos dataset**]{style="color:#444442;"}

---

## Casos prácticos

Realiza todo el ajuste completo multivariante con los datasets:

* `seatpos` del paquete `{faraway}`: datos de 38 conductores donde el objetivo es predecir `hipcenter`, la posición del asiento del conductor, en función de distintas variables (ver `? faraway::seatpos`)

* `Boston` del paquete `{MASS}`:  datos de 560 suburbios de Boston, en el que se han medido 14 variables en cada uno, con el objetivo de predecir `medv` el precio mediano de inmuebles (en millones de dolares), en función de variables estructurales (`rm` y `age`), variables de vecindario (`crim`, `zn`, `indus`, `chas`, `tax`, `ptratio`, `black` y `lstat`), variables de accesibilidad (`dis` y `rad`) y variables de calidad del aire (`nox`)


# Clase 16: variables cualitativas {#clase-16}

[**¿Cómo introducir predictoras cualitativas?**]{style="color:#444442;"}

---

## ...

# Clase 17: depuración {#clase-17}

[**...**]{style="color:#444442;"}

---

## Ausentes

---

## Outliers

---

## ...

# Clase 18: ... {#clase-18}

[**...**]{style="color:#444442;"}

---

## ..


# El mundo Github

[**Trabajar ordenados, publicar resultados, replicabilidad de lo realizado**]{style="color:#444442;"}

---

## ¿Qué es Github?

[**GitHub**]{.hl-yellow} es la plataforma colaborativa más conocida basada en el [**sistema de control de versiones Git**]{.hl-yellow}

. . .

-   [**¿Qué es Git?**]{.hl-purple} Git es un sistema de [**control de versiones**]{.hl-yellow}: una especie de [**Dropbox**]{.hl-yellow} para facilitar la [**programación colaborativa**]{.hl-yellow} entre un grupo de personas, permitiendo llevar la [**trazabilidad de los cambios**]{.hl-yellow} realizados.

. . .

-   [**¿Qué es Github?**]{.hl-purple} Nuestra [**plataforma/interfaz**]{.hl-yellow} para ejecutar el control de versiones: nos servirá no solo para trabajar colaborativamente sino para [**hacer transparente**]{.hl-yellow} el proceso de construcción de nuestros proyectos de código.

. . .

::: callout-important
## Importante

Desde el 4 de junio de 2018 Github es de Microsoft (ergo el código que subas también)
:::

---

## Visión general

Tras hacernos una cuenta en Github, [**arriba a la derecha**]{.hl-purple} tendremos un círculo, y haciendo click en [**Your Profile**]{.hl-purple}, veremos algo similar a esto

::: columns
::: {.column width="55%"}
![](img/github_1.png)
:::

::: {.column width="45%"}
-   [**Edit profile**]{.hl-purple}: nos permite añadir una [**descripción y foto de perfil**]{.hl-yellow}.

-   [**Overview**]{.hl-purple}: en ese panel de cuadrados se [**visualizará nuestra actividad**]{.hl-yellow} a lo largo del tiempo.

-   [**Repositories**]{.hl-purple}: el códugo será subido a [**repositorios**]{.hl-yellow}, el equivalente a nuestras carpetas compartidas en Dropbox.
:::
:::

---

## Primer uso: consumidor

Antes de aprender como crear repositorios, Github también nos servirá para

-   [**Acceder a código**]{.hl-purple} ajeno
-   [**Proponer mejoras**]{.hl-purple} a otros usuarios, e incluso proponer [**correcciones de error que detectemos**]{.hl-yellow} de software que usemos

. . .

-   [**Instalar paquetes de R**]{.hl-purple}. En muchas ocasiones los desarrolladores de paquetes suben las actualizaciones a CRAN cada cierto tiempo, y en otras el software no es suficientemente «amplio» para poder ser subido como paquete.

El código de paquetes que no tengamos subido en CRAN podremos [**instalarlo como código desde Github**]{.hl-yellow}

---

## Instalar desde Github

Por ejemplo, vamos a instalar un paquete llamado [`{peRReo}`](https://github.com/jbgb13/peRReo), cuya única función es darnos [**paletas de colores**]{.hl-yellow} basadas en portadas de [**álbumes de música urbana**]{.hl-yellow}

::: columns
::: {.column width="50%"}
![](img/perrreo.png)
:::

::: {.column width="50%"}
![](img/rosalia.png){width="360"}
:::
:::

. . .

Para ello antes tendremos que instalar un [**conjunto de paquetes para desarrolladores**]{.hl-yellow} llamado `{devtools}`, que nos permitirá la instalación desde Github

```{r}
#| eval: false
install.packages("devtools")
```

---

## Instalar desde Github

Las [**instrucciones de instalación**]{.hl-yellow} suelen venir detalladas en la portada del repositorio

::: columns
::: {.column width="50%"}
![](img/install_perreo.png)
:::

::: {.column width="50%"}
![](img/rosalia.png){width="310"}
:::
:::

. . .

En la mayoría de casos bastará con la función `install_github()` (del paquete que acabamos de instalar), pasándole como argumento la [**ruta del repositorio**]{.hl-yellow} (sin "github.com/").

```{r}
#| eval: false
devtools::install_github("jbgb13/peRReo")
```

Ya puedes perrear con ggplot ;)

---

## Descargar desde Github

La mayoría de veces lo que subamos no será un paquete de R como tal sino que [**subiremos un código más o menos organizado**]{.hl-yellow} y comentado. En ese caso podremos [**descargar el repo entero**]{.hl-yellow} haciendo click [**Code**]{.hl-green} y luego Download ZIP.

Por ejemplo, vamos a descargarnos los scripts de dataviz que han subido desde el [Centre d'Estudis d'Opinió](https://github.com/ceopinio/bop-grafics)

![](img/ceo_github.png)

---

## Ideal

![](img/abogados_simpson.jpeg){width="600"}

[**¿Lo ideal en caso de RTVE?**]{.hl-purple} Tener dos tipos de repositorios

-   Una [**colección de repositorios públicos (producción)**]{.hl-yellow} donde hacer transparente el código y los datos ([**ya validados**]{.hl-purple}), coordinado por un nº reducido de personas.

-   Una [**colección de repositorios privados (desarrollo)**]{.hl-yellow} donde esté todo el equipo colaborando y donde se haga el [**trabajo del día**]{.hl-purple}, con trazabilidad interna.

---

## Nuestro primer repositorio

Vamos a [**crear nuestro primero repositorio**]{.hl-yellow} que servirá además como [**carta de presentación**]{.hl-yellow} de nuestro perfil en Github.

1.  [**Repositories**]{.hl-purple}: hacemos click en las pestaña de Repositories.

2.  [**New**]{.hl-purple}: hacemos click en el [**botón verde New**]{.hl-green} para crear un nuevo repositorio

![](img/new_repo.png)

---

## Nuestro primer repositorio

-   [**Repository name**]{.hl-purple}: el [**nombre del repositorio**]{.hl-yellow}. En este caso vamos a crear un repositorio muy concreto: el nombre debe [**coincidir exactamente con tu nombre de usuario**]{.hl-yellow}

-   [**Description**]{.hl-purple}: descripción de tu repositorio. En este caso será un repo de presentación.

![](img/repo_init_1.png)

---

## Nuestro primer repositorio

-   [**Public vs private**]{.hl-purple}: con cada repositorio tendremos la opción de hacer el repositorio

    -   [**público**]{.hl-purple}: todos los usuarios podrán ver el código así cómo la trazabilidad de su desarrollo (qué se añade y cuándo). Es para mí la opción más recomendable cuando quieres darle [**visibilidad y transparencia**]{.hl-yellow} a tu trabajo
    -   [**privado**]{.hl-purple}: solo tendrán acceso al repositorio aquellos usuarios a los que se lo permitas. No se podrá visualizar ni instalar nada de él fuera de Github.

![](img/repo_init_2.png)

En este caso concreto, dado que será un repositorio de presentación, lo [**haremos público**]{.hl-yellow}.

---

## Nuestro primer repositorio

-   [**Add a README file**]{.hl-purple}: un README file será el archivo donde incluiremos las [**instrucciones y detalles de uso**]{.hl-yellow} a los demás (en el caso de `{peRReo}` era el archivo que contenía los detalles de instalación)

![](img/repo_init_3.png)

De momento [**ignoraremos los demás campos**]{.hl-red} para este primer repositorio.

---

## Nuestro primer repositorio

![](img/repo_init_1.png)

Por defecto Github asume que este repositorio, con el [**mismo nombre que nuestro usuario**]{.hl-yellow} será el repositorio que querremos que se presente de inicio cuando alguien entra en nuestro perfil, y será el repositorio donde \[**incluir en el README.md**\] una presentación de nosotros y un índice de tu trabajo (si quieres).

---

## Nuestro primer repositorio

![](img/profile_github_md.png)

Fíjate que ahora en nuestra [**portada tenemos dicho README.md**]{.hl-yellow} que podemos personalizar a nuestro gusto haciendo uso de [**html y markdown**]{.hl-yellow}.

Aquí puedes ver [**algunos ejemplos de README.MD**](https://github.com/matiassingers/awesome-readme)

---

## Repo de código

Una vez que tenemos nuestro README de presentación (recuerda que puedes [**personalizar a tu gusto con html y markdown**]{.hl-yellow}) vamos a crear un [**repositorio de código**]{.hl-yellow}.

. . .

Si ya era importante [**trabajar con proyectos**]{.hl-yellow} en `RStudio`, cuando lo combinamos con Github es aún más crucial que creemos un proyecto antes de subir el código, así que vamos a crear uno de prueba que se llame `repo-github-1`.

. . .

En dicho proyecto vamos a [**crear un script**]{.hl-yellow} (en mi caso llamado **codigo.R**) en el que deberás hacer los siguientes pasos:

---

## Repo de código

1.  [**Carga**]{.hl-yellow} directamente desde la página del [ISCIII](https://cnecovid.isciii.es/covid19/resources) el archivo llamado `casos_hosp_uci_def_sexo_edad_provres.csv`

```{r}
#| eval: false
#| code-fold: true
# Carga de datos desde ISCIII
datos_covid <- read_csv(file = "https://cnecovid.isciii.es/covid19/resources/casos_hosp_uci_def_sexo_edad_provres.csv")
```

. . .

2.  [**Filtra**]{.hl-yellow} datos de Madrid (`"M"`), de 2020 y con sexo conocido (hombre/mujer). Tras ello quédate con las columnas `fecha`, `sexo`, `grupo_edad`, `num_casos` (ese orden). Por último obtén la suma de casos diarios por fecha y sexo.

```{r}
#| eval: false
#| code-fold: true
# Depuración
datos_madrid <-
  datos_covid |>
  # Filtrado por Madrid y fecha
  filter(provincia_iso == "M" & fecha <= "2020-12-31" & sexo != "NC") |> 
  # Selección de columnas
  select(provincia_iso:fecha, num_casos) |> 
  # Resumen de casos diarios por fecha y sexo
  summarise(num_casos = sum(num_casos), .by = c(fecha, sexo))
  
```

---

## Repo de código

3.  [**Exporta el dataset a un csv**]{.hl-yellow} en una carpeta que se llame `exportado`

```{r}
#| eval: false
#| code-fold: true
# Exportamos datos
write_csv(datos_madrid, file = "./exportado/datos_madrid.csv")
```

. . .

::: columns
::: {.column width="40%"}
4.  Crea una [**gráfica de líneas**]{.hl-yellow} que tenga en el eje x fecha, en el eje y casos, con una curva por sexo (gráficas separadas).

```{r}
#| eval: false
#| code-fold: true
# Gráfica
ggplot(datos_madrid) +
  geom_line(aes(x = fecha, y = num_casos, color = sexo),
            alpha = 0.6, linewidth = 0.7) +
  scale_color_manual(values = c("#85519D", "#278862")) +
  facet_wrap(~sexo) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
:::

::: {.column width="60%"}
![](./img/ggplot.png){width="380"}
:::
:::

. . .

5.  Tras ello [**exportamos la gráfica**]{.hl-yellow}

```{r}
#| eval: false
ggsave(filename = "./exportado/ggplot.png", plot = last_plot(),
       bg = "white", width = 12, height = 8)
```

---

## Repo de código

¿Cómo [**subimos el proyecto**]{.hl-yellow}? Vamos de nuevo a [**crear un proyecto de cero**]{.hl-yellow}. Antes no hemos hablado de dos campos importantes:

::: columns
::: {.column width="50%"}
![](./img/repo-1.png){width="460"}
:::

::: {.column width="50%"}
-   `Add .gitignore` nos permitirá [**seleccionar el lenguaje**]{.hl-yellow} en el que estará nuestro proyecto para que Github lo entienda al sincronizar (y no actualice cosas que no deba).

-   `Choose a license` nos permitirá [**seleccionar la licencia**]{.hl-yellow} que determinará las condiciones en las que otros podrán reusar tu código.
:::
:::

---

## Repo de código

::: columns
::: {.column width="50%"}
![](./img/repo-1-1.png)
:::

::: {.column width="50%"}
Si te fijas traer crearlo tenemos solo 3 archivos: el de licencia, el .gitignore y el readme.md (donde deberíamos escribir una guía de uso de lo que hayamos subido)
:::
:::

Para subir los archivos vamos a clickar en [**Add file \< Upload File**]{.hl-purple} y [**arrastraremos TODOS los archivos**]{.hl-yellow} de la carpeta de nuestro proyecto.

---

## Repo de código

Tras la subida de archivos tendremos un cuadro llamado [**Commit changes**]{.hl-purple}

![](./img/commit-1.png)

Un [**commit**]{.hl-purple} es una [**modificación del repositorio**]{.hl-yellow} con algo que se añade/elimine/modifique, y dicho cuadro es recomendable usarlo para [**resumir en qué consiste la modificación**]{.hl-yellow}, de manera que quede trazado el cambio.

---

## Repo de código

Haciendo click en el reloj donde indica el [**número de commits**]{.hl-yellow} accedemos al [**histórico de commits (cambios)**]{.hl-yellow} con hora, día, autor, comentarios, etc.

![](./img/commit-reloj.png)

---

## Repo de código

Vamos a realizar un [**cambio en nuestro código**]{.hl-yellow}: en tu código local (local --\> tu ordenador), en lugar de filtrar por Madrid haz el [**filtro por Barcelona**]{.hl-yellow}, guarda el código y sube en el repositorio el nuevo archivo (con el mismo nombre, Github hará la sobrescritura)

```{r}
#| eval: false
#| code-line-numbers: "3"
datos_bcn <-
  datos_covid |>
  filter(provincia_iso == "B" & fecha <= "2020-12-31" & sexo != "NC") |> 
  select(fecha, sexo, grupo_edad, num_casos) |> 
  summarise(num_casos = sum(num_casos), .by = c(fecha, sexo))
```

---

## Consulta de commits

![](./img/barcelona_covid.png){width="550"}

::: columns
::: {.column width="40%"}
Si ahora [**consultamos el commit**]{.hl-yellow}, al lado hay un número que lo identifica, y clickando en él nos resume los cambios: no solo [**almacena todas las versiones pasadas**]{.hl-yellow} sino que además nos [**muestra las diferencias entre los archivos cambiados**]{.hl-yellow}
:::

::: {.column width="60%"}
![](./img/commit_cambio.png)
:::
:::

---

## Trazabilidad de cambios

::: columns
::: {.column width="72%"}
![](./img/commit_split.png) ![](./img/commit_unified.png)
:::

::: {.column width="28%"}
Tenemos [**dos modos de visualización**]{.hl-yellow} de los cambios: el modo split nos muestra el antiguo y el nuevo, con las inclusiones en verde y lo que ya no está en rojo; y el modo unified nos muestra todo en un mismo documento.
:::
:::

---

## Recuperación de commits

![](./img/browse-repo.png)

Github nos permite incluso [**recuperar una versión del pasado**]{.hl-yellow} de nuestro repositorio, haciendo click en el tercer icono del commit.

---

## Recuperación de commits

![](./img/branch-commit.png)

Si te fijas ahora al lado de `1 branch` tenemos un [**menú desplegable**]{.hl-yellow} en el que antes ponía `main` y ahora un número identificador del commit. Ya hablaremos de la idea de [**rama (branch)**]{.hl-yellow}

---

## Repo con rmd/qmd

::: columns
::: {.column width="60%"}
Vamos a poner en práctica lo aprendido:

1.  Crea un nuevo repositorio en Github (llamado `repo-github-2`) donde habrá alojado con proyecto de R.

2.  Crea un proyecto en `RStudio` que se llame (por ejemplo) `proyecto-qmd`

3.  Una vez dentro del proyecto en `RStudio` haz click en `File < New File < Quarto Document`
:::

::: {.column width="40%"}
![](./img/primer-qmd.png)
:::
:::

Deberás tener un documento similar a este: un [**quarto markdown (.qmd)**]{.hl-yellow}, un documento que nos permitirá incluir [**markdown + código**]{.hl-yellow} (puede ser `R` o puede ser `Observable`, `D3`, etc).

---

## Repo con rmd/qmd

Este formato es ideal para:

-   [**Trabajar en equipo**]{.hl-yellow} construyendo el borrador de una pieza.
-   Tomar [**apuntes o informes**]{.hl-yellow} para uno mismo.
-   [**Presentar**]{.hl-yellow} tu trabajo a tus compañeros.

::: columns
::: {.column width="50%"}
![](./img/prueba-qmd-html.png)
:::

::: {.column width="50%"}
Si te fijas ahora nuestro repositorio tiene un archivo con formato `.html`...es decir...

[**¡Es una web!**]{.hl-yellow}
:::
:::

---

## Github pages

¿Cómo [**convertir nuestro repositorio en una web**]{.hl-yellow}?

![](./img/github-pages.png)

1.  Haz click en `Settings`
2.  Ve al apartado `Pages`
3.  En el subapartado `branch` selecciona la única rama que tenemos ahora (`main`)
4.  Selecciona la carpeta donde tengas el `.html` (en web complejas estará como en cualquier web en `docs`, en algo simple estará en la ruta raiz del repositorio)
5.  Haz click en `Save`

---

## Github pages

Si te fijas en la [**parte superior del repositorio**]{.hl-yellow} ahora tenemos un icono naranja, que nos indica que la [**web está en proceso de ser desplegada (deploy)**]{.hl-orange}

![](./img/github-naranja.png)

---

## Github pages

Pasados unos segundos (dependiendo del tamaño de la web y tu conexión a internet) ese [**icono pasará a ser un check verde**]{.hl-green}: habemus web

![](./img/github-verde.png)

El [**link de la web por defecto**]{.hl-yellow} será `{nombre_usuario}.github.io/{nombre_repo}`

---

## Github pages

![](./img/github-pages-deploy.png)

¡Un momento! Ahora mismo nuestra web [**no nos está mostrando nuestro .qmd**]{.hl-red}, sino por defecto el [**README.md**]{.hl-yellow}.

. . .

Para que Github entienda que queremos visualizar ese `.html` que hemos generado a partir del `.qmd` vamos en nuestro proyecto local a [**borrar**]{.hl-yellow} todo lo que no sea nuestro archivo `.Rproj` y nuestro archivo `.qmd`, y vamos a [**cambiar el nombre**]{.hl-yellow} a este último llamándolo `index.qmd`, y volvemos a compilarlo para [**generar un index.html**]{.hl-yellow}

---

## Github pages

Vamos a [**subir a Github ese nuevo proyecto**]{.hl-yellow} con el cambio de nombre (llamado `repo-github-3`) para ver luego las diferencias entre uno y otro

![](./img/repo-index-html.png)

---

## Github pages

Si [**repetimos el proceso para hacer una Page**]{.hl-yellow} y esperamos al tick verde...

::: columns
::: {.column width="50%"}
![](./img/index-html-qmd.png)
:::

::: {.column width="50%"}
Si a tu `.qmd` ya le llamas de inicio `index.qmd`, automáticamente, al detectar Github un `index.html`, interpreta que ese [**archivo index.html**]{.hl-yellow} es el que define la web (y puedes personalizar añadiendo un archivo `css` de estilos)

[**Habemus web**]{.hl-green} simplemente clickando en Pages :)
:::
:::

---

## Repo con diapositivas

Vamos a crear el último repositorio que se llamará `repo-diapos`, y crear un proyecto en `RStudio` del mismo nombre (por ejemplo). Una vez creado le daremos a `File < New File < Quarto Presentation`.

::: columns
::: {.column width="45%"}
![](img/quarto-slides.png)
:::

::: {.column width="55%"}
La forma de escribir será igual que un `.qmd` normal solo que ahora [**cada diapositiva la separaremos**]{.hl-yellow} con un `---` (usando archivos de estilos podemos personalizar lo que queramos)

Llama al archivo directamente `index.qmd`, súbelo a Github y con un click en Pages tienes una [**web con tus diapositivas**]{.hl-yellow}
:::
:::

---

## Uso de Gitkraken

La forma más [**sencilla para trabajar de manera colaborativa**]{.hl-yellow} en Github, y tenerlo sincronizado con nuestro local, es hacer uso de [Gitkraken](https://www.gitkraken.com/download)

::: columns
::: {.column width="50%"}
![](img/gitkraken-repo.png)
:::

::: {.column width="50%"}
Una vez dentro clickamos en el icono de la carpeta (`Repo Management`) y si ya tenemos el repositorio en Github seleccionamos `Clone`, indicando donde queremos clonar (en nuestro local) y que [**repositorio de Github queremos clonar**]{.hl-yellow}.
:::
:::

---

## Uso de Gitkraken

::: columns
::: {.column width="50%"}
![](img/view-change.png)
:::

::: {.column width="50%"}
Una vez clonado, la idea es que cada [**cambio que hagamos en local nos aparecerá en Gitkraken**]{.hl-yellow} como `View changes`.
:::
:::

---

## Uso de Gitkraken

Cuando tengas suficientes cambios como para [**actualizar el repositorio**]{.hl-yellow} (tampoco tiene sentido actualizar con cada edición), verás algo similar a esto con todos los [**commits realizados**]{.hl-yellow}

::: columns
::: {.column width="50%"}
![](img/stage-all-changes.png)
:::

::: {.column width="50%"}
Podrás decidir cuáles de los [**commits locales quieres incluir en remoto**]{.hl-yellow}, bien uno a uno o en `Stage all changes` (para todos)
:::
:::

---

## Uso de Gitkraken

Tras incluir los commits deberás incluir un [**título y descripción del commit**]{.hl-yellow}

![](img/titulo-commit.png)

---

## Uso de Gitkraken

Tras hacerlo verás que ahora tenemos [**dos iconos separados en una especie de árbol**]{.hl-yellow} (¿te acuerdas de la `branch` o rama?):

-   [**Ordenador**]{.hl-purple}: la versión del repositorio que tienes en tu [**ordenador**]{.hl-yellow}.

-   [**Logo**]{.hl-purple}: la versión del repositorio que tienes [**subida en remoto**]{.hl-yellow}

![](img/split-gitkraken.png)

---

## Uso de Gitkraken

Mientras eso suceda solo tendrás sincronizado tu ordenador con Gitkraken, pero no con Github. Para ello haremos [**click en Push**]{.hl-yellow} (con `Pull` podrás forzar a tener en local lo mismo que en remoto).

![](img/push-gitkraken.png)

---

## Branchs

Como hemos mencionado ya en varias ocasiones, hay un elefante en la habitación que aún no hemos mentado: las [**ramas o branchs**]{.hl-yellow} de un repositorio.

. . .

Imagina que estáis trabajando varios en un proyecto y teneís una versión que funciona pero que queréis [**modificar en paralelo a partir del estado actual**]{.hl-yellow} del repositorio.

. . .

Las [**ramas**]{.hl-yellow} nos permiten partir de una versión común del repositorio y hacer cambios que [**no afecten a los demás**]{.hl-yellow}

---

## Branchs

Para [**crear una rama**]{.hl-yellow} a partir del estado actual de repositorio haremos click en `Branch` y le pondremos un nombre

![](img/branch-button.png)

Una vez creada verás [**dos iconos**]{.hl-yellow} y un [**menú desplegable**]{.hl-yellow} con las distintas ramas en las que quieres hacer el commit. Imagina que realizas un cambio pero [**no quieres añadirlo a la rama principal**]{.hl-yellow}: puedes hacer el [**commit en tu rama propia en LOCAL**]{.hl-yellow} (lo harás en la rama activa de tu menú de branchs).

---

## Branchs

La primera vez te pedirá que escribas la [**rama en REMOTO**]{.hl-yellow} con la quieres sincronizar tu rama en local. [**Consejo**]{.hl-green}: ponle el mismo nombre en remoto que en local.

![](img/name-branch.png)

---

## Branchs

Fíjate que ahora tenemos el ordenador y el logo en el mismo sitio. Esto no significa que tengas ambas ramas en tu local, solo que [**Gitkraken tiene ambas sincronizadas**]{.hl-yellow}: clickando en cualquiera de ellas, tus archivos en tu ordenador cambiarán.

![](img/both-branchs.png)

---

## Pull request

Lo más recomendable es que [**solo se incorpore de una rama secundaria**]{.hl-yellow} a la rama principal aquello que está [**validado por un/a coordinador/a**]{.hl-yellow} del repositorio, asegurándose que todo funciona correctamente.

Cuando queramos incluirlo haremos [**click con botón derecho**]{.hl-yellow} en el icono de la rama secundaria y seleccionamos `Start a pull request to origin from...`

![](img/pull-request-menu.png)

. . .

Una [**pull request**]{.hl-yellow} será una [**petición al responsable de la rama principal**]{.hl-yellow} para incluir los cambios

---

## Pull request

::: columns
::: {.column width="60%"}
![](img/create-pull-request.png)
:::

::: {.column width="40%"}
En el cuadro que no se abre deberemos escribir:

-   La [**rama**]{.hl-yellow} a la que hacer el `merge` (normalmente la `main`)
-   Título y resumen de los cambios
-   Puedes incluso asignar un [**revisor**]{.hl-yellow} entre los colaboradores del repo.
-   Puedes asignar [**etiquetas**]{.hl-yellow}
:::
:::

---

## Pull request

Mientras no se acepte aparecerá un [**icono de rama**]{.hl-yellow} y un +1 en Pull Requests

![](img/pending-branch.png){width="500"}

. . .

Si somos al mantenedor del repositorio, haciendo click en el menú nos saldrán las ramas que nos quieren hacer hacer `merge`

![](img/menu-pr.png){width="500"}

---

## Pull request

Al hacer click se abrirá un [**cuadro de Pull Request**]{.hl-yellow} para decidir si

-   [**Revisar**]{.hl-yellow} los cambios
-   [**Aprobar**]{.hl-yellow} el `merge`
-   [**Añadir comentarios**]{.hl-yellow} al que ha solicitado el `merge` por si queremos solicitar algún cambio [**antes de ser aprobado**]{.hl-yellow}

## ![](img/menu-pr-2.png)

## Pull request

Tras revisar todo y aprobarlo clickaremos en `Confirm merge`, y tras ello podremos decidir si esa rama que era paralela a la principal la queremos [**eliminar**]{.hl-yellow} o dejar visible a todos (consejo: dejar visible para tene [**trazabilidad**]{.hl-yellow} del proyecto de trabajo)

::: columns
::: {.column width="50%"}
![](img/merge-branch.png)
:::

::: {.column width="50%"}
![](img/delete-branch.png)
:::
:::

