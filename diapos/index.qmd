---
title: "Aprendizaje Supervisado I"
subtitle: "M√©todos de predicci√≥n lineal"
title-slide-attributes:
  data-background-image: img/data-science-2.jpeg
  data-background-size: cover
  data-background-opacity: "0.2"
author: "Grado en Ciencia de Datos Aplicada ‚Ä¢ curso 2023-2024"
affiliation: Facultad de Estudios Estad√≠sticos (UCM)
lang: es
language: custom_lang.yml
format: 
  revealjs:
    theme: [default, style.scss]
    chalkboard: true
    multiplex: true
    menu:
      side: left
      width: normal
    footer: "[<strong>Javier √Ålvarez Li√©bana</strong>](...) ‚Ä¢ Grado en Ciencia de Datos Aplicada (UCM) ‚Ä¢ curso 2023-2024"
    slide-number: c/t
execute:
  echo: true
---

# Empieza lo bueno...empieza la modelizaci√≥n

[**Vamos a juntar las piezas del puzzle para hacer ¬´magia¬ª**]{style="color:#444442;"}

```{r}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
```

---

## ¬°Buenas!

[**Correo**]{.hl-green}: **<javalv09@ucm.es>**. [**Despacho**]{.hl-green}: 722 (3¬™ planta). [**Tutor√≠as**]{.hl-green}: lunes (14:30-16:00), martes (13:00-15:00) y viernes (13:00-14:00).

::: columns
::: {.column width="30%"}
![](img/me.jpeg)
:::

::: {.column width="70%"}
::: incremental
-   [**Javier √Ålvarez Li√©bana**]{.hl-yellow}, de Carabanchel (Bajo).

-   Licenciado en Matem√°ticas (UCM). [**Doctorado en estad√≠stica**]{.hl-yellow} (UGR).

-   Encargado de la [**visualizaci√≥n y an√°lisis de datos covid**]{.hl-yellow} del Principado de Asturias (2021-2022).

-   Miembro de la [**Sociedad Espa√±ola de Estad√≠stica e IO**]{.hl-yellow} y la [**Real Sociedad Matem√°tica Espa√±ola**]{.hl-yellow}.
:::
:::
:::

. . .

Actualmente, [**investigador y docente en la Facultad de Estad√≠stica de la UCM**]{.hl-yellow}. Divulgando por [**Twitter**](https://twitter.com/dadosdelaplace) e [**Instagram**](https://instagram.com/javieralvarezliebana)


---

## Objetivos

::: columns
::: {.column width="37%"}
![](https://assets-global.website-files.com/6092cb6b4ac959f39728dd26/6188a97fa499b5fbfe410417_target%20(1).png)
:::

::: {.column width="63%"}
::: incremental
- Empezar a [**relacionar asignaturas**]{.hl-yellow} como matem√°ticas, inferencia y R.

- Aprender los fundamentos del [**aprendizaje estad√≠stico**]{.hl-yellow} (ahora llamado Machine Learning o data science)

- Pasar de los descriptivo a lo predictivo: [**construir nuestros primeros modelos**]{.hl-yellow}

- Entender en profundidad el contexto de la [**predicci√≥n lineal**]{.hl-yellow}.


:::
:::
:::

---

## Evaluaci√≥n

-   [**Asistencia**]{.hl-yellow}. Se [**valorar√° muy positivamente**]{.hl-purple} la participaci√≥n. Si se [**restar√°n puntos si eres expulsado de clase**]{.hl-purple}: -0.3 la 1¬™ vez, -0.6 la 2¬™, -1.2 la 3¬™...

. . .

- [**Evaluaci√≥n continua**]{.hl-yellow}: 1 **examen a papel** (25%), **2 entregas R en clase** (20%-30%) y **entrega grupal √∫ltimo d√≠a** (3-6p obligatoriamente, 25%)
  
. . .

- [**Examen final**]{.hl-yellow}:
  - [**Menos de un 6 (sin la grupal)**]{.hl-purple} -> examen de R en lugar de la grupal.
  - [**M√°s de un 6.5 (con la grupal)**]{.hl-purple} -> podr√°s decidir **peso del final entre un 20% y un 100%**.
  - [**Entre 5 y 6.5 (con la grupal)**]{.hl-purple} -> podr√°s decidir **peso del final entre un 40% y un 100%**.
  - [**Por debajo de 5 (con la grupal)**]{.hl-purple} -> podr√°s decidir **peso del final entre un 70% y un 100%**.
  

---

## Planificaci√≥n


* [**Entrega I (20%)**]{.hl-yellow}: semana 27-29 de febrero de 2024

* [**Entrega II (30%)**]{.hl-yellow}: semana 2-4 de abril de 2024

* [**Parcial con papel y boli (25%)**]{.hl-yellow}: 16-18 de abril de 2024 

* [**Entrega grupal (25%)**]{.hl-yellow}: 9 de mayo de 2024 (incluyendo lo visto hasta el 30 de abril)

&nbsp;

(se admiten propuestas)

. . . 

[**Clase a recuperar otro d√≠a**]{.hl-yellow}: clase del 21 de marzo.

---

## Materiales

* [**Diapositivas**]{.hl-yellow}: las diapositivas que usaremos en el aula a lo largo del curso, estructuradas por clases, estar√°n disponibles y actualizadas en **<https://javieralvarezliebana.es/docencia-R-supervisado-2324/diapos>** 

En el men√∫ de las diapositivas (abajo a la izquierda) tienes una [**opci√≥n para descargarlas en pdf**]{.hl-yellow} en `Tools` (consejo: no lo hagas hasta el final del curso ya que ir√°n modific√°ndose)
  
&nbsp;

* [**Material**]{.hl-yellow}: [**scripts de cada tema**](https://github.com/dadosdelaplace/docencia-R-supervisado-2324/tree/main/material) y materiales extras

* [**Res√∫menes de paquetes**]{.hl-yellow}: [**chuletas de los paquetes**](https://github.com/dadosdelaplace/docencia-R-supervisado-2324/tree/main/fichas%20paquetes) en formato .pdf

---


## Requisitos

Para el curso los √∫nicos requisitos ser√°n:

1.  [**Conexi√≥n a internet**]{.hl-yellow} (para la descarga de algunos datos y paquetes).

2.  [**Instalar R y RStudio**]{.hl-yellow}: la descarga la haremos (gratuitamente) desde <https://cran.r-project.org/> y  <https://posit.co/download/rstudio-desktop/>

3. Se dar√°n por asumido conocimientos aprendidos de [**R base, tidyverse y ggplot**]{.hl-yellow}

4. Se dar√°n por asumido conocimientos aprendidos de [**Quarto, diapositivas en Quarto y Github**]{.hl-yellow}. Para las entregas [**SOLO SE VALORAR√Å**]{.hl-purple} la salida html correspodiente.

5. [**Recomendable**]{.hl-yellow}: saber usar la calculadora en modo estad√≠stico.


---

## Listado de clases

* [**Clase 1**](#clase-1): repaso de descriptiva
* [**Clase 2**](#clase-2): importancia de la visualizaci√≥n
* [**Clase 3**](#clase-3): intro al aprendizaje estad√≠stico. Sesgo vs varianza. Supervisado vs no supervisado. Correlaci√≥n vs dependencia vs causalidad.


# Clase 1: repaso {#clase-1}

[**Objetivo de la predicci√≥n lineal. Concepto de linealidad. Repaso de estad√≠stica descriptiva**]{style="color:#444442;"}

---

## ¬øQu√© es predecir?

Como veremos m√°s adelante, en esta asignatura vamos a tratar principalmente lo que se conoce el aprendizaje estad√≠stico como [**predicci√≥n (continua)**]{.hl-yellow}

. . .

Dada una [**variable objetivo (variable dependiente)**]{.hl-yellow}, y con la informaci√≥n aportada por un conjunto de [**variables predictoras (covariables)**]{.hl-yellow}, el objetivo ser√° construir un modelo que consiga dar una estimaci√≥n/predicci√≥n lo ¬´mejor posible¬ª

. . .

Es importante que - de momento - distingamos dos conceptos:

* [**Estimaci√≥n**]{.hl-yellow}: el modelo aprende de unos datos e intenta estimar dichos valores que ha usado.
* [**Predicci√≥n**]{.hl-yellow}: el modelo aprende de unos datos e intenta estimar valores que el **modelo no conoce**.

M√°s adelante los llamaremos ¬´predicci√≥n en train¬ª y ¬´predicci√≥n en test¬ª

---

## ¬øQu√© es la linealidad?

En matem√°ticas decimos que una funci√≥n $f(x)$ es [**lineal**]{.hl-yellow} cuando se cumple:

* [**Propiedad aditiva**]{.hl-purple}: $f(x + y) = f(x) + f(y)$

* [**Propiedad homog√©nea**]{.hl-purple}: $f(k*x) = k*f(x)$ (donde $k$ es una constante en $\mathbb{R}$).

Ambas se pueden resumir en $f(a*x + b*y) = a*f(x) + b*f(y)$

. . .

En estad√≠stica llamamos [**modelo de predicci√≥n lineal**]{.hl-yellow} a un modelo que usa la informaci√≥n de covariables $X_1, X_2, \ldots, X_p$, de manera que su informaci√≥n siempre [**se relacionen entre s√≠ con sumas y restas**]{.hl-yellow}.

- [**Ejemplos lineales**]{.hl-green}: $y = 2*x_1 - 3$ o  $y = 4 - \frac{x_1}{2} + 3*x_2$

- [**Ejemplos no lineales**]{.hl-red}: $y = 2*\frac{1}{x_1}$ o  $y = 4 - x_{1}^{2} - x_2$ o $y = ln(x_1) + cos(x_2)$

En general son modelos que se pueden representar con rectas (una sola predictora), planos (dos predictoras), etc.

---

## Repaso: continua vs discreta

* **Caracteres**: cada una de las caracter√≠sticas o cualidades que se podr√≠an medir o analizar para cada individuo de la poblaci√≥n (y de los que disponemos el valor para cada individuo de la muestra).

* **Modalidades**: diferentes valores que puede adoptar una caracter√≠stica o variable.

&nbsp;

Como veremos m√°s adelante, en el √°mbito del aprendizaje estad√≠stico va a ser fundamental tener clara la diferencia entre una [**variable cualitativa y cuantitativa**]{.hl-purple} y entre [**variable continua y otra discreta**]{.hl-yellow}. ¬øCu√°l es la diferencia?


---

## Repaso: continua vs discreta

Imagina las siguientes variables:

- ¬øTienes hermanos? (si/no)
- Color de zapatillas
- Nivel de estudios
- N√∫mero de hermanos
- N√∫mero de pelos en la cabeza
- Resultado de un dado
- Temperatura ¬∫C
- Estatura o peso

&nbsp;

[**¬øCu√°l es la diferencia entre ellas**]{.hl-yellow}


---

## Repaso: continua vs discreta



* [**Cualitativas**]{.hl-yellow}: **cualidades o categor√≠as**. Ejemplos: sexo, estado civil, estudios, etc.
  * [**Ordinales**]{.hl-purple}: admiten relaci√≥n jerarquica (suspenso-aprobado-bien, sano-leve-grave).
  * [**Nominales**]{.hl-purple}: no tienen asociada una jerarqu√≠a (sexo, religi√≥n, color de zapatillas, etc).

. . .

* [**Cuantitativas**]{.hl-yellow}: cuantificaci√≥n num√©rica.
  * [**Discretas**]{.hl-purple} (barras): se pueden contar y enumerar (aunque sean infinitos) (granos de arena, n¬∫ hermanos, etc) ‚Üí se **suman**
  * [**Continuas**]{.hl-purple}: toman infinitos valores y entre dos valores cualesquiera hay a su vez infinitas opciones (estatura, peso, etc) ‚Üí se **integran**

---

## Repaso: continua vs discreta

![](img/vitro-fuego-discretas.jpg)


---

## Repaso: poblaci√≥n vs muestra

En estad√≠stica llamaremos [**poblaci√≥n**]{.hl-yellow} al universo te√≥rico, al colectivo de individuos a estudiar, o de posibles elementos/eventos de los podr√≠amos tener observaciones (ejemplo: 47 millones de espa√±oles).

. . .

Cada uno de esos elementos o eventos se llaman **individuos**.

. . .

Aunque nuestro objetivo ser√° conocer algunas de las propiedades de la poblaci√≥n, la [**poblaci√≥n suele ser inaccesible**]{.hl-red} en su totalidad ‚Üí [**SELECCI√ìN**]{.hl-green} de un conjunto de individuos

---

## Repaso: poblaci√≥n vs muestra

Para ello en estad√≠stica usamos lo que se conoce como [**muestra**]{.hl-yellow}: un subconjunto finito, de tama√±o $n$, ¬´representativo¬ª de la poblaci√≥n (en estudio estad√≠stico realizado sobre la totalidad de una poblaci√≥n se denomina **censo**).

. . .

Para ello existe una rama conocida como [**muestreo**]{.hl-yellow}

* [**Probabil√≠stico**]{.hl-purple}: todos los individuos tienen oportunidad de ser seleccionados, y la probabilidad de que suceda puede ser modelizada (al azar, por estratos, etc).

* [**No probabil√≠stico**]{.hl-purple}: algunos elementos de la poblaci√≥n no tienen posibilidad de selecci√≥n (sesgo de exclusi√≥n), o su probabilidad no puede ser conocida.

. . .

ü§î ¬øSer√≠a adecuado hacer una encuesta sobre el streamer favorito de los j√≥venes a trav√©s de una encuesta realizada por tel√©fono fijo?


---

## Sesgos en el muestreo

![](img/sampling-bias.jpg)

. . .

[**Sesgo de selecci√≥n**]{.hl-yellow}: aparece cuando no se tiene en cuenta la forma en la que se han recogido los datos.

--- 


## Sesgos en el muestreo


![](img/dewey.jpg)

El ejemplo m√°s famoso es el caso [**¬´Dewey defeats Truman¬ª (Dewer derrota a Truman)**]{.hl-yellow}, el titular con el que abri√≥ el Chicago Tribune en 1948, el mismo d√≠a en el que Truman gan√≥ al rep√∫blicano Dewer en las elecciones de 1948: sin esperar a los resultados, se basaron en una encuesta telef√≥nica (sin contar con el sesgo que, en aquella √©poca, solo la clase alta ten√≠a tel√©fono).

---



## Sesgos en el muestreo

![](img/superviviente.jpg)

¬øD√≥nde reforzar√≠as los aviones?

---

## Sesgos en el muestreo

![](img/superviviente.jpg)



El [**sesgo del superviviente**]{.hl-yellow} (un tipo de sesgo de selecci√≥n) aparece cuando se toma una muestra de un fen√≥meno ignorando si los individuos elegidos tienen las mismas opciones respecto al mismo.


---

## Repaso: medidas de centralizaci√≥n

* [**Media**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la media muestral $\overline{x}$ se define como la **suma de todos los valores dividida por el tama√±o muestral**

$$\overline{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$

. . .

[**Geom√©tricamente**]{.hl-purple}: es el **valor ¬´m√°s cercano¬ª de todos los datos a la vez** (minimiza las distancias al cuadrado)

---

## Media muestral


[**VENTAJAS**]{.hl-green}

* F√°cil de calcular y entender
* F√°cil y eficiente de programar
* Siempre existe (para cuantitativas)

. . .

[**DESVENTAJAS**]{.hl-red}

* No es un valor de los datos (la media de {1, 2, 3, 4} es 2.5)
* **Poco robusta** (valores at√≠picos le afectan mucho)
* Solo se puede definir para variables cuantitativas


---

## Repaso: medidas de centralizaci√≥n

* [**Mediana**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la mediana muestral se define como el **valor que es mayor o igual que al menos el 50%**, y menor igual que al menos el 50% de los datos

$$Me_{x} = \arg \min_{x_i} \left\lbrace F_i > 0.5 \right\rbrace, \quad Me_x = e_{i-1} + \frac{0.5 - F_{i-1}}{F_i - F_{i-1} }a_i$$

La mediana es el [**valor de en medio**]{.hl-purple} si ordenamos los datos (y si se pueden ordenar...)

---

## Mediana muestral


[**VENTAJAS**]{.hl-green}

* Suele ser un valor de la muestra
* Un poco m√°s robusta que la media


. . .

[**DESVENTAJAS**]{.hl-red}

* Muy ineficiente (requiere un algoritmo de ordenaci√≥n)
* Solo definida para  cuantitativas o cualitativas ordinales

---


## Repaso: medidas de centralizaci√≥n

* [**Moda**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la moda muestral se define como el **valor o valores m√°s repetidos** (en caso de que existan).

$$Mo_x = \arg \max_{x_i} f_i, \quad Mo_x = e_{i-1} + \frac{d_i - d_{i-1}}{\left(d_i - d_{i-1} \right) + \left(d_i - d_{i+1} \right)}a_i$$


. . .

[**Gr√°ficamente**]{.hl-purple}: representa el ¬´pico¬ª de un diagrama de barras o un histograma

---

## Moda muestral


[**VENTAJAS**]{.hl-green}

* Es un valor de la muestra
* Muy robusta
* **Se puede calcular para cualquier** cuanti o cuali



. . .

[**DESVENTAJAS**]{.hl-red}

* No siempre existe (amodal) y pueden existir varias (bimodal, trimodal, etc)
* Poco usada en inferencia


---

## Repaso: medidas de centralizaci√≥n

![](img/ine-salarios-oculto.jpg)

**¬øCu√°l es la mediana, la media y la moda?**

---

## Repaso: medidas de centralizaci√≥n

![](img/ine-salarios.jpg)

---

## Repaso: medidas de dispersi√≥n

![](img/iker-jimenez.jpg)

¬øQu√© tiene que ver la imagen con la dispersi√≥n?


---

## Repaso: medidas de dispersi√≥n

![](img/extremos.jpg)

El cambio clim√°tico no solo es porque aumente la [**temperatura media (centralizaci√≥n)**]{.hl-yellow} sino por la aparici√≥n cada vez m√°s frecuente de fen√≥menos extremos 


Aumento de la [**variabilidad**]{.hl-yellow} ‚Üí aumento de la [**DISPERSI√ìN**]{.hl-yellow}

---

## Repaso: medidas de dispersi√≥n

[**¬øC√≥mo medir lo que se alejan los datos de la media?**]{.hl-yellow}

. . .

![](img/primera-idea-varianza.jpg)

Una **primera idea** podr√≠a ser [**medir la distancia de cada dato al centro**]{.hl-yellow}, es decir, restar cada dato de la media, y despu√©s realizar su promedio.

---

![](img/wait-for-it.jpg)

---

## Repaso: medidas de dispersi√≥n

Imagina que tenemos la siguiente muestra $X = \left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$.

**¬øCu√°nto vale la media?**

. . .

La media vale 0 y la distancia a ella es...la propia muestra $\left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$. **¬øCu√°l es el promedio de dichas distancias?**

. . .

Pues...de nuevo vale 0.

. . .

Si la dispersi√≥n es 0...[**¬øno hay dispersi√≥n?**]{.hl-red} ¬øNo deber√≠a de dar 0 solo cuando los datos sean constantes?

---

## Repaso: medidas de dispersi√≥n

Para **evitar que se cancelen** los signos lo que haremos ser√° calcular el [**promedio PERO de las distancias al cuadrado**]{.hl-yellow}, la conocida como [**varianza**]{.hl-yellow}

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 $$

. . .


::: callout-warning
# Cuidado

Tomar el valor absoluto (para evitar que se cancelen los signos) suele ser una mala idea en matem√°ticas (no es derivable como funci√≥n).
:::

---

## Repaso: medidas de dispersi√≥n


[**Problema**]{.hl-red}: si los datos est√°n en metros, la varianza estar√° en...metros cuadrados


. . .

:::: columns
::: {.column width="50%"}

¬øTiene sentido medir la dispersi√≥n de nuestra estatura en baldosas?

:::

::: {.column width="50%"}

![](img/albert-rivera.jpg)
:::

::::

---

## Repaso: medidas de dispersi√≥n

Para tener una [**medida de dispersi√≥n en las unidades de los datos**]{.hl-yellow} calcularemos la [**desviaci√≥n t√≠pica**]{.hl-yellow}, como la ra√≠z cuadrada de la varianza

$$s_{x} = \sqrt{s_{x}^{2}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2} = \sqrt{\overline{x^2} - \overline{x}^2}$$


---

## Repaso: medidas de dispersi√≥n

Todav√≠a tenemos un peque√±o problema.

Imagina que queremos **comparar la dispersi√≥n de dos conjuntos** de datos, estaturas de personas y di√°metros de n√∫cleos de c√©lulas. Y Supongamos que las medias son 170 cm y 5 micr√≥metros, y la desviaci√≥n t√≠pica de 1 cm y 1.5 micr√≥metros.

[**¬øQu√© conjunto de datos es m√°s disperso?**]{.hl-yellow}

. . .

Para tener una  **medida de dispersi√≥n adimensional** definiremos el [**coeficiente de variaci√≥n**]{.hl-yellow} 

$$CV_{x} = \frac{s_{x}}{\left| \overline{x} \right|}$$

---

## Repaso: medidas de localizaci√≥n

Las [**medidas de posici√≥n o localizaci√≥n**]{.hl-yellow} nos localizan los datos, siendo valores que **nos dividen un conjunto ordenado** en subconjuntos del mismo tama√±o (ejemplo: mediana es percentil 50).

* **Percentil**: valores $P_{\alpha}$ del conjunto ordenado que dejan por debajo, al menos, el $\alpha$% de datos y $\alpha$% por encima.       
* **Decil**: valores $D_{\alpha}$ que dividen los datos en 10 partes iguales.

* **Cuartil**: valores $C_{\alpha}$ o $q_{\alpha}$ que dividen los datos en 4 partes iguales.

---

## Repaso: covarianza y correlaci√≥n

[**¬øQu√© es en realidad la varianza?**]{.hl-yellow}

. . .

La [**varianza es el promedio de las desviaciones al cuadrado**]{.hl-yellow} (respecto a la media), apareciendo dos veces dicha desviaci√≥n: puede ser entendida como una [**medida que cuantifica la relaci√≥n de una variable CONSIGO MISMA**]{.hl-yellow}

. . .

[**¬øY si qui√©semos medir la relaci√≥n de una variable X respecto a otra variable Y (en lugar de consigo misma)?**]{.hl-purple}


---

## Repaso: covarianza y correlaci√≥n

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 \quad \text{(varianza)}$$


La idea detr√°s de la [**covarianza**]{.hl-yellow} es justo esa: sustituir una de esas desviaciones de la X por la desviaci√≥n de la Y.


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

---

## Repaso: covarianza y correlaci√≥n

Es importante entender algunas [**propiedades de la covarianza**]{.hl-yellow}

. . .

* [**Signo**]{.hl-purple}: la covarianza puede ser tanto positiva como negativa como 0: al eliminar el cuadrado de la varianza, ya no es necesario que sea positiva


. . .

* [**¬øQu√© cuantifica?**]{.hl-purple} La covarianza mide la [**relaci√≥n LINEAL**]{.hl-red} (en torno a una recta) entre dos variables

. . .

* [**¬øQu√© dice su signo?**]{.hl-purple} El signo de la covarianza nos indicar√° la [**direcci√≥n de la dependencia lineal**]{.hl-yellow}: si es positiva, la relaci√≥n ser√° creciente (cuando X crece, Y crece); si es negativa, la relaci√≥n ser√° decreciente (cuando X crece, Y decrece)


---

## Repaso: covarianza y correlaci√≥n

Al igual que pasaba con la varianza, la [**covarianza depende de las unidades y magnitudes**]{.hl-yellow} de los datos, as√≠ que lo que haremos ser√° [**estandarizar la covarianza**]{.hl-yellow}. Definiremos la [**coeficiente correlaci√≥n lineal (de Pearson)**]{.hl-yellow} como la covarianza dividida entre el producto de las desviaciones t√≠picas (adimensional)


$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

. . .

Tiene el [**mismo signo que la covarianza**]{.hl-yellow} (el denominador es siempre positivo) y sus [**valores siempre est√°n entre -1 y 1**]{.hl-yellow}

* m√°s cerca de -1 o 1 ‚Üí relaci√≥n lineal m√°s fuerte
* m√°s cerca de 0 ‚Üí ausencia de relaci√≥n **LINEAL**

---

## Repaso: covarianza y correlaci√≥n

![](img/correlaciones.jpg)


# Clase 2: importancia del dataviz {#clase-2}

[**¬øBasta con calcular la correlaci√≥n para cuantificar si un ajuste es adecuado?**]{style="color:#444442;"}


---

## Conjunto anscombe

En el paquete `{datasets}` se encuentra el dataset conocido como [**cuarteto de Anscombe**]{.hl-yellow}, un dataset que cuenta con 4 conjuntos de datos.

```{r}
anscombe_tb <- as_tibble(datasets::anscombe)
anscombe_tb
```

. . .

[**Primera tarea: convierte a tidydata**]{.hl-yellow}

---

## Conjunto anscombe

```{r}
anscombe_x <- 
  anscombe_tb |>
  pivot_longer(cols = x1:x4, names_to = "dataset", values_to = "x") |> 
  select(-contains("y")) |> 
  mutate(dataset = str_remove_all(dataset, "x"))

anscombe_y <- 
  anscombe_tb |>
  pivot_longer(cols = y1:y4, names_to = "dataset", values_to = "y") |> 
  select(-contains("x"), -dataset)

anscombe_tidy <-
  anscombe_x |> mutate("y" = anscombe_y$y)
anscombe_tidy
```

---

## Conjunto anscombe

[**¬øQu√© caracter√≠sticas muestrales tienen?**]{.hl-yellow} Calcula la media, varianza, desv. t√≠pica, covarianza y correlaci√≥n en cada dataset

. . .

```{r}
anscombe_tidy |> 
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

. . .

Si te fijas [**todos los datasets tienen los mismos momentos**]{.hl-yellow} muestrales, incluso tendr√≠an el mismo ajuste de regresi√≥n...¬øser√°n el mismo dataset desordenado?

---

## Conjunto anscombe

[**Visualiza los 4 datasets**]{.hl-yellow}

. . .

```{r}
#| code-fold: true
ggplot(anscombe_tidy) +
  geom_point(aes(x = x, y = y, color = dataset), size = 3, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~dataset) + theme_minimal()
```

---

## Conjunto anscombe

```{r}
#| echo: false
ggplot(anscombe_tidy) +
  geom_point(aes(x = x, y = y, color = dataset), size = 3, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~dataset) + theme_minimal()
```


Por suerte o por desgracia [**no todo son matem√°ticas**]{.hl-red}: antes de pensar que modelo es mejor para nuestros datos, es important√≠simo realizar un [**an√°lisis exploratorio**]{.hl-yellow} de los datos (incluyendo visualizaci√≥n)

---

## Datasaurus

Podemos visualizarlo de manera a√∫n m√°s extrema con el dataset `datasaurus_dozen` del paquete `{datasauRus}` (ver m√°s en <https://www.research.autodesk.com/publications/same-stats-different-graphs/>)

```{r}
library(datasauRus)
datasaurus_dozen |>
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

---

## Datasaurus

```{r}
#| code-fold: true
ggplot(datasaurus_dozen |> filter(dataset != "wide_lines"),
       aes(x = x, y = y, color = dataset)) +
  geom_point(size = 2.5, alpha = 0.75) +
  geom_smooth(method = "lm", se = FALSE)  +
  facet_wrap(~dataset, ncol = 3) +
  theme_minimal()
```

---

# Clase 3: profundizando en la correlaci√≥n {#clase-3}

[**Matrices de correlaci√≥n y covarianza. Correlaci√≥n vs causalidad vs dependencia**]{style="color:#444442;"}


---

## Correlaci√≥n lineal: sin agrupar


Como dec√≠amos, la idea detr√°s de la [**covarianza**]{.hl-yellow} es una "varianza" entre dos variales (la varianza es una covarianza de una variable consigo misma), midiendo el **promedio de lo que se desv√≠a cada una respecto a su media**

$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

. . .

Para calcularla en `R` basta con usar la funci√≥n `cov()`

```{r}
starwars |> 
  drop_na(mass, height) |> 
  summarise(cov(mass, height))
```

---

## Correlaci√≥n lineal: sin agrupar

Vamos a practicar una vez m√°s como [**hacerlo a mano con el siguiente ejercicio**]{.hl-yellow}.

---

## Cor lineal: sin agrupar

En la tabla inferior se han recopilado (del 2013 al 2022) la **temperatura media en el mes de abril en Madrid (variable X, en ¬∫C)** y el **n√∫mero de d√≠as (variable Y) en el que el nivel de ozono super√≥ las 0.20 ppm (partes por mill√≥n)**

* ¬øCu√°l fue media de d√≠as en los que se super√≥ umbral de ozono de 0.20 ppm?
* ¬øCu√°l fue media de d√≠as en los que se super√≥ umbral de ozono en los a√±os que la temperatura media en marzo super√≥ los 17.4¬∫C?
* ¬øCu√°l es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)

---


## Correlaci√≥n lineal: sin agrupar

Repite el ejercicio con pocas l√≠neas de c√≥digo `R`

* ¬øCu√°l fue la media de d√≠as en los que se super√≥ el umbral de ozono de 0.20 ppm?
* ¬øCu√°l fue la media de d√≠as en los que se super√≥ el umbral de ozono en los a√±os que la temperatura media en marzo super√≥ los 17.4¬∫C?
* ¬øCu√°l es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)


---


## Correlaci√≥n lineal: sin agrupar

Realiza lo que consideres tanto a mano como en `R`

* ¬øExiste alguna **relaci√≥n de dependencia entre las variables**? ¬øDe qu√© tipo? ¬øC√≥mo de fuerte o d√©bil es dicha relaci√≥n? ¬øEn qu√© direcci√≥n es dicha relaci√≥n?


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

---

## Correlaci√≥n lineal: sin agrupar

No s√© si te has fijado qu√© sucede cuando intentamos [**calcular la covarianza/correlaci√≥n de varias variables**]{.hl-yellow}, por ejemplo vamos a calcular la (cuasi)covarianza de todas las variables num√©ricas de starwars.

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

. . .

Podemos usar la funci√≥n `cov()` sin m√°s, fuera de un resumen, obteniendo lo que se conoce como [**matriz de (cuasi)covarianzas**]{.hl-yellow} y que tendr√° un papel fundamental en estad√≠stica ya que contiene la informaci√≥n (= varianza) del dataset.

---

## Matriz de covarianzas


```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

Adem√°s de ser [**sim√©trica**]{.hl-yellow}...¬øqu√© tenemos en la [**diagonal**]{.hl-yellow}?

. . .

La [**matriz de (cuasi)covarianzas**]{.hl-yellow} se denota como $\Sigma$ y sus elementos se define como $\Sigma_{ii} = s_{x_i}^{2}$ para la diagonal y $\Sigma_{ij} = \Sigma_{ji} = s_{x_i x_j}$ fuera de ella.

. . .

::: callout-important
## Importante

Recuerda que los softwares estad√≠sticos nos devuelven siempre la [**cuasi covarianza**]{.hl-purple}, dividido entre $n-1$ y no entre $n$. La cuasivarianza y la cuasicovarianza son los [**mejores estimadores muestrales (insesgados)**]{.hl-yellow} de los respectivos par√°metros poblaciones

:::

---

## Matriz de correlaciones

De la misma manera con `cor()` podemos [**calcular la matriz de correlaciones**]{.hl-yellow} (en este caso sin el `cuasi` ya que se cancelan denominadores)

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cor()
```

. . .

La [**matriz de correlaciones**]{.hl-yellow} se denota como $R$ y sus elementos se define como $r_{ii} = 1$ para la diagonal y $r_{ij} = r_{x_ix_j}$ fuera de ella, y nos proporciona la dependencia lineal entre variables ya de manera **estandarizada**.

---

## Matriz de correlaciones

[**¬øSe te ocurre alguna manera de calcular la matriz de correlaciones a partir de la de covarianzas?**]{.hl-yellow}

---

## Correlaci√≥n lineal: datos agrupados

Imagina ahora que queremos calcular la covarianza/correlaci√≥n de los siguientes datos que representan 24 tiradas de dados

![](img/dados.png)

¬øC√≥mo calcular la covarianza/correlaci√≥n agrupando los datos?

---

## Correlaci√≥n lineal: datos agrupados

![](img/tabla-bi-freq.png)
---

## Correlaci√≥n lineal: datos agrupados

---

## Correlaci√≥n vs dependencia


Podemos tener [**variables incorreladas**]{.hl-red}, con correlaci√≥n nula, pero que [**exista dependencia entre ellas**]{.hl-green}: la covarianza/correlaci√≥n [**SOLO CAPTURA relaciones lineales**]{.hl-yellow}, nada m√°s.

. . .

Veamos un ejemplo sencillo con $X = \left\lbrace -1, 0, 1 \right\rbrace$ y $Y = X^2 =  \left\lbrace 1, 0, 1 \right\rbrace$. 

* La media de ambas es nula
* La media del producto es la media de $XY = \left\lbrace -1, 0, 1 \right\rbrace$, que es de nuevo nula
* As√≠ la covarianza $\overline{x*y} - \overline{x}*\overline{y}$ es nula a pesar de tener la mayor dependencia posible (dependencia funcional)

---

## Correlaci√≥n vs dependencia

![](img/covarianza-no-lineal.png)

En relaciones no lineales como la de la imagen, la **correlaci√≥n estar√° cercana a cero** (ya que no hay relaci√≥n lineal) pero existe una [**dependencia**]{.hl-yellow}. Diremos que [**dos variables son dependientes entre s√≠**]{.hl-yellow} cuando existe un **patr√≥n num√©rico que las relaciona**

. . .

* [**Independencia implica incorrelaci√≥n**]{.hl-green}
* [**Incorrelaci√≥n NO implica independencia**]{.hl-red}

---

## Correlaci√≥n vs dependencia

![](img/escenarios-covarianza.png)

---

## Dependencia vs causalidad

![](img/helados-moreno.jpg)

Si $X$ e $Y$ fuesen variables dependientes (lineales o no)...[**¬øimplicar√≠a que el ascenso/descenso de una provoca el ascenso/descenso de la otra?**]{.hl-yellow}

. . .

Imagina por ejemplo dos variables: nivel de bronceado (X) y consumo de helados (Y). ¬øSon dependientes? Aparentemente s√≠ ya que su comportamiento es similar. **¬øUna causa la otra?**

---

## Dependencia vs causalidad

![](img/helados-moreno.jpg)

Diremos que dos variables tienen una [**relaci√≥n causal o de causalidad**]{.hl-yellow} cuando una variable es la CAUSA del comportamiento de la otra, algo que no sabremos solo con estad√≠stica (sino con conocimiento experto, en este caso de nutricionistas y m√©dicos)

. . .

[**Correlaci√≥n NO IMPLICA causalidad**]{.hl-red} (al menos no tiene por qu√©)

---

## Dependencia vs causalidad


![](img/chocolate-correlacion.jpg)


Este fen√≥meno es conocido como [**correlaciones esp√∫reas**]{.hl-yellow}, y aparece cuando dos variables presentan una [**relaci√≥n matem√°tica**]{.hl-green} pero sin [**ning√∫n tipo de relaci√≥n causal o l√≥gica**]{.hl-red}. Puedes ver m√°s en <https://www.tylervigen.com/spurious-correlations>

---

## Dependencia vs causalidad

![](img/confounder.jpg)

Dicho patr√≥n matem√°tico puede deberse al mero azar o la existencia de lo que se conoce como [**variables de confusi√≥n**]{.hl-yellow} (en el caso del helado, el verano es el que causa ambas).

La rama que se dedica al [**estudio de la causalidad**]{.hl-yellow}, combinando las ramas de la **estad√≠stica, filosof√≠a, sociolog√≠a y psicolog√≠a** se le conoce como [**inferencia causal**]{.hl-yellow} y es fundamental en un an√°lisis m√°s profunde de las relaciones entre las variables (sobre todo en campos como la econom√≠a o la sociolog√≠a)


# Clase 4: primeras regresiones {#clase-4}

[**Historia regresi√≥n. Aprendizaje supervisado. Regresi√≥n lineal univariante**]{style="color:#444442;"}

---

## Historia de la reg

> Hay una regla universal: cualquier pariente tuyo es probablemente m√°s mediocre que t√∫¬ª

La [**historia de regresi√≥n**]{.hl-yellow} se remonta a **Francis Galton**, primo de Charles Darwin, que adem√°s de estad√≠stico fue psic√≥logo, ge√≥grafo y, por desgracia, el primer eugen√©sico (de hecho acu√±√≥ el termino)

. . .

Tambi√©n fue el primero en proponer m√©todos de clasificaci√≥n de huellas en medicina forense e incluso se le atribuye el primer mapa meteorol√≥gico de la historia

---

## Regresi√≥n y Darwin

Galton mostr√≥ fascinaci√≥n por ¬´El origen de la especies¬ª de su primo. Sin embargo, [**Galton no se centraba en los mejor adaptados sino en los que √©l llama mediocres**]{.hl-yellow}

. . .

Seg√∫n Galton, las sociedades estaban fomentando la mediocridad, interfiriendo en la selecci√≥n natural, as√≠ que empez√≥ a **estudiar si el talento era o no hereditario**.


. . .

¬øSu conclusi√≥n? El [**talento se disipaba**]{.hl-yellow} a lo largo de las generaciones

---

## Regresi√≥n a la mediocridad

En 1886 public√≥ ¬´Regression towards mediocrity in hereditary stature¬ª, un art√≠culo que cambiar√≠a la estad√≠stica: fue el [**primer uso documentado de lo que hoy conocemos como recta de regresi√≥n**]{.hl-yellow} 

. . .

:::: columns
::: {.column width="38%"}

![](img/reg_dalton.jpg)

:::

::: {.column width="62%"}

Galton analiz√≥ la estatura de 205 hijos y padres, observando que, de nuevo, los valores extremos se disipaban: a lo largo de las generaciones hab√≠a una [**regresi√≥n (un retroceso) a la mediocridad (entendida como la media)**]{.hl-yellow} 


:::
::::

Hijos de altos eran algo m√°s bajitos, e hijos de bajitos eran algo m√°s altos.

---

## Regresi√≥n a la mediocridad

:::: columns
::: {.column width="60%"}

![](img/reg-galton-coef.jpg)

:::

::: {.column width="40%"}

Galton no solo observ√≥ que las [**estaturas ¬´regresaban¬ª a un valor medio sino que lo hac√≠an con un patr√≥n**]{.hl-yellow}, con un factor constante de $2/3$: si los padres se desviaban $+3$ por encima de la media, los hijos se desviaban solo $(2/3)*3 = +2$ por encima de la media.

:::
::::

---

## Aprendizaje estad√≠stico

La regresi√≥n lineal es el modelo m√°s simple de lo que se conoce como [**aprendizaje estad√≠stico (Machine Learning)**]{.hl-yellow}, en concreto del conocido como [**aprendizaje supervisado**]{.hl-yellow}

![](img/ml.jpg)


---

## Ciencia de Datos


La [**ciencia de datos**]{.hl-yellow} es precisamente la rama que integra las matem√°ticas, la estad√≠stica, la probabilidad, el Machine Learning e incluso el Big Data

![](img/stats-ml.jpg)

---

## Aprendizaje ¬øsupervisado?

![](img/unsupervised-learning.jpg)

---

## Aprendizaje ¬øsupervisado?

En el campo del Machine Learning hay principalmente dos tipos de metodolog√≠as:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje supervisado**]{.hl-purple}: tendremos dos tipos de variables, la [**variable dependiente (output/target)**]{.hl-yellow} que se quiere predecir/clasificar, normalmente denotada como $Y$, y las [**variables independientes (inputs) o explicativas o predictoras**]{.hl-yellow}, que contienen la informaci√≥n disponible. Ejemplos: regresi√≥n, knn, √°rboles, etc.
 

:::

::: {.column width="50%"}

![](img/supervised.jpg)
:::
::::

---

## Aprendizaje ¬øsupervisado?

En el campo del Machine Learning hay principalmente dos tipos de metodolog√≠as:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje no supervisado**]{.hl-purple}: no existe la distinci√≥n entre target y variables explicativas ya que [**no tenemos etiquetados los datos**]{.hl-yellow}, no sabemos a priori la respuesta correcta. El aprendizaje no supervisado [**buscar√° patrones**]{.hl-yellow} basados en similitudes/diferencias. Ejemplos: PCA, clustering, redes neuronales, etc.

:::


::: {.column width="50%"}

![](img/unsupervised.jpg)

:::
::::

---

## Clasificaci√≥n vs predicci√≥n

Como hemos comentado, la [**regresi√≥n lineal**]{.hl-yellow} se enmarca dentro del [**predicci√≥n supervisada**]{.hl-yellow}

* [**Predicci√≥n**]{.hl-purple}: la [**variable objetivo es una variable cuantitativa continua**]{.hl-yellow} (por ejemplo, precio, glucosa, peso, etc).

* [**Clasificaci√≥n**]{.hl-purple}: la [**variable objetivo es una variable cualitativa**]{.hl-yellow} (por ejemplo, especie de flor, ausencia/presencia de enfermedad, si/no, etc) o **cuantitativa discreta** (por ejemplo, n√∫mero de accidentes). La etiqueta tomar√° un valor dentro del conjunto de modalidades permitidas, pudiendo ser **binaria** (si/no) o **multiclase** (A, B, C, D).

&nbsp;

üìö Ver ¬´The elements of Statistical Learning¬ª (Hastie et al., 2008)

# Clase 5: ajuste de regresi√≥n {#clase-5}

[**Interpretaci√≥n de coeficientes. M√©todo m√≠nimos cuadrados**]{style="color:#444442;"}

---

## Modelo predictivo

Dentro del marco de la [**predicci√≥n supervisada**]{.hl-yellow} un modelo tendr√° siempre la siguiente forma:

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$

* $X$ ser√°n los [**datos**]{.hl-yellow}

* $f(\cdot)$ ser√° nuestro [**modelo**]{.hl-yellow}, es decir, el [**valor esperado de $Y$**]{.hl-yellow} con la informaci√≥n que tenemos.

* $\mathbf{X} = \left(X_1, \ldots, X_p \right)$ ser√°n nuestras [**predictoras o variables independientes**]{.hl-yellow}

* $\varepsilon$ ser√° el [**error o ruido**]{.hl-yellow}, una [**variable aleatoria de media 0**]{.hl-yellow} $E \left[\varepsilon | \boldsymbol{X} = x \right] = 0$ (el error deber√≠a ser reducido a **algo aleatorio (irreducible)**, aunque en estad√≠stica SIEMPRE nos vamos a equivocar).

---

## Modelo predictivo

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$


El objetivo es intentar [**estimar dicha variable objetivo**]{.hl-yellow} minimizando al m√°ximo el error cometido (la parte que podemos evitar).

. . .

El [**modelo estimado**]{.hl-yellow} se definir√° como

$$\widehat{Y} := \widehat{E \left[Y | \boldsymbol{X} = x \right]}  = \widehat{f}\left(X_1, \ldots, X_p \right), \quad \widehat{\varepsilon} = \widehat{Y} - Y$$

* $\widehat{Y}$ ser√°n las [**estimaciones**]{.hl-yellow}, definidas como la estimaci√≥n del [**valor esperado de $Y$**]{.hl-yellow} con la informaci√≥n que tenemos.

* $\widehat{f}$ ser√° el [**modelo estimado**]{.hl-yellow}

* $\widehat{\varepsilon}$ los [**errores estimados**]{.hl-yellow} (con media muestral igual a cero).


---

## Reg. lineal univariante

En el caso concreto de la [**regresi√≥n lineal**]{.hl-yellow} nuestro modelo ser√° un **hiperplano lineal** (en el caso de una variable, una simple recta):

$$E \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p$$

. . .

Su estimaci√≥n ser√° por tanto

$$\widehat{Y} := \widehat{E \left[Y | \boldsymbol{X} = x \right]} = \widehat{f}(\mathbf{X}) = \widehat{\beta_0} + \widehat{\beta_1} X_1 + \ldots + \widehat{\beta_p} X_p $$

* El modelo $\widehat{f}(\cdot)$ ser√° una [**recta/hiperplano lineal**]{.hl-yellow}

* En el caso de la regresi√≥n lineal [**univariante**]{.hl-yellow} tendremos por tanto $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1X$ ($p = 1$)

* El objetivo ser√° obtener la estimaci√≥n de los $\widehat{\beta}$ tal que **minimicemos el error**

---

## Varianza residual

Como ya hemos comentado, una manera de cuantificar lo que nuestro modelo se equivoca es lo que llamamos [**varianza residual o error cuadr√°tico medio**]{.hl-yellow} (varianza del error), que podemos estimar como sigue (ya que la [**media muestral de los errores ser√° cero**]{.hl-purple})

$$s_{r}^{2} = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \left( \widehat{\varepsilon}_{i} - \overline{\widehat{\varepsilon}}_{i}\right)^2 = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} $$
¬øC√≥mo quedar√≠a la f√≥rmula si **desarrollo lo que vale los errores estimados** $\widehat{\varepsilon}$?

. . .


$$s_{r}^{2} := s_{r}^{2}\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)= \frac{1}{n} \sum_{i=1}^{n} \left(Y_i - \widehat{Y}_i \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\widehat{\beta}_0 + \widehat{\beta}_1X_i \right)\right]^2$$

---

## M√©todo m√≠nimos cuadrados

El [**m√©todo de los m√≠nimos cuadrados**]{.hl-yellow}, originariamente planteado a la vez por Legendre y Gauss, consiste en establecer que los [**par√°metros √≥ptimos**]{.hl-yellow} ser√°n aquellos $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$ que minimicen dicha suma de cuadrados (que minimicen la varianza residual)

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} s_{r}^{2}\left(\beta_0, \beta_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} n*s_{r}^{2}\left(\beta_0, \beta_1 \right)$$

Vamos a desarrollar ese cuadrado...

--- 


## M√©todo m√≠nimos cuadrados

$$s_{r}^{2}\left(\beta_0, \beta_1 \right) = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\beta_0 + \beta_1 X_i \right)\right]^2$$ 
Vamos a desarrollar ese cuadrado...

. . .

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$




---

## M√©todo m√≠nimos cuadrados

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$

¬øC√≥mo encontrar el m√≠nimo de una funci√≥n?

. . .

Efectivamente, derivando.

$$\frac{\partial s_{r}^{2}}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{0} + 2 \beta_1 X_i - 2Y_i \right] = 2 \left( \beta_{0} + \beta_{1} \overline{x} - \overline{y} \right)$$ 

$$\frac{\partial s_{r}^{2}}{\partial \beta_1} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{1}X_{i}^{2} + 2 \beta_0 X_i - 2Y_iX_i \right] = 2 \left( \beta_{1} \overline{x^2}+\beta_{0}\overline{x} - \overline{xy} \right)$$ 

---

## M√©todo m√≠nimos cuadrados


Si lo agrupamos en un sistema y lo [**igualamos a cero**]{.hl-yellow} para encontrar el √≥ptimo tenemos

$$\left\{\beta_{0} + \beta_{1} \overline{x}   = \overline{y} \atop \overline{x} \beta_{0} + \overline{x^2} \beta_{1} = \overline{xy} \right.$$ 

¬øC√≥mo resolver un sistema?

. . .

Haciendo la regla de Cramer tenemos que

$$\widehat{\beta}_1 = \frac{\overline{xy} - \overline{x}*\overline{y}}{\overline{x^2} - \overline{x}^2} = \frac{s_{xy}}{s_{x}^{2}}$$

$$\widehat{\beta}_0 = \frac{\overline{y}*\overline{x^2} - \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2}  = \overline{y} + \frac{\overline{y}*\overline{x}^2- \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2} = \overline{y} - \widehat{\beta}_1 \overline{x}$$


---

## Estimaci√≥n reg. univariante


Los [**estimadores de los par√°metros de una regresi√≥n lineal univariante**]{.hl-yellow} ser√°n por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

. . .

Es importante [**distinguir lo poblaci√≥n de lo muestral**]{.hl-yellow}

* Los par√°metros $\left( \beta_0, \beta_1 \right)$ son desconocidos, los [**par√°metros poblacionales**]{.hl-yellow}

* Los par√°metros $\left( \widehat{\beta}_0, \widehat{\beta}_1 \right)$ son estimados a partir de los datos, son [**variables aleatorias**]{.hl-yellow} ya que han sido calculados en funci√≥n de una muestra aleatoria $\left\lbrace \left(x_i, y_i \right) \right\rbrace_{i=1}^{n}$ de la poblaci√≥n $\left(X, Y \right) $

---

## Estimaci√≥n reg. univariante

Los [**estimadores de los par√°metros de una regresi√≥n lineal univariante**]{.hl-yellow} ser√°n por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

¬øCu√°l es su [**interpretaci√≥n**]{.hl-yellow}?

. . .

* [**Ordenada en el origen**]{.hl-yellow}: tambi√©n llamado **intercepto**, y denotado como $\beta_0$ su valor real, es el valor de $Y$ cuando $X=0$. Es decir, $\widehat{\beta}_0$ se puede interpretar como la estimaci√≥n $\widehat{Y}$ cuando $X = 0$ (cuando dicha estimaci√≥n tenga sentido).


. . .

* [**Pendiente**]{.hl-yellow}: denotado com $\beta_1$ su valor real, cuantifica el incremento de $Y$ cuando $X$ aumenta una unidad. Es decir, $\widehat{\beta}_1$ se puede interpretar como la variaci√≥n de la **estimaci√≥n $\widehat{Y}$ cuando $X$ tiene un incremento unitario**.

---

## Regresi√≥n en R

Para hacer un [**ajuste de regresi√≥n lineal univarainte en R**]{.hl-yellow} solo necesitamos utilizar la funci√≥n `lm()` (de linear model) con dos argumentos:

* `data = ...`: los datos de los que queremos sacar el ajuste (sin ausentes de momento)
* `formula = ...`: la f√≥rmula del ajuste (y frente a x, expresado como `y ~ x`)

```{r}
datos <- 
  starwars |>
  drop_na(mass, height) 
ajuste_lineal <- lm(data = datos, formula = mass ~ height)
```

---

## Regresi√≥n en R

Una vez realizado el ajuste podemos [**resumirlo**]{.hl-yellow} con `summary()`

```{r}
ajuste_lineal |> summary()
```

. . .

¬øQu√© representa cada bloque de la salida?

---

## Regresi√≥n en R

```{r}
ajuste_lineal |> summary()
```

* `Call: ...`: la orden que hemos ejecutado

* `Residuals: ...`: un resumen en forma de [**cuartiles de los residuales**]{.hl-yellow}, es decir, de los [**errores**]{.hl-yellow} (f√≠jate que aunque la media de los errores siempre es 0, no tiene porque serlo la mediana, de hecho que no lo sea ya nos indica que normales no van a ser seguramente).


---

## Regresi√≥n en R

```{r}
ajuste_lineal |> summary()
```


* `Coefficients: ...`: de momento solo nos interesa la columna [**Estimate**]{.hl-yellow} que nos da las estimaciones de los par√°metros. En la fila `Intercept` siempre ir√° $\widehat{\beta}_0$, y el resto de filas tendr√° el nombre de la variable predictora a la que multiplica el par√°metro (en este caso la fila `height` corresponde a la estimaci√≥n $\widehat{\beta}_1$).

---


## Regresi√≥n en R

```{r}
ajuste_lineal |> summary()
```

En nuestro caso particular, el [**ajuste de regresi√≥n**]{.hl-yellow} viene dado por $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1 X = -11.4868 + 0.6240*X$, donde $X$ es la estatura: por cada cm extra que mida un personaje, el modelo estima que el peso aumenta 0.6240 kg, y la estimaci√≥n de peso para un persona que mida 0 cm es de -11.4868 kg (obviamente esta estimaci√≥n mucho sentido no tiene)

---

## Regresi√≥n en R

```{r}
ajuste_lineal |> summary()
```

* `Multiple R-squared: ...` (de momento lo dem√°s lo ignoramos): es el conocido como $R^2$ o bondad de ajuste. Hablaremos de √©l en las pr√≥ximas clases, pero de momento, nos basta saber que es una [**m√©trica de calidad del ajuste**]{.hl-yellow} que va de 0 (peor) a 1 (mejor).

---

## Predicci√≥n en R

Una vez estimado el modelo (sus coeficientes) podemos usarlo, bien para  [**estimar valores que el modelo conoce**]{.hl-yellow} (de los que aprendi√≥) o para [**predecir nuevos valores**]{.hl-yellow} de los que el modelo no ha aprendido

. . .

Simplemente necesitamos la funci√≥n `predict(modelo, nuevos_datos)`, usando como argumentos el **modelo entrenado** y un [**nuevo dataset que contenga una/s columna/s llamadas igual que nuestras variables predictores**]{.hl-yellow} con los valores de la predictora para los que queremos predecir

```{r}
predicciones <- 
  tibble("height" = c(15, 81.3, 153.1, 189.3, 210.3),
         "mass_predict" = predict(ajuste_lineal, tibble("height" = height)))
predicciones
```

---

## Rango de fiabilidad

Aunque hablaremos en profundidad de la bondad de ajuste, supongamos que tenemos un **modelo extremadamente bueno**, con una bondad de ajuste $R^2 = 0.99$. ¬øSer√≠a fiable nuestro modelo anterior para predecir el peso de un personaje con $X = 15$ cm?

. . .

```{r}
predicciones
```

Si te fijas la [**¬°predicci√≥n del peso es negativa!**]{.hl-red}: por muy beb√© que sea, algo pesar√°. ¬øPor qu√© sucede esto?

---

## Rango de fiabilidad


![](img/rango_1.png)
![](img/rango_2.png)

Nuestro [**modelo solo puede aprender de los datos que conoce**]{.hl-yellow}: solo podremos predecir para valores nuevos de las predictoras [**dentro del rango de las predictoras con las que entrenamos**]{.hl-yellow} el modelo

Da igual lo bueno que sea el modelo: las [**predicciones fuera del rango de las predictoras de entrenamiento**]{.hl-red} no ser√°n fiables ya que el modelo no sabe lo que sucede fuera.

# Clase 6: diagnosis  {#clase-6}

[**Hip√≥tesis e inferencia del modelo**]{style="color:#444442;"}

---

## Repaso


* [**Modelo lineal**]{.hl-yellow}: descrito como $Y = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p X_p + \varepsilon$, buscamos estimar el **valor esperado**  $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p x_p$, donde $\boldsymbol{X}$ es la poblaci√≥n y $x$ la muestra.

. . .

* [**Caso univariante**]{.hl-yellow}: $p = 1$ tal que $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 x$.

. . .

* [**Estimaci√≥n**]{.hl-yellow}: $\widehat{Y}$ en funci√≥n de $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$. De todas las rectas posibles, queremos la [**recta de regresi√≥n m√≠nimos-cuadrados**]{.hl-yellow}: **minimiza la varianza residual** (calculamos sus derivadas parciales, igualamos a cero y resolvemos el sistema). Esos estimadores ser√°n $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$  y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$.

. . .

* [**Predicci√≥n**]{.hl-yellow}: podemos estimar o predecir valores con la recta $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_0 X$, siempre y cuando $X = x$ est√© dentro del rango de valores de entrenamiento del modelo.

---

## Diagnosis

Hasta ahora no hemos pedido nada a la muestra $\left\lbrace \left( X_i, Y_i \right) \right\rbrace_{i=1}^{n}$, ¬øpara qu√© necesitar√≠amos [**hip√≥tesis**]{.hl-yellow} entonces?

. . .

La raz√≥n es que, hasta ahora, lo √∫nico que hemos podido realizar es una [**estimaci√≥n puntual**]{.hl-yellow} de los par√°metros, pero dado que dichos estimadores ser√°n variables aleatorias, necesitaremos realizar [**inferencia estad√≠stica**]{.hl-yellow} sobre ellos (recuerda: los par√°metros son simpleme estimaciones para esa muestra de la poblaci√≥n, de forma que dada otra muestra, la recta ser√° distinta).

. . .

Para poder cuantificar la [**variabilidad y precisi√≥n de nuestras estimaciones**]{.hl-yellow} necesitaremos que los datos cumplan ciertas [**hip√≥tesis probabil√≠sticas**]{.hl-purple}: lo interesante no es la estimaci√≥n puntual de los par√°metros a partir de la muestra sino lo que [**podamos inferir de ellos a la poblaci√≥n**]{.hl-green}

---

## Diagnosis

En el caso de la regresi√≥n lineal univariante pediremos [**4 hip√≥tesis**]{.hl-yellow}

. . .

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1 x$

. . .

2. [**Homocedasticidad**]{.hl-green}: el modelo no podr√° explicar toda la informaci√≥n (a veces se equivocar√° por arriba y otras por abajo), pero necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow}, tal que $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \boldsymbol{X} = x \right] = cte < \infty$, que no var√≠e seg√∫n aumenta o disminuye la $X$.

. . .

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$

. . .

4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser independientes entre s√≠ (el error en una observaci√≥n no depende de otras). En particular, ser√°n **incorrelados**

$${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] - E \left[\varepsilon_i \right] E \left[\varepsilon_j \right] = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$$

---


## Diagnosis


Las 4 hip√≥tesis se pueden [**resumir de manera te√≥rica**]{.hl-yellow} en

$$Y | \boldsymbol{X} = x \sim N \left(\beta_0 + \beta_1x, \sigma_{\varepsilon}^2 \right)$$

. . .

Su [**versi√≥n muestral**]{.hl-purple} ser√≠a simplemente

$$y_i | \boldsymbol{X} = x_i \sim N \left(\beta_0 + \beta_1x_i, \sigma_{\varepsilon}^2 \right), \quad i=1,\ldots,n$$

---

## Inferencia de los par√°metros

Las hip√≥tesis nos permiten decir (lo demostraremos m√°s adelante) que los [**par√°metros estimados siguen una distribuci√≥n (condicionada) normal**]{.hl-yellow} de [**media el par√°metro**]{.hl-purple}  a estimar y de [**varianza el conocido como standard error**]{.hl-purple}

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

. . .

Por las propiedades de la normal y el teorema central del l√≠mite 

$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)}| \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$

---

## Inferencia de los par√°metros


$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

Veremos m√°s adelante porqu√© pero de momento...estos son los $SE$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

¬øQu√© **propiedades** tienen estos estimadores?


---

## Inferencia de los par√°metros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

* [**Insesgados**]{.hl-yellow}: ambos son [**estimadores insesgados**]{.hl-green} ya que la esperanza de la estimaci√≥n es el valor a estimar. $E \left[ \widehat{\beta}_j \right] = \beta_j$

. . .
 
* [**Precisi√≥n vs tama√±o muestral**]{.hl-yellow}: cuando $n \to \infty$, sus varianzas tienden a cero, es decir, $SE \left(\widehat{\beta}_j\right)^2 \to^{n \to \infty} 0$ ya que $n$ est√° dividiendo. Traducci√≥n: [**a m√°s datos, mayor precision**]{.hl-green} (menos varianza tendr√°n los estimadores si repetimos la toma de muestras)


---

## Inferencia de los par√°metros


$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$

. . .

* [**Precisi√≥n vs var residual**]{.hl-yellow}: cuanto m√°s grande sea la varianza del error $\sigma_{\varepsilon}^{2}$, $SE \left(\widehat{\beta}_j\right)^2$ crecer√° (es decir, [**m√°s ruido implicar√° m√°s imprecisi√≥n**]{.hl-red})

. . .

* [**Precisi√≥n vs varianza de X**]{.hl-yellow}: cuanto m√°s grande sea la varianza de la predictora $s_{x}^2$, $SE \left(\widehat{\beta}_j\right)^2$ decrecer√°, tal que $SE \left(\widehat{\beta}_j\right)^2 \to^{s_{x}^2 \to \infty} 0$. Dicho de otra forma: cu√°nta [**m√°s informaci√≥n (varianza) contenga nuestra tabla, mayor precisi√≥n**]{.hl-green}.

. . .

* [**Precisi√≥n vs media X**]{.hl-yellow}: solo afecta a la estimaci√≥n de $\beta_0$, cuya [**precisi√≥n decrece cuando la media aumenta**]{.hl-red}. Esto tiene sentido ya que cu√°nto m√°s grande en media sean los datos, menos fiable ser√° la predicci√≥n para $X=0$.

---

## Inferencia de los par√°metros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


Pero tenemos un problema: la [**varianza residual poblacional**]{.hl-yellow} (varianza poblaci√≥n del error) $\sigma_{\varepsilon}^{2}$ es [**desconocida**]{.hl-yellow}. Como suele pasar, la [**sustituiremos por un estimador (insesgado)**]{.hl-yellow} de la misma (ya demostaremos...), donde $p$ es el n√∫mero de variables (lo que R llama `std error` en la salida)

. . .

$$\widehat{\sigma_{\varepsilon}^{2}} = \frac{n}{n-p-1} s_{r}^{2} = \frac{1}{n-p-1} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 =_{p=1} \frac{1}{n-2} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2$$
---

## Inferencia de los par√°metros



$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$


¬øQu√© suele suceder cuando en una normal **desconocemos la varianza real y la sustituimos por su estimador insesgado**?

. . .

Si [**sustituimos el error standard por su estimador**]{.hl-yellow} (sustituyendo la varianza residual por su estimador insesgado), obtenemos que sigue una **t-Student**

$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$
donde $\widehat{SE} \left(\widehat{\beta}_0\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right]$ y $\widehat{SE} \left(\widehat{\beta}_1\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n s_{x}^2}$

---

## Inferencia de los par√°metros


```{r}
ajuste_lineal |> summary()
```

* `std error`: es lo que hemos denotado como $\widehat{SE} \left(\widehat{\beta}_j\right)^2$

. . .

* `t value`: es el estad√≠stico $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ que hemos dicho que sigue una t-Student (f√≠jate que hemos puesto $\beta_j = 0$)

## Inferencia de los par√°metros


```{r}
ajuste_lineal |> summary()
```

* `Pr(>|t|)`: representa un p-valor. ¬øDe qu√© contraste se te ocurre?

. . .

* `Residual standard error`: es el estimador insesgado de la varianza residual, que hemos denotado como $\widehat{\sigma_{\varepsilon}^{2}}$

---

## Inferencia de los par√°metros

:::: columns
::: {.column width="55%"}

$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$

:::

::: {.column width="45%"}

![](img/tablas-intervalos.png)

:::
::::

Si te acuerdas de inferencia, esto implica que los [**intervalos de confianza de nuestros par√°metros**]{.hl-yellow} son

$$IC \left(\alpha, \widehat{\beta}_j \right) = \left[\widehat{\beta}_j - t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right), \widehat{\beta}_j + t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right) \right]$$
donde $t_{n-p-1; \alpha/2}$ es el cuantil $1-\alpha/2$ (deja a la izquierda una probabilidad de $1-\alpha/2$).

---


## Inferencia de los par√°metros

Podemos calcular los intervalos de confianza de los par√°metros en `R` con `confint()`

```{r}
confint(ajuste_lineal, level = 0.99)
confint(ajuste_lineal, level = 0.95)
confint(ajuste_lineal, level = 0.9)
```

---

## Inferencia de los par√°metros

[**¬øQu√© significa realmente un intervalo de confianza?**]{.hl-yellow}

. . .

La **probabilidad de que el par√°metro real** caiga dentro [**NO EXISTE**]{.hl-red}: el par√°metro es desconocido pero fijo, no aleatorio, as√≠ que no sentido calcular su probabilidad.

. . .

Si obtenemos para un par√°metro un intervalo $[-1, 1]$ al 95%, no significa que $P(parametro \in [-1, 1]) = 0.95$: significa que [**si tomamos 1000 muestras distintas de la poblaci√≥n**]{.hl-yellow} y calculamos para cada una el intervalo de confianza, [**aproximadamente 950 intervalos de confianza contendr√°n dentro el par√°metro real**]{.hl-yellow}

. . .

Un [**intervalo al 95% implica que habr√° una frecuencia esperada de 0.95**]{.hl-yellow} de que intervalos que no conocemos (porque se derivan de muestras que no hemos tomado) contengan al par√°metro real, pero [**no es la probabilidad de que tu intervalo calculado contenga a dicho par√°metro**]{.hl-red}: nos habla la precisi√≥n de nuestra metodolog√≠a de estimaci√≥n, no del par√°metro.


---

## Inferencia de los par√°metros

[**Deberes**]{.hl-yellow}. Dada una poblaci√≥n normal $\mu = 3$ y $\sigma = 1.2$, crea un c√≥digo que genere 500 muestras distintas (tama√±o $n = 100$ cada una), de manera que para cada una apliques un `t.test()` para calcular un IC para $\mu$. Tras ello, gr√°fica los intervalos como se muestra en la imagen (haz uso de `geom_segment()`)

```{r}
#| echo: false
mu <- 3
sigma <- 1.2
m <- 500
conf_int <- tibble("id" = 1:m, "low" = NA, "high" = NA)

for (i in 1:m) {
  
  sample <- rnorm(n = 100, mean = mu, sd = sigma)
  hyp_test <- t.test(x = sample)
  conf_int[i, 2] <- hyp_test$conf.int[1]
  conf_int[i, 3] <- hyp_test$conf.int[2]
}
conf_int <-
  conf_int |> 
  mutate(true_param = mu >= low & mu <= high)

ggplot(conf_int) +
  geom_segment(aes(y = id, yend = id, x = low, xend = high,
                   color = true_param, linewidth = true_param)) +
  geom_vline(xintercept = mu) +
  scale_linewidth_manual(values = c(1.1, 0.2)) +
  theme_minimal() +
  guides(linewidth = "none") +
  labs(x = "Valores del intervalo",
       y = "id intervalo",
       color = "¬øContiene mu real?")
```

---


## Inferencia de los par√°metros

Y si tenemos inferencia, tenemos contrastes: ¬øte acuerdas de los p-valores que devuelve la tabla para cada par√°metro?

. . .

Para cada par√°metro se realiza un [**contraste de significancia**]{.hl-yellow}: ¬øcu√°nta evidencia hay en mis datos para poder decir que el [**valor estimado de mi par√°metro es distinto de 0**]{.hl-yellow}?

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

. . .

El estad√≠stico usado es el [**t value**]{.hl-yellow} que devuelve `R` definido como $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ y que, [**BAJO EL SUPUESTO DE QUE LA HIP√ìTESIS NULA SEA CIERTA**]{.hl-purple}, sigue una t-Student $t_{n-p-1}$

---

## Inferencia de los par√°metros

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

Este contraste significancia en realidad **contesta a la siguiente pregunta**

¬øTiene mi variable $X_j$ un [**efecto lineal SIGNIFICATIVO**]{.hl-yellow} en mi variable $Y$?

. . .

&nbsp;

Esa pregunta puede ser respondida desde un punto de vista **frecuentista** haciendo uso de los [**p-valores**]{.hl-yellow}: si $p-value < \alpha$, se suele rechazar la nula (rechazamos que la variable no tenga efecto lineal significativo, es decir, aceptamos que s√≠ lo tiene); en caso contrario no se suele rechazar (que **no es lo mismo que aceptarla**). Pero...

---

## Par√©ntesis: p-valor

[**¬øQu√© representa un p-valor?**]{.hl-yellow}

. . .

Hablaremos m√°s adelante de ellos pero este es un [**peque√±o resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>

. . .

* [**Habla sobre los datos**]{.hl-yellow}. Los p-valores nos pueden servir como indicadores de **c√≥mo de incompatible son los datos respecto a un modelo/hipotesis/explicaci√≥n asumida**: habla sobre los datos, no sobre la veracidad de la hipotesis nula per se o la probabilidad de que los datos hayan salido tan extremos por azar. 

---

## Par√©ntesis: p-valor

[**¬øQu√© representa un p-valor?**]{.hl-yellow}

Hablaremos m√°s adelante de ellos pero este es un [**peque√±o resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>



* [**Realidades complejas, decisiones complejas**]{.hl-yellow}. Cuidado con reducir a decisiones binarias realidades complejas. La regla del p-valor es una herramienta m√°s, pero no debe ser la √∫nica en la que nos basemos para decidir. [**Otros aspectos a considerar**]{.hl-yellow}: calidad de las medidas, dise√±o del estudio, evidencia externa en la literatura respecto a la causalidad subyacente, validez de las hip√≥tesis planteadas, etc

---

## Par√©ntesis: p-valor

[**¬øQu√© representa un p-valor?**]{.hl-yellow}

Hablaremos m√°s adelante de ellos pero este es un [**peque√±o resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>


* [**No hagas cherry picking**]{.hl-red}: muestra de manera transparentes que has probado, que ha salido y que no, y no te quedes solo con lo que sale bien o los p-valores que te convenga (**p-hacking**).

---

## Par√©ntesis: p-valor

[**¬øQu√© representa un p-valor?**]{.hl-yellow}

Hablaremos m√°s adelante de ellos pero este es un [**peque√±o resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>


* [**Significancia estad√≠stica**]{.hl-yellow} no implica significacia real (cient√≠fica, humana, econ√≥mica, etc). Obtener [**p-valores muy peque√±os no implica un mayor efecto**]{.hl-red} que otros p-valores no tan peque√±os (y viceversa). [**Cualquier efecto, por peque√±o que sea, puede derivar en p-valores peque√±os si el tama√±o uestral o la precisi√≥n de las medidas es suficiente alto**]{.hl-yellow}, y al contrario (efectos evidentes pueden derivar en p-valores altos si $n$ es peque√±o)

---

## Par√©ntesis: p-valor

[**¬øQu√© representa un p-valor?**]{.hl-yellow}

Hablaremos m√°s adelante de ellos pero este es un [**peque√±o resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>


* [**Alternativas**]{.hl-purple}: intervalos de confianza, de credibilidad, m√©todos bayesianos, false discovery rates.

&nbsp;

üìö Ver m√°s en <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>, <https://anabelforte.com/2020/11/15/contraste/> y <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877414/>

---

## Inferencia de los par√°metros



```{r}
ajuste_lineal |> summary()
```

Si te fijas ambos p-valores est√°n por encima de $\alpha = 0.05$ (umbral adoptado habitualmente) los que nos dice que [**no hay evidencias de los datos sean compatibles con afirmar que haya efectos lineales significativos**]{.hl-yellow}

. . .

¬øY si probamos a quitar $\beta_0$ (es decir, la respuesta est√° centrada)?

---

## Inferencia de los par√°metros

Para ello basta a√±adir un `-1` antes de las variables predictoras.

```{r}
lm(data = datos, formula = mass ~ -1 + height) |> summary()
```

F√≠jate que ahora, am√©n que [**hay un efecto lineal significativo de la altura**]{.hl-yellow} y de tener $R^2$ mayor, $\beta_1$ tiene una precisi√≥n mayor. En este caso [**solo pod√≠a quitar uno**]{.hl-yellow} (perder√≠amos $X$), pero veremos m√°s adelante c√≥mo decidir cu√°l quitar si tuvi√©semos varias variables.

# Clases 7 y 8: caso pr√°ctico  {#clase-7-8}

[**¬øDe qu√© depende el precio del vino?**]{style="color:#444442;"}

---

## Caso pr√°ctico

Vamos a poner en pr√°ctica lo aprendido con el dataset `wine.csv` disponible en el campus.

```{r}
datos <- read_csv(file = "./datos/wine.csv")
datos
```

---

## Caso pr√°ctico


El conjunto de datos est√° formado por 27 observaciones (cosechas de vino rojo Burdeos) y 7 variables

* `Year`, `Age`: a√±o de la cosecha y n√∫mero de a√±os en barrica.
* `Price`: precio en 1990-1991 de cada cosecha (en **escala log**)
* `WinterRain`: lluvia (en mm) que cay√≥ ese a√±o en invierno.
* `AGST`: crecimiento medio de la temperatura (en grados Celsius) durante la temporada.
* `HarvestRain`: lluvia (en mm) que cay√≥ ese a√±o durante la cosecha.
* `FrancePop`: poblaci√≥n (miles de habitantes) de Francia.

Ver m√°s en detalles en <https://doi.org/10.1080/09332480.1995.10542468>

. . .

[**El objetivo: predecir el precio**]{.hl-yellow}


---

## Caso pr√°ctico

```{r}
datos
```

Para predecir el precio vamos a usar (de momento) una [**regresi√≥n lineal univariante**]{.hl-yellow}, donde $Y = precio$ y deberemos elegir la predictora $X$ m√°s apropiada.

---

## Pasos a seguir

1. [**An√°lisis exploratorio inicial**]{.hl-yellow}:
  - ¬øLas variables son [**num√©ricas (continuas)**]{.hl-purple}?
  - ¬øTienen [**problemas de rango**]{.hl-purple} (por ejemplo, pesos negativos)? ¬øTienen datos ausentes?
  - ¬øC√≥mo se [**distribuyen las variables**]{.hl-purple}? Ideas: res√∫men num√©rico, histogramas/densidades, boxplots, gr√°ficos de viol√≠n, etc
  - ¬øHay [**datos at√≠picos**]{.hl-purple}?

---

## Pasos a seguir

2. [**An√°lisis de dependencia**]{.hl-yellow}:
  - ¬øQu√© [**predictora est√° m√°s correlacionada (linealmente) con la variable objetivo**]{.hl-purple} a predecir? ¬øExiste otro tipo de dependencia (pendiente implementar en `R`)?
  - ¬øC√≥mo se [**relacionan las predictoras**]{.hl-purple} entre s√≠? ¬øEst√°n correlacionadas? Ideas: matriz de correlaciones, diagramas de dispersi√≥n vs Y, corrplots, etc

. . .

3. [**Formulaci√≥n**]{.hl-yellow} del modelo

. . .

4. [**Fase de estimaci√≥n**]{.hl-yellow}:
  - ¬øCu√°nto valen los [**par√°metros estimados**]{.hl-purple}? ¬øC√≥mo queda el ajuste?
  - ¬øQu√© [**interpretaci√≥n**]{.hl-purple} tienen?


---

## Pasos a seguir

5. [**Fase de diagnosis**]{.hl-yellow} (paquetes `{performance}` y `{olsrr}`):
  - ¬øCumplen los datos las [**hip√≥tesis par√°metricas**]{.hl-purple} requeridas para poder hacer inferencia? 
  - ¬øC√≥mo modificar los datos para que se cumplan?
  - An√°lisis de residuales
  
. . .

6. [**Fase de inferencia**]{.hl-yellow}:
  - ¬øQu√© [**variabilidad**]{.hl-purple} tienen las estimaciones de nuestro par√°metros?
  - ¬øLas predictoras/intercepto tienen un [**efecto lineal significativo**]{.hl-purple}?
  - ¬øDebemos [**re-entrenar el modelo**]{.hl-purple} sin alguno de ellos?

---

## Pasos a seguir

7. [**Fase de evaluaci√≥n**]{.hl-yellow}:
  - ¬øEs significativo el modelo? [**ANOVA: an√°lisis de la varianza**]{.hl-purple}
  - ¬øQu√© informaci√≥n de la predictora explica el modelo? [**Par√°metros de bondad de ajuste**]{.hl-purple} ($R^2$ por ejemplo)

. . .

8. [**Fase de predicci√≥n**]{.hl-yellow}


---

## An√°lisis exploratorio inicial


1. [**An√°lisis exploratorio inicial**]{.hl-yellow}:
  - ¬øLas variables son [**num√©ricas (continuas)**]{.hl-purple}?
  - ¬øTienen [**problemas de rango**]{.hl-purple} (por ejemplo, pesos negativos)? ¬øTienen datos ausentes?
  
&nbsp;

Para responder a dichas preguntas lo m√°s sencillos es hacer uso de la funci√≥n `skim()` del paquete `{skimr}`

```{r}
#| eval: false
library(skimr)
datos |> skim()
```

. . .

En este caso [**no tenemos ausentes ni problemas de codificaci√≥n**]{.hl-green}


---

## An√°lisis exploratorio inicial

  - ¬øC√≥mo se [**distribuyen las variables**]{.hl-purple}?
  - ¬øHay [**datos at√≠picos**]{.hl-purple}?


&nbsp;

Para ello podemos [**visualizar la distribuci√≥n de cada variable**]{.hl-yellow} haciendo uso de densidades, histogramas y/o boxplots (por ejemplo), pero si queremos ahorrar tiempo en visualizar convertimos nuestro dataset en [**tidydata**]{.hl-yellow}

```{r}
datos_tidy <-
  datos |> 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "values")
datos_tidy
```

---

## An√°lisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(x = values)) +
  geom_density(aes(color = variable, fill = variable),
               alpha = 0.75) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

---

## An√°lisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(x = values)) +
  geom_histogram(aes(color = variable, fill = variable),
               alpha = 0.75, bins = 10) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

---

## An√°lisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(x = values)) +
  geom_histogram(aes(color = variable, fill = variable),
               alpha = 0.75, bins = 10) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

---

## An√°lisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(y = values, x = variable)) +
  geom_boxplot(aes(color = variable, fill = variable),
               alpha = 0.75) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

[**No hay valores at√≠picos (respecto a los percentiles)**]{.hl-green}

---

## An√°lisis exploratorio inicial

Podemos incluso gr√°ficar todos los scatter plot sin transformar los datos con `facet_matrix()` del paquete `{ggforce}`

```{r}
#| code-fold: true
library(ggforce)
ggplot(datos) +
  geom_point(aes(x = .panel_x, y = .panel_y)) +
  facet_matrix(vars(everything())) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  theme_minimal()
  
```

---

## An√°lisis exploratorio inicial

Podemos tambi√©n visualizar con un [**scatter plot**]{.hl-yellow} todas las variables y adem√°s sus correlaciones, con el paquete `{GGally}` y la funci√≥n `ggpairs(), sin necesidad de convertir a tidydata.

```{r}
#| code-fold: true
library(GGally)
ggpairs(datos) +
  theme_minimal()
```
   
---


## An√°lisis de dependencia

2. [**An√°lisis de dependencia**]{.hl-yellow}:
  - ¬øQu√© [**predictora est√° m√°s correlacionada (linealmente) con la variable objetivo**]{.hl-purple} a predecir? ¬øExiste otro tipo de dependencia (pendiente implementar en `R`)?
  - ¬øC√≥mo se [**relacionan las predictoras**]{.hl-purple} entre s√≠? ¬øEst√°n correlacionadas? Ideas: matriz de correlaciones, diagramas de dispersi√≥n vs Y, corrplots, etc

. . .

Este paso ser√° crucial en el [**contexto multivariante**]{.hl-yellow} pero en este caso simplemente vamos a ver como se relacionan linealmente las predictoras entre s√≠, y cu√°l de ellas es la **m√°s adecuada para predecir linealmente** `precio`

---

## An√°lisis de dependencia

El primer paso es la [**matriz de correlaciones**]{.hl-yellow} con la funci√≥n `cor()` o con la funci√≥n `correlate()` del paquete `{corrr}` (importa en tibble m√°s visual)

```{r}
library(corrr)
datos |> correlate()
```

. . .

* [**Respecto a Y**]{.hl-yellow}: predictoras con mayor cor lineal son `AGST` (m√°s calor, menos cosechas, sube el precio) y `HarvestRain` (m√°s lluvias, m√°s cosechas, baja el precio, ¬°el signo importa!)

* [**Dependencia entre predictoras**]{.hl-yellow}: las variables `Age`, `Year` y `FrancePop` presentan la misma informaci√≥n.


---

## An√°lisis de dependencia

Tambi√©n podemos usar `corrplot()` del paquete `{corrplot}`, al que le pasamos una matriz de correlaciones cl√°sica y nos la [**visualiza**]{.hl-yellow}.

```{r}
library(corrplot)
datos |> cor() |> corrplot(method = "ellipse")
```

Puedes ver distintas opciones de visualizaci√≥n en <https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html>

---

## An√°lisis de dependencia

Otra opci√≥n es [**visualizar con un scatter plots todas las predictoras vs Y**]{.hl-yellow}, pivotando antes nuestro dataset (solo pivotamos las predictoras)

:::: columns
::: {.column width="65%"}

```{r}
#| code-fold: true
datos_tidy <-
  datos |> pivot_longer(cols = -Price, names_to = "variable",
                        values_to = "values")
ggplot(datos_tidy, aes(x = values, y = Price)) + 
  geom_point(aes(color = variable), alpha = 0.7) +
  geom_smooth(method = "lm") +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal()
```

:::

::: {.column width="35%"}

No solo comprobamos que las rectas con m√°s pendiente son `AGST` y `HarvestRain`, adem√°s los puntos parecen poder ajustarse a una recta sin otro patr√≥n identificable.

:::
::::

Esto es importante hacerlo ya que debemos [**descartar posibles correlaciones esp√∫reas**]{.hl-red} (ver ejemplo datasaurus)

---

## Formulaci√≥n del modelo

Una vez que hemos decidido que dos predictoras usaemos, vamos por tanto a plantear [**dos modelos univariantes**]{.hl-yellow}

$$Price = \beta_0 + \beta_1*AGST + \varepsilon$$
$$Price = \beta_0 + \beta_1*HarvestRain + \varepsilon$$

---

## Fase de estimaci√≥n


4. [**Fase de estimaci√≥n**]{.hl-yellow}:
  - ¬øCu√°nto valen los [**par√°metros estimados**]{.hl-purple}? ¬øC√≥mo queda el ajuste?
  - ¬øQu√© [**interpretaci√≥n**]{.hl-purple} tienen?


&nbsp;

Para ello ejecutaremos ambos modelos con `lm()`

```{r}
ajuste_AGST <- lm(data = datos, formula = Price ~ AGST)
ajuste_harvest <- lm(data = datos, formula = Price ~ HarvestRain)
```


---

## Fase de estimaci√≥n

[**Ajuste con AGST**]{.hl-yellow}

```{r}
ajuste_AGST |> summary()
```

* $\beta_0=$ `r round(ajuste_AGST$coefficients[1], 3)`: predicci√≥n del precio cuando $AGST = 0$ es de -3 (recuerda que est√° en escala logart√≠mica)

* $\beta_1=$ `r round(ajuste_AGST$coefficients[2], 3)`: por cada grado de aumento, el precio sube `r round(ajuste_AGST$coefficients[2], 3)` (escala log).

---

## Fase de estimaci√≥n

[**Ajuste con AGST**]{.hl-yellow}

```{r}
#| echo: false
ajuste_AGST |> summary()
```

* [**Residuales**]{.hl-yellow}: adem√°s de media cero, parecen presentar una distribuci√≥n sim√©trica con la mediana en torno al cero. Adem√°s se tiene que $\widehat{\sigma}_{\varepsilon}^{2}=\frac{1}{n-2}\sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = 0.4819$ (estimador insesgado de la varianza residual) y $R^2 = 0.4456$ (bondad de ajuste)


---

## Fase de estimaci√≥n

[**Ajuste con harvestRain**]{.hl-yellow}


```{r}
ajuste_harvest |> summary()
```

* $\beta_0=$ `r round(ajuste_harvest$coefficients[1], 3)`: predicci√≥n del precio cuando la lluvia fue nula es de 7.679 (recuerda que est√° en escala logart√≠mica)

* $\beta_1=$ `r round(ajuste_harvest$coefficients[2], 3)`: por cada litro de lluvia, precio baja `r round(1000*ajuste_AGST$coefficients[2], 3)` (escala log).

---

## Fase de estimaci√≥n

[**Ajuste con harvestRain**]{.hl-yellow}

```{r}
#| echo: false
ajuste_harvest |> summary()
```

* [**Residuales**]{.hl-yellow}: adem√°s de media cero, parecen presentar una distribuci√≥n sim√©trica con la mediana en torno al cero. Adem√°s se tiene que $\widehat{\sigma}_{\varepsilon}^{2}=\frac{1}{n-2}\sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = 0.5577$ (algo m√°s grande que el otro ajuste) y $R^2 = 0.2572$ (algo m√°s peque√±o que el otro ajuste) -> de momento es mejor el primer modelo.

---

## Fase de diagnosis

5. [**Fase de diagnosis**]{.hl-yellow} (paquetes `{performance}` y `{olsrr}`):
  - ¬øCumplen los datos las [**hip√≥tesis par√°metricas**]{.hl-purple} requeridas para poder hacer inferencia? 
  - ¬øC√≥mo modificar los datos para que se cumplan?
  - An√°lisis de residuales
  
&nbsp;

Recuerda que [**necesitamos verificar antes las hip√≥tesis**]{.hl-yellow} para poder hacer inferencia con los par√°metros, as√≠ que vamos a ello con el paquete `{performance}` y el paquete `{olsrr}`

---

## Fase de diagnosis

```{r}
library(performance)
check_model(ajuste_AGST)
```

---

## Diagnosis: linealidad

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1 x$


```{r}
#| code-fold: true
check_model(ajuste_AGST)
```

Si te fijas el gr√°fico que se refiere a ello est√° [**visualizando residuales vs valores estimados**]{.hl-yellow}: est√° volviendo a plantear un segundo modelo de regresi√≥n donde ahora $\widehat{\varepsilon}_i = \gamma_0 + \gamma_1 \widehat{y}_i$

---

## Diagnosis: linealidad

```{r}
linealidad <- lm(data = tibble("fitted" = ajuste_AGST$fitted.values,
                               "residuals" = ajuste_AGST$residuals),
                 formula = residuals ~ fitted)
linealidad |> summary()
```

Si te fijas [**ambos par√°metros no son significativamente distintos de 0**]{.hl-green}: no presentan una tendencia (lineal al menos)

---

## Diagnosis: linealidad

M√°s adelante probaremos alguna otra cosa pero de momento nos basta con eso. Tambi√©n podemos [**visualizar nosotros ese scatter plot residuales vs estimaciones**]{.hl-yellow}

```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "residuals" = ajuste_AGST$residuals),
       aes(x = fitted, y = residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm") +
  theme_minimal()
```

---

## Diagnosis: homocedasticidad

2. [**Homocedasticidad**]{.hl-green}: necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow}, tal que $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \boldsymbol{X} = x \right] = cte < \infty$.

```{r}
check_heteroscedasticity(ajuste_AGST)
```

. . .

El gr√°fico titulado `Homogeneity of variance` nos visualiza la ra√≠z cuadrada del valor absoluto de los residuos estandarizados frente a las predicciones (se conoce como [**gr√°fico de escala-localizaci√≥n**]{.hl-yellow})

---

## Diagnosis: homocedasticidad

Si visualizamos los [**residuales**]{.hl-yellow} deber√≠an estar en torno a 0, dentro de una banda constante (varianza constante)


```{r}
#| code-fold: true
ggplot(tibble("id" = 1:length(ajuste_AGST$residuals),
              "residuals" = ajuste_AGST$residuals),
       aes(x = id, y = residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

---


## Diagnosis: homocedasticidad

Si visualizamos el gr√°fico de [**escala-localizaci√≥n**]{.hl-yellow} deber√≠amos obtener un diagrama de dispersi√≥n cuya recta de regresi√≥n saliese casi plana en torno al 1.


```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "sqrt_std_residuals" = sqrt(abs((ajuste_AGST$residuals - mean(ajuste_AGST$residuals)) / sd(ajuste_AGST$residuals)))),
       aes(x = fitted, y = sqrt_std_residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

Seg√∫n el gr√°fico no deber√≠amos asumir homocedasticidad. **¬øPor qu√© el contraste no la rechaza?**

---

## Diagnosis: homocedasticidad


```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "sqrt_std_residuals" = sqrt(abs((ajuste_AGST$residuals - mean(ajuste_AGST$residuals)) / sd(ajuste_AGST$residuals)))),
       aes(x = fitted, y = sqrt_std_residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

Con el poco tama√±o muestral que tenemos, es complicado tener evidencias que refuten la hip√≥tesis nula (y el gr√°fico puede estar parcialmente dise√±ado). Por eso es la [**hip√≥tesis m√°s dif√≠cil de cumplir**]{.hl-yellow}. Lo importante es que en [**la recta de regresi√≥n al dibujar los residuos no se aprecia una banda cuya anchura se modifique groseramente**]{.hl-green}, m√°s o menos constante


---

## Diagnosis: normalidad

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$

Con la funci√≥n `ols_test_normality()` del paquete `{olsrr}` podemos obtener diferentes contrastes de normalidad

```{r}
library(olsrr)
ols_test_normality(ajuste_AGST)
```

Nos centraremos en los contrastes de `Shapiro-Wilk`, `Kolmogorov-Smirnov` y `Anderson-Darling`: [**no se rechaza normalidad**]{.hl-yellow}

---

## Diagnosis: normalidad

Adem√°s del contraste podemos visualizar con `stat_qq()` y `stat_qq_line()` el conocido como [**Q-Q plot**]{.hl-yellow}: enfrenta los cuantiles de una muestra con los cuantiles de una normal te√≥rica, teniendo que **obtener los puntos en torno a una recta** (especilamente en el centro).


```{r}
#| code-fold: true
ggplot(tibble("residuals" = ajuste_AGST$residuals)) +
  stat_qq(aes(sample = residuals)) +
  stat_qq_line(aes(sample = residuals))
```


---

## Diagnosis: independencia

4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser independientes entre s√≠ (el error en una observaci√≥n no depende de otras). En particular, ser√°n **incorrelados**

$${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] - E \left[\varepsilon_i \right] E \left[\varepsilon_j \right] = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$$

```{r}
check_autocorrelation(ajuste_AGST)
```

Por √∫ltimo, `check_autocorrelation()` comprueba como efectivamente los [**residuales/errores son independientes**]{.hl-yellow}, haciendo un test de autocorrelaci√≥n (nos tiene que salir lo contrario a una serie temporal, que el error i no depende del i-1).

---

## Diagnosis: independencia

Otra forma de verlo es [**visualizando los residuos respecto a su versi√≥n con retardo**]{.hl-yellow} (por ejemplo, $\left(\widehat{\varepsilon}_1, \widehat{\varepsilon}_2, \widehat{\varepsilon}_3, \ldots, \widehat{\varepsilon}_{n-1} \right)$ vs $\left(\widehat{\varepsilon}_2, \widehat{\varepsilon}_3, \ldots, \widehat{\varepsilon}_n \right)$ 

```{r}
#| code-fold: true
ggplot(tibble("lag1" = ajuste_AGST$residuals[-length(ajuste_AGST$residuals)],
              "residuals" = ajuste_AGST$residuals[-1]),
       aes(x = residuals, y = lag1)) +
  geom_point(size = 3, alpha = 0.7) + 
  geom_smooth(method = "lm") + 
  theme_minimal()
```

---

## Fase de diagnosis

```{r}
check_model(ajuste_AGST)
```

En nuestro caso se cumplen todas las hip√≥tesis (algunas m√°s fuertemente que otras).

[**Repite el proceso con el otro modelo**]{.hl-yellow}

---

## Fase de diagnosis

```{r}
check_predictions(ajuste_AGST)
```

Nos faltan dos gr√°ficas por comentar:

* `Posterior Predictive Checks`: [**simula distintas variables respuesta**]{.hl-yellow} suponiendo que el modelo fuese cierto (a√±adiendo ruido aleatorio) y lo compara con la muestra. [**Si lo observado se distancia mucho de las simulaciones**]{.hl-red} es que el modelo planteado no ajusta bien a la muestra.

---

## Fase de diagnosis

```{r}
check_outliers(ajuste_AGST)
```

* `Influential Observations`: nos permite identificar [**observaciones influyentes**]{.hl-yellow}, marcando aquellas (con su id de fila) que se salgan fuera de la banda definida por la conocida como **distancia de Cook** denotada como $D_i$ (realiza, para cada observaci√≥n, la suma de todos los cambios de la regresi√≥n cuando la observaci√≥n $i$ es retirada: si hay muchos cambios al cambiar una observaci√≥n, es que era muy influyente)

. . .

Diferencia dos tipos: 

* [**outliers**]{.hl-yellow}: valor at√≠pico de la **respuesta** pudiendo perturbar la varianza residual
* [**high-leverage points**]{.hl-yellow}: valor at√≠pico en alguna de las **predictoras**

---

## Fase de inferencia

6. [**Fase de inferencia**]{.hl-yellow}:
  - ¬øQu√© [**variabilidad**]{.hl-purple} tienen las estimaciones de nuestro par√°metros?
  - ¬øLas predictoras/intercepto tienen un [**efecto lineal significativo**]{.hl-purple}?
  - ¬øDebemos [**re-entrenar el modelo**]{.hl-purple} sin alguno de ellos?

&nbsp;

Una vez verificadas las hip√≥tesis lo que haremos ser√° [**inferir conclusiones de la poblaci√≥n en funci√≥n de la muestra**]{.hl-yellow}

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Variabilidad**]{.hl-yellow} de las estimaciones de nuestro par√°metros

  - $\widehat{SE} \left( \widehat{\beta}_0 \right)$ igual a 2.344 por lo que (aprox) $\widehat{\beta_0} \sim N(-3.547, 2.344)$
  - $\widehat{SE} \left( \widehat{\beta}_1 \right)$ igual a 0.143  por lo que (aprox) que $\widehat{\beta_1} \sim N(0.643, 0.143)$
  

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Estad√≠stico**]{.hl-yellow} del contraste

  - $\frac{\widehat{\beta}_0 - 0}{\widehat{SE} \left( \widehat{\beta}_0 \right)}$ igual a -1.5 (valor que tendr√≠as que buscar en las tablas a mano)
  - $\frac{\widehat{\beta}_1 - 0}{\widehat{SE} \left( \widehat{\beta}_1 \right)}$ igual a 4.483 (valor que tendr√≠as que buscar en las tablas a mano)
  
  
  
---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Efecto (lineal)**]{.hl-yellow}: si nos fijamos en la tabla, el p-valor de $\beta_0$ es 0.146052. Si adoptamos $\alpha = 0.05$ como suele ser habitual, el contraste $H_0:~\beta_0 = 0$ vs $H_1:~\beta_0 \neq 0$ nos dice que [**no podemos rechazar de forma significativa la hip√≥tesis nula**]{.hl-red} (no sucede con $\beta_1$, si sucediese no habr√≠a modelo)

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

[**¬øY si quitamos dicho par√°metro?**]{.hl-yellow}

. . .

Para quitarlo a√±adimos un -1 al modelo

```{r}
ajuste_AGST_sin_beta0 <- lm(data = datos, formula = Price ~ -1 + AGST)
```
 
---

## Re-aprendiendo

```{r}
ajuste_AGST_sin_beta0 |> summary()
```

* La [**bondad de ajuste**]{.hl-yellow} ha pasado de $R^2 = 0.446$ a $R^2 = 0.9953$

* La [**variabilidad de la estimaci√≥n**]{.hl-yellow} $\widehat{SE} \left( \widehat{\beta}_1 \right)$  ha pasado de 0.143 a 0.005757.

---

## Comparar modelos

Aunque no hemos hablado en profundidad de las **m√©tricas de evaluaci√≥n** podemos [**comparar los modelos**]{.hl-yellow} con `compare_performance()` del paquete `{performance}`

```{r}
#| echo: false
ajuste_harvest_sin_beta0 <- lm(data = datos, formula = Price ~ -1 + HarvestRain)
```
```{r}
compare_performance(ajuste_AGST, ajuste_AGST_sin_beta0,
                    ajuste_harvest, ajuste_harvest_sin_beta0)
```

# Clases 9: evaluaci√≥n y predicci√≥n {#clase-9}

[**¬øC√≥mo se puede evaluar un modelo? ¬øQu√© m√©tricas existen? ¬øC√≥mo predecir?**]{style="color:#444442;"}

---

## Repitamos el proceso


Para interiorizar lo aprendido, vamos a repetir todo el proceso con el conjunto `datos_linearreg.csv` (las variables predictoras representan diferentes variables meteorol√≥gicas y la variable objetivo `y` la temperatura media en primavera, para distintas ciudades).

```{r}
datos <- read_csv(file = "./datos/datos_linearreg.csv")
datos
```

---

## Regresi√≥n lineal

Debes seguir los siguientes pasos de la manera m√°s detallada posible

1. [**An√°lisis exploratorio inicial**]{.hl-yellow} tanto num√©rico como visualizando. ¬øSon num√©ricas sin problemas de codificaci√≥n? ¬øC√≥mo se distribuyen? ¬øHay datos at√≠picos?

2. [**An√°lisis de dependencia**]{.hl-yellow}. ¬øQu√© predictora correlaciona m√°s con la objetivo? ¬øC√≥mo se relacionan las predictoras entre s√≠?

3. [**Formulaci√≥n**]{.hl-yellow} del modelo

4. [**Fase de estimaci√≥n**]{.hl-yellow}. ¬øCu√°les son los par√°metros? ¬øC√≥mo se interpretan?

5. [**Fase de diagnosis**]{.hl-yellow}

6. [**Fase de inferencia**]{.hl-yellow}. ¬øQu√© variabilidad tiene la estimaci√≥n? ¬øHay efecto significativo? 


---

## Fase de evaluaci√≥n

```{r}
ajuste_lineal <- lm(data = datos, formula = y ~ x1)
ajuste_lineal |> summary()
```

Ten√≠amos pendiente la fase final: [**fase de evaluaci√≥n**]{.hl-yellow}

--- 

## Fase de evaluaci√≥n

7. [**Fase de evaluaci√≥n**]{.hl-yellow}:
  - ¬øEs significativo el modelo? [**ANOVA: an√°lisis de la varianza**]{.hl-purple}
  - ¬øQu√© informaci√≥n de la predictora explica el modelo? [**Par√°metros de bondad de ajuste**]{.hl-purple} ($R^2$ por ejemplo)
  - ¬øQu√© otras m√©tricas o herramientas podemos usar para [**cuantificar la calidad predictora de nuestro ajuste**]{.hl-purple}
  
. . . 

Una de las herramientas m√°s √∫tiles para evaluar nuestro modelo es [**enfrentar los valores ajustados con los valores reales**]{.hl-yellow} (dado que los conocemos al ser aprendizaje supervisado)

---

## Fase de evaluaci√≥n


```{r}
#| code-fold: true
ggplot(tibble("y" = datos$y, "y_est" = ajuste_lineal$fitted.values),
       aes(x = y, y = y_est)) +
  geom_point(size = 1.2, alpha = 0.75) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  labs(x = "Valores reales", y = "Valores estimados")
```

En el gr√°fica podemos ver como los [**valores reales vs estimados**]{.hl-green} est√°n muy cercanos a la diagonal: el error cometido es muy peque√±o.

---

## Fase de evaluaci√≥n

Podemos considerar algunas [**m√©tricas para cuantificar el acierto del modelo**]{.hl-yellow}

. . .

* [**SSE**]{.hl-red} (sum of squared errors): definido como la suma de errores al cuadrado

$$SSE = \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 = \sum_{i=1}^{n} \left( Y_i - \widehat{Y}_i \right)^2$$
F√≠jate que dado que $\widehat{\sigma}_{\varepsilon} = \widehat{\sigma}_{r} = \frac{n}{n-p-1} s_{r}^{2} =  \frac{1}{n-p-1} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2$ (el que `R` llama `Residual standard error`), tenemos que $SSE = (n-p-1)\widehat{\sigma}_{\varepsilon}$

. . .

* [**MSE**]{.hl-red}: media de lo anterior $MSE = s_{r}^{2} = \frac{1}{n} SSE  = \frac{1}{n} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 = \frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}$

. . .

Ambas m√©tricas nos hablan de la [**varianza del error**]{.hl-red}, es decir, de lo que el [**modelo no es capaz de explicar**]{.hl-red}

---


## Fase de evaluaci√≥n



* [**SSR**]{.hl-green} (regressions sum of squares): definido como la suma de las desviaciones de cada predicci√≥n a su media (al tener estimadores insesgados, la media de las estimaciones $\overline{\widehat{Y}}$ es la misma que la de la variable a estimar $\overline{Y}$)

$$SSR = \sum_{i=1}^{n} \left( \overline{Y} - \widehat{Y}_i \right)^2 = \sum_{i=1}^{n} \left( \overline{\widehat{Y}}_i - \widehat{Y}_i \right)^2$$

. . .


* [**MSR**]{.hl-green}: media de lo anterior $MSR = s_{\widehat{y}}^2 = \frac{1}{n} SSR  = \frac{1}{n} \sum_{i=1}^{n} \left( \overline{Y} - \widehat{Y}_i \right)^2$

. . .

Ambas m√©tricas nos hablan de la [**variaci√≥n de Y en torno a la regresi√≥n**]{.hl-green}, es decir, la variaci√≥n de $\overline{Y}$ que es explicada por la media condicional estimada $(Y_i|X=x_i) \sim \widehat{\beta}_0 + \widehat{\beta}_1 X_i$, cuantifica la [**informaci√≥n de Y explicada por el modelo**]{.hl-green}

---

## Fase de evaluaci√≥n


* [**SST**]{.hl-yellow} (total sum of squares): definido como la suma de las desviaciones de la variable objetivo $Y$ a su media.

$$SST =  \sum_{i=1}^{n} \left( Y_i - \overline{Y} \right)^2 $$

. . .

* [**Varianza muestral de Y**]{.hl-yellow}: la media de lo anterior (la varianza de $Y$)

$$s_{y}^2= \frac{1}{n} SST  = \frac{1}{n}\sum_{i=1}^{n} \left( Y_i - \overline{Y} \right)^2$$



. . .


Ambas m√©tricas nos hablan de la [**variaci√≥n total de Y**]{.hl-yellow}, es decir, la [**informaci√≥n total de nuestra variable objetivo**]{.hl-yellow}

---

## ANOVA

As√≠ tenemos 3 tipos de m√©tricas:

* `SST` y $s_{y}^2$: el [**total de info a explicar**]{.hl-yellow}

* `SSR` y $MSR$: el [**total de info explicada por el modelo**]{.hl-green}

* `SSE` y $MSE$: el [**total de info NO explicada por el modelo**]{.hl-red} (a veces se usa la ra√≠z cuadrada del MSE, conocido como $RMSE$, o el $MAE$, tomando valor absoluto en los errores).

. . .

Se pueden demostrar que, [**SOLO EN EL CASO LINEAL**]{.hl-purple}, desarrollando el sumatorio $SST =  \sum_{i=1}^{n} \left( Y_i - \overline{Y} \right)^2 = \sum_{i=1}^{n} \left( \left(Y_i - \widehat{Y}_i \right) +  \left(  \widehat{Y}_i - \overline{Y} \right) \right)^2$ se llega a que

[**SST**]{.hl-yellow} =  [**SSR**]{.hl-green} + [**SSE**]{.hl-red}

---


## ANOVA

[**SST**]{.hl-yellow} =  [**SSR**]{.hl-green} + [**SSE**]{.hl-red}


. . .

[$s_{y^2}$]{.hl-yellow} = [$s_{\widehat{y}}^2$]{.hl-green} + [$s_{r}^2$]{.hl-red} (equivalente, [$s_{y^2}$]{.hl-yellow} = [$s_{\widehat{y}}^2$]{.hl-green} + [$\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}$]{.hl-red})

. . .

Esta decomposici√≥n (solo se cumple en el caso lineal) se conoce como [**ANOVA o an√°lisis de la varianza**]{.hl-yellow} y podemos hacerlo en `R` con `aov()` o `anova()`

:::: columns
::: {.column width="45%"}

```{r}
ajuste_lineal |> aov()
```

:::

::: {.column width="55%"}

```{r}
ajuste_lineal |> anova()
```

:::
::::

---

## ANOVA

```{r}
ajuste_lineal |> aov()
```

| Terms | x1 (predictora) | Residuals |
|:---------:|:-----:|:------:|
| Sum of Squares  | SSR   |    SSE | 
| Deg. of Freedom  (grados libertad)  | p  |  n - p - 1 |

&nbsp;

`Residual standard error`: $\widehat{\sigma}_{\varepsilon}^{2}= \frac{n}{n-p-1} s_{r}^{2} = \frac{1}{n-p-1} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2}$

---

## ANOVA

```{r}
ajuste_lineal |> anova()
```

|  | Df (grados) | Sum Sq | Mean Sq | F value | Pr(>F) |
|:---------:|:-----:|:------:|:------:|:------:|:------:|
| x1  | p  | SSR | $\frac{SSR}{p}$ | $F-value = \frac{\frac{SSR}{p}}{\frac{SSE}{n-p-1}}$ | p-valor F test
| Residuals  | n - p - 1 | SSE | $\widehat{\sigma}_{\varepsilon}^{2} = \frac{SSE}{n-p-1}$ | | |

El F-value es el estad√≠tico  $F = \frac{\frac{SSR}{p}}{\frac{SSE}{n-p-1}} \sim F_{p, n-p-1}$ asociado al [**contraste de significaci√≥n global**]{.hl-yellow}

$$H_0:~\beta_1 = \ldots = \beta_p = 0 \quad vs \quad H_1:~\text{existe al menos un} \quad \beta_j \neq 0$$

---

## ANOVA

```{r}
ajuste_lineal |> anova()
```


$$H_0:~\beta_1 = \ldots = \beta_p = 0 \quad vs \quad H_1:~\text{existe al menos un} \quad \beta_j \neq 0$$

El contraste pretende responder a: [**¬øexiste una dependencia lineal entre $Y$ y el CONJUNTO de predictoras?**]{.hl-yellow} (global, no par√°metro a par√°metro).

. . .

Si se [**rechaza**]{.hl-yellow} significa que [**existe al menos un predictor cuyo efecto LINEAL sobre Y es significativo**]{.hl-yellow}.

. . .

[**Importante**]{.hl-red}: en el caso de la reg. lineal univariante, $F-value$ y $p-value$ del ANOVA es equivalente al $t-value$ y $p-value$ del contraste de significaci√≥n para $\beta_1$ (ya que...no hay m√°s).

---

## Bondad de ajuste

El conocido como $R^2$ o [**bondad de ajuste**]{.hl-yellow} o el [**coeficiente de determinaci√≥n**]{.hl-yellow} es una forma sencilla, y relacionada con el ANOVA, para cuantificar la [**cantidad de informaci√≥n explicada por el modelo**]{.hl-yellow}

. . .

De hecho es, literal, un [**ratio de informaci√≥n explicada**]{.hl-yellow} (lo que `R` llama `Multiple R-squared`, ya hablaremos de su versi√≥n ajustada en regresi√≥n multivariante)

$$R^2 = \text{Ratio info explicada} = \frac{SSR}{SST}$$

. . .

En el caso lineal, por lo visto en el ANOVA

$$R^2= \frac{SSR}{SST} = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST} = 1 - \frac{s_{r}^{2}}{s_{y}^{2}} =  1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^{2}}{s_{y}^{2}}$$

. . .

Por definici√≥n, la [**bondad de ajuste est√° entre 0 y 1**]{.hl-yellow}.

---

## Bondad de ajuste

[**Deberes: demuestra que $R^2 = r_{y \widehat{y}}^2$ y que en el caso de $p=1$ coincide adem√°s con $R^2 = r_{x,y}^2$**]{.hl-yellow}




$$R^2 = r_{y \widehat{y}}^2 =_{p=1} r_{x,y}^2$$

. . .

La bondad de ajuste tiene un [**problema importante**]{.hl-red}: no solo depende del modelo sino que tambi√©n de los datos. ¬øDe qu√© depende? ¬øPor qu√© es [**peligroso usar ciegamente $R^2$ para decidir**]{.hl-red}?



---

## Bondad de ajuste

$$R^2= \frac{SSR}{SST} = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST} = 1 - \frac{s_{r}^{2}}{s_{y}^{2}} =  1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^{2}}{s_{y}^{2}}$$

¬øDe qu√© depende?

. . .

* [**M√°s predictoras**]{.hl-yellow} implica que [**$R^2$ crece**]{.hl-yellow}, ¬°incluso aunque dichas [**predictoras no sean significativas**]{.hl-red}! (esto lo arreglaremos con el $R^2$ ajustado en el futuro)

* [**M√°s ruido, menor $R^2$**]{.hl-yellow}

* [**Ignora si el modelo cumple las hip√≥tesis**]{.hl-yellow}: un modelo con un alto $R^2$ puede dar predicciones nefastas.
  
  
---

## Predictoras vs R2

Vamos a repetir pero [**a√±adiendo 5 y 22 variables m√°s**]{.hl-yellow}, sin efecto lineal con $Y$.

```{r}
datos_extras <-
  tibble("y" = datos$y, "x1" = datos$x1, "x2" = rnorm(1e3), "x3" = rnorm(1e3),
         "x4" = rnorm(1e3), "x5" = rnorm(1e3), "x6" = rnorm(1e3), "x7" = rnorm(1e3),
         "x8" = rnorm(1e3), "x9" = rnorm(1e3), "x10" = rnorm(1e3), "x11" = rnorm(1e3),
         "x12" = rnorm(1e3), "x13" = rnorm(1e3), "x14" = rnorm(1e3), "x15" = rnorm(1e3),
         "x16" = rnorm(1e3), "x17" = rnorm(1e3), "x18" = rnorm(1e3), "x19" = rnorm(1e3),
         "x20" = rnorm(1e3), "x21" = rnorm(1e3), "x22" = rnorm(1e3), "x23" = rnorm(1e3))
ajuste_6pred <- lm(data = datos_extras, formula = y ~ x1 + x2 + x3 + x4 + x5 + x6)
ajuste_23pred <- lm(data = datos_extras, formula = y ~ .)
compare_performance(ajuste_lineal, ajuste_6pred, ajuste_23pred)
```

Con `compare_performance()` podemos comparar m√©tricas de modelos.

---

## Ruido vs R2

Hemos dicho que el ruido afecta...¬øqu√© crees que **pasar√° si fijamos la parte determin√≠stica y solo modificamos el ruido**? Vamos a [**simular 6 modelos**]{.hl-yellow}, con exactamente los mismos $\beta_0$ y $\beta_1$ (es decir, **mismo ajuste**) pero con diferente varianza en el ruido (supongamos que $X \sim N(3, 1.5)$).

$$\text{Modelo 1: } Y = -1.2 + 3.2X + N(0, 0.25)$$
$$\text{Modelo 2: } Y = -1.2 + 3.2X + N(0, 1)$$

$$\text{Modelo 3: } Y = -1.2 + 3.2X + N(0, 1.5)$$

$$\text{Modelo 4: } Y = -1.2 + 3.2X + N(0, 2)$$

$$\text{Modelo 5: } Y = -1.2 + 3.2X + N(0, 4)$$


$$\text{Modelo 6: } Y = -1.2 + 3.2X + N(0, 8)$$

---

## Ruido vs R2


```{r}
#| code-fold: true
x <- rnorm(n = 1000, mean = 3, sd = 1.5)
eps1 <- rnorm(n = 1000, mean = 0, sd = 0.25)
eps2 <- rnorm(n = 1000, mean = 0, sd = 1)
eps3 <- rnorm(n = 1000, mean = 0, sd = 1.5)
eps4 <- rnorm(n = 1000, mean = 0, sd = 2)
eps5 <- rnorm(n = 1000, mean = 0, sd = 4)
eps6 <- rnorm(n = 1000, mean = 0, sd = 8)
datos <- tibble("x" = x, "y1" = -1.2 + 3.2*x + eps1,
                "y2" = -1.2 + 3.2*x + eps2, "y3" = -1.2 + 3.2*x + eps3,
                "y4" = -1.2 + 3.2*x + eps4, "y5" = -1.2 + 3.2*x + eps5,
                "y6" = -1.2 + 3.2*x + eps6)
ajuste_1 <- lm(data = datos, formula = y1 ~ x)
ajuste_2 <- lm(data = datos, formula = y2 ~ x)
ajuste_3 <- lm(data = datos, formula = y3 ~ x)
ajuste_4 <- lm(data = datos, formula = y4 ~ x)
ajuste_5 <- lm(data = datos, formula = y5 ~ x)
ajuste_6 <- lm(data = datos, formula = y6 ~ x)
```

```{r}
compare_performance(ajuste_1, ajuste_2, ajuste_3, ajuste_4, ajuste_5, ajuste_6)
```

---

## Ruido vs R2

En todos los casos el [**ajuste debe ser (aprox) el mismo**]{.hl-yellow} ya que el modelo de regresi√≥n busca [**modelizar los efectos lineales no aleatorios**]{.hl-yellow} entre la variable objetivo y los predictores.

. . .

[**Moraleja**]{.hl-green}: tener un $R^2$ no implica que el modelo sea malo, ya que la [**cantidad de informaci√≥n no modelizable**]{.hl-yellow} puede deberse a una [**cantidad alta de ruido (algo aleatorio no predecible)**]{.hl-red}. Por ello es importante usar m√°s herramientas que un mero coeficiente para valorar un ajuste (por ejemplo, en campos como la sociolog√≠a o la econom√≠a la bondad de ajuste ser√° generalmente bajo)

. . .

&nbsp;

[**Deberes**]{.hl-yellow}: ¬øc√≥mo ilustrar gr√°ficamente que a mayor varianza del ruido, menor es $R^2$? Dise√±a un estudio de simulaci√≥n para ello con distintos modelos y gr√°fica la ca√≠da de $R^2$.

---

## Diagnosis vs R2

Entonces, si tenemos un modelo con un alto $R^2$, [**¬øno hace falta que cumpla las hip√≥tesis?**]{.hl-yellow}

. . .

Vamos a simular un modelo que [**incumple**]{.hl-red}

* Linealidad

* Homocedasticidad

. . .

La variable predictora $x_i = 0.01 + 0.01*(i-1)$ ($i = 1,\ldots, n = 200$) ser√° la siguiente

```{r}
x <- seq(0.01, 2, l = 200)
```


[**¬øC√≥mo podr√≠amos crear una $Y$ cuya relaci√≥n con $X$ sea no lineal?**]{.hl-yellow}

---

## Diagnosis vs R2

[**¬øC√≥mo podr√≠amos crear una $Y$ cuya relaci√≥n con $X$ sea no lineal?**]{.hl-yellow}

Tenemos muchas maneras, por ejemplo:

$$Y = X + X^2 + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

$$Y = \log(X^2) - cos(X) + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

$$Y = \frac{1}{X + 1} + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

$$Y = 1 - 2X(1 + 0.25 \sin(4 \pi X)) + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

. . .

Simula este √∫ltimo modelo (con $\sigma_{\varepsilon} = 0.5$) y realiza el ajuste

---

## Diagnosis vs R2

```{r}
x <- seq(0.15, 2, l = 200)
eps <- rnorm(n = 200, sd = 0.5)
y <- 1 - 2 * x * (1 + 0.25*sin(4 * pi * x)) + eps
datos <- tibble("y" = y, "x" = x)
ajuste_lineal <- lm(data = datos, formula = y ~ x)
ajuste_lineal |> summary()
```

. . .

[**¬øC√≥mo podr√≠amos hacer que la hip√≥tesis de homocedasticidad no se cumpla**]{.hl-yellow}


---

## Diagnosis vs R2

Vamos a considerar que la [**varianza del error no es cte, crece seg√∫n aumenta x**]{.hl-yellow}

$$y_i = f(x_i) + \varepsilon_i, \quad \varepsilon_i \sim N(0, \sigma_{\varepsilon,i}), \quad \sigma_{\varepsilon,i} = 0.25 * x_{i}^2 \quad i=1, \ldots, 200$$

```{r}
#| code-fold: true
x <- seq(0.01, 2, l = 200)
eps <- rnorm(n = 200, sd = 0.25 * x^2)
y <- 1 - 2 * x * (1 + 0.25*sin(4 * pi * x)) + eps
datos <- tibble("y" = y, "x" = x, "sigma" =  0.25 * x^2)
ajuste_lineal <- lm(data = datos, formula = y ~ x)
ajuste_lineal |> summary()
```

---

## Diagnosis vs R2

```{r}
#| code-fold: true
ggplot(datos, aes(x = x, y = y)) +
  geom_point(aes(color = sigma), size = 3, alpha = 0.75) +
  geom_smooth(method = "lm") +
  theme_minimal()
```

Aunque el $R^2$ es bastante alto, el modelo no tiene sentido, dando [**estimaciones cada vez m√°s erradas**]{.hl-red} seg√∫n nos movemos hacia la derecha en el eje X.


---

## Cosicas a examen

Algunas cosas que podr√≠an caer en la primera entrega:

. . .

* Realizar un [**an√°lisis exploratorio inicial**]{.hl-yellow} lo m√°s completo posible. Elegir de manera justificada [**la mejor predictora**]{.hl-yellow}

. . .

* Saber [**ajustar e INTERPRETAR**]{.hl-yellow} un modelo de regresi√≥n.

. . .

* Realizar e interpretar la [**fase de diagnosis**]{.hl-yellow} lo m√°s completa posible para luego interpretar (si procede) la [**inferencia**]{.hl-yellow} del modelo.

. . .

* Saber hacer una [**correcta fase de evaluaci√≥n**]{.hl-yellow} dle modelo.

. . .

* Saber [**simular modelos de regresi√≥n**]{.hl-yellow} (tanto que cumplan las hip√≥tesis como que no las cumplan).


# Clase 10: predicci√≥n y resumen {#clase-10}

[**Fase de predicci√≥n. Resumen**]{style="color:#444442;"}


---

## Resumen

Un breve resumen de lo aprendido sobre reg. lineal univariante

* [**Modelo supervisado de predicci√≥n**]{.hl-yellow}: hay una variable objetivo $Y$ continua (num√©rica) cuyo valor real conocemos

. . .

* [**La visualizaci√≥n importa**]{.hl-yellow}: no fies tu an√°lisis solo a los par√°metros matem√°ticos, la visualizaci√≥n ayuda a entender los datos.

. . .

* [**Relaci√≥n entre variables**]{.hl-yellow}: buscamos predictoras muy correladas (linealmente) con $Y$ y lo m√°s incorreladas/independientes entre ellas.

. . .

* [**Modelo**]{.hl-yellow}: el modelo par√°metrico se resume en $Y = \beta_0 + \beta_1 X + \varepsilon$, donde $\varepsilon$ es una variable aleatoria (ruido).


. . .

* [**Estimaci√≥n**]{.hl-yellow}: la estimaci√≥n viene modelizada bajo la hip√≥tesis de linealidad

$$E[Y|X=x] = \beta_0 + \beta_1 x$$




# Clase 11: entrega I {#clase-11}


[**Entrega I**]{style="color:#444442;"}

# Clase 12: incumpliendo hip√≥tesis {#clase-12}

[**¬øC√≥mo simular datos que incumplan las hip√≥tesis?**]{style="color:#444442;"}

---

## Incumpliendo hip√≥tesis

Los contenidos vistos en esta clase se subir√°n resumidos en formato notebook.


# Clase 13: reg. multivariante {#clase-13}

[**Introducci√≥n a la regresi√≥n multivariante. Formulaci√≥n del modelo y estimaci√≥n**]{style="color:#444442;"}

## Formulaci√≥n multivariante

De aqu√≠ en adelante llamaremos [**modelo multivariante**]{.hl-yellow} a todo modelo en el que $p > 1$ (es decir, tenemos m√°s de una variable predictora).


$$Y = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right)$$
tal que $E \left[ \varepsilon | \left( X_1 = x_1, \ldots, X_p = x_p \right) \right] = 0$.

. . .

En el caso del [**modelo lineal multivariante**]{.hl-yellow} se traducir√° por tanto en


$$E \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p, \quad \widehat{Y} = \widehat{\beta_0} + \displaystyle \sum_{j=1}^{p} \widehat{\beta_j} X_j $$

El objetivo seguir√° siendo obtener la estimaci√≥n de los $\widehat{\beta}$ tal que [**minimicemos el error**]{.hl-yellow}

---

## Formulaci√≥n matricial


Su formulaci√≥n muestral la podemos expresar mediante la [**matriz de dise√±o**]{.hl-yellow}

$$\mathbf{X} =\begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}$$

tal que $Y_i = \beta_0 + \sum_{j=1}^{p} \beta_j X_j + \varepsilon_i$, para todo elemento de la muestra $i=1,\ldots, n$


$$ \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix}_{(p+1)\times1} + \begin{pmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{pmatrix}_{n\times1}$$


---

## Formulaci√≥n matricial

La estimaci√≥n ser√° por tanto


$$\widehat{\mathbf{Y}} = \begin{pmatrix} \widehat{Y}_1 \\ \widehat{Y}_2 \\ \vdots \\ \widehat{Y}_n \end{pmatrix}_{n\times1} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \widehat{\beta}_1 \\ \widehat{\beta}_2 \\ \vdots \\ \widehat{\beta}_p \end{pmatrix}_{(p+1)\times1}$$


---

## Varianza residual

Como suced√≠a antes, el objetivo ser√° minimizar la [**varianza residual o error cuadr√°tico medio**]{.hl-yellow} (varianza del error), o lo que hemos llamado $SSR$

$$SSR \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \displaystyle \sum_{i=1}^{n} \left(Y_i - \beta_0 - \beta_1 X_{i1} - \ldots - \beta_p X_{ip} \right)^2 $$
¬øC√≥mo quedar√≠a **matricialmente**?

. . .

$$SSR \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = \varepsilon^{T} \varepsilon$$

As√≠, aplicando el [**m√©todo de los m√≠nimos cuadrados**]{.hl-yellow}, los [**par√°metros √≥ptimos**]{.hl-yellow} ser√°n aquellos $\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p\right)$ que minimicen dicha suma

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p \right) = \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)$$


Y repetimos la idea: [**calcularemos la derivada respecto a $\widehat{\boldsymbol{\beta}}$ e igualamos a cero**]{.hl-yellow}

--- 


## M√©todo m√≠nimos cuadrados

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p \right) = \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)$$

Derivando matricialmente tenemos (recuerda: $\left(A B \right)^{T} = B^{T} A^{T}$)

$$\begin{eqnarray}\frac{\partial SSR \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=& \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} \nonumber \\  &=& \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} + \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}}{\partial \boldsymbol{\beta}} \nonumber \\  &=& -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X} - \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \mathbf{X}^{T} \nonumber \\  &=& -\mathbf{X}^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)  - \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \mathbf{X}^{T} \nonumber \\ &=& -2\mathbf{X}^{T} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = 0 \end{eqnarray}$$

---

## M√©todo m√≠nimos cuadrados


$$\begin{eqnarray}\frac{\partial SSR \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=&  -2\mathbf{X}^{T} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = 0 \end{eqnarray}$$

Si descartamos el $-2$ como constante, tenemos que

$$\mathbf{X}^{T} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = 0 \longrightarrow \mathbf{X}^{T}  \mathbf{Y} = \mathbf{X}^{T} \mathbf{X} \boldsymbol{\beta}$$

Si ahora despejamos $\beta$  multiplicando en ambos lados por $\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}$ tenemos finalmente

$$\widehat{\boldsymbol{\beta}}=\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$$

---

## Estimaci√≥n

F√≠jate que cuando $p = 1$ tenemos que

$$\widehat{\mathbf{Y}} = \begin{pmatrix} \widehat{Y}_1 \\ \vdots \\ \widehat{Y}_n \end{pmatrix}_{n\times1} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \begin{pmatrix} 1 & X_{11} \\\vdots & \vdots \\ 1 & X_{n1} \end{pmatrix}_{n \times 2}\begin{pmatrix} \widehat{\beta}_0 \\ \widehat{\beta}_1 \end{pmatrix}_{2\times1}$$

y que por tanto $\widehat{\boldsymbol{\beta}}= \left(\begin{pmatrix} 1 & \ldots & 1 \\ X_{11}  & \ldots & X_{n1} \end{pmatrix} \begin{pmatrix} 1 & X_{11} \\ 1 & X_{21}  \\ \vdots & \vdots \\ 1 & X_{n1} \end{pmatrix} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$

[**Deberes**]{.hl-red}: ser√≠a interesante que para el examen supieses demostrar que cuando $p=1$ esa expresi√≥n acaba en 

$$\widehat{\beta}_1  = \frac{s_{xy}}{s_{x}^{2}}, \quad \widehat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$$

---

## Estimaci√≥n

$$\widehat{\boldsymbol{\beta}}=\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$$

* La [**estimaci√≥n**]{.hl-yellow} ser√° por tanto

$$\widehat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\beta}}=\mathbf{X}\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y} = \mathbf{H}\mathbf{Y}$$

* La matriz $\mathbf{H} = \mathbf{X}\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}$ se conoce como [**hat matrix o matriz de proyecci√≥n**]{.hl-yellow} (ya que hace que las estimaciones $\hat{y}$ sean en realidad los valores $y$ proyectados verticalmente sobre el plano de regresi√≥n ajustado).



---

## Interpretaci√≥n

Al igual que pasaba antes ser√° importante [**distinguir lo poblaci√≥n de lo muestral**]{.hl-yellow}

* Los par√°metros $\left( \beta_0, \beta_1, \ldots, \beta_p \right)$ son desconocidos, los [**par√°metros poblacionales**]{.hl-yellow}

* Los par√°metros $\left( \widehat{\beta}_0, \widehat{\beta}_1 , \ldots, \widehat{\beta}_p\right)$ son estimados a partir de los datos, son [**variables aleatorias**]{.hl-yellow} ya que han sido calculados en funci√≥n de una muestra aleatoria.

&nbsp;

[**¬øC√≥mo se interpretan ahora los par√°metros?**]{.hl-yellow}

---

## Interpretaci√≥n

$$\widehat{\mathbf{Y}} = \mathbf{X}\widehat{\boldsymbol{\beta}}= \widehat{\beta}_0 + \sum_{j=1}^{p} \widehat{\beta}_j X_j$$

* [**Ordenada en el origen**]{.hl-yellow}: tambi√©n llamado **intercepto**, y denotado como $\beta_0$ su valor real, es el valor de $Y$ cuando $X_1 = \ldots = X_p = 0$. Es decir, $\widehat{\beta}_0$ se puede interpretar como la estimaci√≥n $\widehat{Y}$ cuando [**TODAS las predictoras son nulas**]{.hl-purple}


. . .

* [**Pendientes**]{.hl-yellow}: denotadas como $\beta_j$, para $j=1,\ldots, p$, su valor real, cuantifica el incremento de $Y$ cuando solo $X_j$ aumenta una unidad. Es decir, $\widehat{\beta}_j$ se puede interpretar como la variaci√≥n de la estimaci√≥n $\widehat{Y}$ cuando [**$X_j$ tiene un incremento unitario y el RESTO DE PREDICTORAS permanecen fijas**]{.hl-purple}.

---

## Diagnosis

En el caso multivariante las [**4 hip√≥tesis**]{.hl-yellow} se convierten en

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es 

$$E \left[Y | \left( \boldsymbol{X}_1 = x_1, \ldots, \boldsymbol{X}_p = x_p \right) \right] = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$$

2. [**Homocedasticidad**]{.hl-green}: necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow} $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \left( \boldsymbol{X}_1 = x_1, \ldots, \boldsymbol{X}_p = x_p \right) \right] = cte < \infty$

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$


4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser en independientes, y en particular **incorrelados** ${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$

$$Y | \left( \boldsymbol{X}_1 = x_1, \ldots, \boldsymbol{X}_p = x_p \right) \sim N \left(\beta_0 + \beta_1x_1 + \ldots + \beta_p x_p, \sigma_{\varepsilon}^2 \right)$$


---

## Inferencia 

Ahora las hip√≥tesis nos permiten decir  que los [**par√°metros estimados siguen una distribuci√≥n normal multivariante**]{.hl-yellow} de [**media el vector de par√°metros**]{.hl-purple}  a estimar y de [**matriz de covarianzas la varianza residual por $\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}$**]{.hl-purple}

$$\widehat{\boldsymbol{\beta}} \sim N_{p+1}\left(\boldsymbol{\beta}, \sigma_{\varepsilon}^2 \left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}\right)$$

. . .

Esa normal multivariante, componente a componente, deriva en

$$\widehat{\beta}_j\sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad \frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)} \sim N \left(0, 1 \right), \quad \frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} \sim t_{n-p-1}$$

donde $SE \left(\widehat{\beta}_j\right)^2 = \sigma_{\varepsilon}^2 v_j$ y $\widehat{SE} \left(\widehat{\beta}_j\right)^2 = \widehat{\sigma}_{\varepsilon}^2 v_j$, con $v_j$ el elemento $j$-√©simo de la diagonal de $\left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}$ (matriz que representa la **variabilidad de predictores**)


---

## Caso pr√°ctico: wine.csv

Vamos a volver a usar nuestros datos `wine.csv`

```{r}
datos <- read_csv(file = "./datos/wine.csv")
datos
```

Pero esta vez no vamos a seleccionar ninguna variable previamente. Para [**ajustar un modelo multivariante**]{.hl-yellow} basta con a√±adir variables `+`

```{r}
ajuste_uni <- lm(data = datos, formula = Price ~ AGST)
ajuste_2 <- lm(data = datos, formula = Price ~ AGST + HarvestRain)
ajuste_full <- lm(data = datos, formula = Price ~ .)
```


---

## Caso pr√°ctico: wine.csv

F√≠jate que ahora la tabla `coefficients` tiene una [**l√≠nea por covariable**]{.hl-yellow} (m√°s $\beta_0$) y adem√°s en este caso dice `(1 not defined because of singularities)`: la [**matriz de covarianzas no es invertible**]{.hl-red} ya que el [**determinante es 0**]{.hl-red} (en este caso `Year` es "igual" que `Age`)

```{r}
ajuste_full |> summary()
```

---

## Caso pr√°ctico: wine.csv

Una vez eliminada, en la √∫ltima l√≠nea `F-statistic: ... p-value: 2.232e-07`: se est√° [**rechazando la hip√≥tesis nula**]{.hl-green} del contraste de significaci√≥n global (existe [**alguna predictora cuyo efecto lineal es significativo**]{.hl-green}). Recuerda que la nula es $H_0:~\beta_1 = \ldots = \beta_5 = 0$


```{r}
ajuste_full <- lm(data = datos |> select(-Age), formula = Price ~ .)
ajuste_full |> summary()
```

---

## Caso pr√°ctico: wine.csv

Si nos fijamos en la tabla de coeficientes el [**modelo ajustado**]{.hl-yellow} es

$$\begin{eqnarray}\widehat{Precio} &=& 2.496 - 0.0137 * Year + 0.0012 * WinterRain + 0.614 * AGST \nonumber \\ &-& 0.0038 * HarvestRain - 0.00002 * FrancePop \nonumber \end{eqnarray}$$

```{r}
ajuste_full |> summary()
```

---

## Caso pr√°ctico: wine.csv

$$\begin{eqnarray}\widehat{Precio} &=& 2.496 - 0.0137 * Year + 0.0012 * WinterRain + 0.614 * AGST \nonumber \\ &-& 0.0038 * HarvestRain - 0.00002 * FrancePop \nonumber \end{eqnarray}$$

La [**interpretaci√≥n**]{.hl_yellow} es

* La estimaci√≥n del modelo cuando todas las covariables son 0 (sin poblaci√≥n, sin lluvia ni temperatura, a√±o 0) es de 2.496 (escala log)

* Por ejemplo, si el resto de variables permaneciesen fijas, por cada litro de lluvia que caiga de m√°s durante la cosecha (agosto-septiembre), el modelo estima que el precio baja 3.8 (escala log)

---

## Caso pr√°ctico: wine.csv

Si nos fijamos en la tabla de coeficientes tenemos **2 predictoras cuyo efecto lineal no se acepta que sea significativo**: `Year`, `FrancePop` (adem√°s de $\beta_0$) ya que el contraste de significaci√≥n $H_0:~\beta_j = 0$ vs $H_1:~\beta_j \neq 0$ nos devuelve un $p-valor > \alpha$

```{r}
ajuste_full |> summary()
```

---

## Caso pr√°ctico: wine.csv

La pregunta que intentaremos resolver en futuras clases es:

[**¬øC√≥mo decidir que predictoras seleccionamos?**]{.hl-yellow} El problema si quitamos todas las no significativas de manera simult√°nea es que no sabemos qu√© **efectos puede haber entre las propias predictoras**

```{r}
ajuste_full |> summary()
```

---

## Caso pr√°ctico: wine.csv

F√≠jate que ahora en la [**fase de diagnosis**]{.hl-yellow} tenemos una **sexta gr√°fica a chequear**: una gr√°fica que nos calcula la conocida como `VIF` (Variance Inflaction), que nos [**cuantifica la colinealidad (efectos lineales) entre las predictoras**]{.hl-yellow}

```{r}
check_model(ajuste_full)
```

---

## ANOVA

Ahora `anova()` en realidad nos hace una [**ANOVA secuencial**]{.hl-yellow} (variable a variable)


```{r}
ajuste_full |> anova()
```

---

## ANOVA

Ahora `anova()` en realidad nos hace una [**ANOVA secuencial**]{.hl-yellow} (variable a variable)


|  | Df (grados) | Sum Sq | Mean Sq | F value | Pr(>F) |
|:---------:|:-----:|:------:|:------:|:------:|:------:|
| $x_1$  | 1  | $SSR_1$ | $\frac{SSR_1}{1}$ | $F-value = \frac{\frac{SSR_1}{1}}{\frac{SSE}{n-p-1}}$ | $p_1$
| ...  | ...  | ... | ... | $F-value = \frac{\frac{SSR}{p}}{\frac{SSE}{n-p-1}}$ | p-valor F test
| $x_p$  | 1  | $SSR_p$ | $\frac{SSR_p}{1}$ | $F-value = \frac{\frac{SSR_p}{1}}{\frac{SSE}{n-p-1}}$ | $p_p$
| Residuals  | n - p - 1 | SSE | $\widehat{\sigma}_{\varepsilon}^{2} = \frac{SSE}{n-p-1}$ | | |


---

## ANOVA


```{r}
ajuste_full |> anova()
```


Ahora $SSR_j$, con $j=1,\ldots,p$, representa la **suma de residuos al cuadrado** $SSR$ cuando incluimos la predictora j-√©sima $X_j$, frente a cuando no lo hacemos, tal que

$$SSR_j = SSR \left(X_1, \ldots, X_j \right) - SSR \left(X_1, \ldots, X_{j-1} \right)$$

As√≠ los p-valores $p_j$ son individuales (los mismos que los de la tabla de coeficientes).



# Clase 14: selecci√≥n de modelos {#clase-14}

[**Selecci√≥n secuencial de modelos**]{style="color:#444442;"}

---

## Bondad de ajuste

Como ya hemos hablado, el conocido como $R^2$ o [**bondad de ajuste**]{.hl-yellow} o el [**coeficiente de determinaci√≥n**]{.hl-yellow} es una forma sencilla, y relacionada con el ANOVA, para cuantificar la [**cantidad de informaci√≥n explicada por el modelo**]{.hl-yellow} definida como

$$R^2 = \text{Ratio info explicada} = \frac{SSR}{SST} =_{lineal} 1 - \frac{SSE}{SST} = 1 - \frac{s_{r}^{2}}{s_{y}^{2}} =  1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^{2}}{s_{y}^{2}}$$


Por definici√≥n, la [**bondad de ajuste est√° entre 0 y 1**]{.hl-yellow} pero adem√°s [**presenta un problema en el caso multivariante**]{.hl_red}


* [**M√°s predictoras**]{.hl-yellow} implica que [**$R^2$ crece**]{.hl-yellow} (al aumentar $p$), ¬°incluso aunque dichas [**predictoras no sean significativas**]{.hl-red}!

---
  
## R2 ajustado

Para evitar dicho problema vamos a definir la conocida como [**bondad de ajuste ajustada**]{.hl-yellow} 

$$R_{adj}^2 = 1 - \frac{\frac{SSE}{n-p-1}}{\frac{SST}{n-1}} = 1 - \frac{SSE}{SST}\frac{n-1}{n-p-1}  = 1 - \frac{\widehat{\sigma}_{\varepsilon}^{2}}{SST}(n-1)$$


. . .

As√≠ el $R_{adj}^2$ [**no depende del n√∫mero de predictoras de manera directa**]{.hl-green} (si sigue dependiendo del ruido, de manera que descender√° solo cuando incrementar $p$ implica reducir el error, es decir, variables con un efecto significativo). Se cumple adem√°s que que si $p=1$

$$\lim_{n\to \infty} R_{adj}^2 = \lim_{n\to \infty} \left(1 - \frac{\widehat{\sigma}_{\varepsilon}^{2}}{SST}(n-1) \right) = \lim_{n\to \infty} \left(1 - \frac{\widehat{\sigma}_{\varepsilon}^{2}}{SST}(n-p-1) \right) = R^2$$


---

## R2 ajustado

Para ver mejor su diferencia debes realizar el siguiente [**estudio de simulaci√≥n**]{.hl-yellow} (si quieres fija semilla `set.seed(12345)`):

. . .

1. Considera dos predictoras $X_1 \sim N(0, 1)$ y $X_2 \sim N(0, 1)$ de tama√±o muestral $n = 200$.  Considera el ruido como $\varepsilon \sim N(0, \sigma = 12)$

. . .

2. Simula el modelo $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \varepsilon_i$ con $\beta_0 = 0.01$, $\beta_1 = 1.5$ y $\beta_2 = -1.5$

. . .

3. Realiza el ajuste con todas las predictoras y calcula el $R^2$ y el $R_{adj}^2$ (lo necesitamos guardar, as√≠ usa la f√≥rmula no la salida del `lm()`)

. . .

4. Repite este proceso $M = 300$ veces (vuelve a simular las variables, vuelve a construir el modelo, y obt√©n de nuevo las bondades de ajuste, de manera que tengamos $300$ de cada uno)

---

## R2 ajustado


Lo que deber√≠a salirte si fijas `set.seed(12345)`

```{r}
#| echo: false
M <- 300
n <- 200
R2 <- R2_adj <- rep(NA, M)
p <- 2

for (i in 1:M) {
  
  x_1 <- rnorm(n)
  x_2 <- rnorm(n)
  eps <- rnorm(n, mean = 0, sd = 12)
  
  y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
  datos <- tibble("y" = y, "x_1" = x_1, "x_2" = x_2)
  ajuste <- lm(data = datos, y ~ .)
  
  R2[i] <- 1 - var(ajuste$residuals) / var(datos$y)
  R2_adj[i] <-
    1 - (var(ajuste$residuals) / var(datos$y)) * ((n - 1) / (n - p - 1))
}

library(ggridges)
ggplot(tibble("id" = 1:M, "R2" = R2, "R2_adj" = R2_adj) |> 
         pivot_longer(cols = -id, names_to = "R2_type",
                      values_to = "values")) +
  geom_density_ridges(aes(x = values, y = R2_type,
                          color = R2_type, fill = R2_type),
                      alpha = 0.5) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  coord_flip() +
  theme_minimal()
```


---

## R2 ajustado

Repite el estudio de simulaci√≥n  a√±adiendo 

1. Una nueva predictora "basura" $X_3 \sim N(0, 2)$ tal que $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + 10^{-10} X_{i3} + \varepsilon_i$.

2. Dos nuevas predictora "basura" $X_3 \sim N(0, 2)$ y $X_4 \sim N(0, 2)$ tal que $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + 10^{-10} X_{i3} + 10^{-10} X_{i4} + \varepsilon_i$.

...

3. 195 nuevas predictora "basura" $X_j \sim N(0, 2)$  tal que $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \displaystyle \sum_{j=3}^{195} 10^{-10} X_{ij} + \varepsilon_i$.


Ahora debes obtener $300*195$ valores de $R^2$ y $R_{adj}^2$ (300 simulaciones por cada nueva predictora que a√±adimos)

---

## R2 ajustado

![](img/predictoras-R2.png)


```{r}
#| echo: false
#| eval: false
M <- 300
n <- 200
p <- 195
R2 <- R2_adj <- matrix(NA, M, p)

set.seed(12345)
for (i in 1:M) {
  
  x_1 <- rnorm(n)
  x_2 <- rnorm(n)
  eps <- rnorm(n, mean = 0, sd = 12)
  y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
  
  X <- matrix(NA, n, p)
  for (j in 1:p) {
    
    X[, j] <- rnorm(n, mean = 0, sd = 2)
    y <- y + 1e-10 * X[, j]
    datos <- tibble("y" = y, "x_1" = x_1, "x_2" = x_2, X[, 1:j])
    ajuste <- lm(data = datos, y ~ .)
    
    R2[i, j] <- 1 - var(ajuste$residuals) / var(datos$y)
    R2_adj[i, j] <-
      1 - (var(ajuste$residuals) / var(datos$y)) *
      ((n - 1) / (n - (j + 2) - 1))
  }
}

mean_R2 <- R2 |> colMeans()
mean_R2_adj <- R2_adj |> colMeans()

gg <- ggplot()
for (i in 1:M) {
  gg <-
    gg +
    geom_line(data =
                 tibble("p" = 1:p, "R2" = R2[i, ],
                        "R2_adj" = R2_adj[i, ]) |>
                 pivot_longer(cols = -p, names_to = "R2_type",
                              values_to = "values"),
               aes(x = p, y = values, color = R2_type),
              linewidth = 0.01, alpha = 0.1) +
    geom_line(data = tibble("p" = 1:p, "R2" = mean_R2, "R2_adj" = mean_R2_adj) |>
                 pivot_longer(cols = -p, names_to = "R2_type",
                              values_to = "values"),
              aes(x = p, y = values, color = R2_type), linewidth = 1.5)
}
gg1 <- 
  gg +
  theme_minimal() +
  scale_y_continuous(limits = c(-0.5, 1.1)) +
  labs(x = "N√∫mero de predictoras basura")
```

---

## Multicolinealidad

Uno de los mayores problemas de los [**modelos lineales multivariantes**]{.hl-yellow} es el conocido como [**problema de colinealidad**]{.hl-red}

. . .

Imagina un modelo $Y = \beta_0 + \beta_1 X_1 + \beta_2X_2 + \beta_3 X_3$, donde $X_3$ es definida como (literal) la suma de $X_1$ y $X_2$.

```{r}
n <- 200
x_1 <- rnorm(n, sd = 2)
x_2 <- rnorm(n, sd = 2)
x_3 <- x_1 + x_2
eps <- rnorm(n)

y <- 0.5 - 2*x_1 + 3*x_2 -2.5*x_3
datos <- tibble(y, x_1, x_2, x_3)
ajuste <- lm(data = datos, formula = y ~ .)
```

---

## Multicolinealidad

Como ya vimos en el ejemplo de `wine.csv`, cuando tenemos [**dos o m√°s predictoras que dependen linealmente entre s√≠**]{.hl-yellow} la matriz $\left(X^{T} X \right)^{-1}$ no se puede invertir ya que es singular (determinante igual a 0), as√≠ que debe eliminar una de las ecuaciones para que el problema sea de rango completo

Esto se conoce como [**colinealidad exacta**]{.hl-yellow}

```{r}
ajuste |> summary()
```

---

## Multicolinealidad

La colinealidad exacta es solo el ejemplo m√°s extremo de lo que se conoce como [**problema de colinealidad**]{.hl-red}: un problema que aparece cuando varias predictoras est√°n **altamente correladas**. 
Un [**problema de colinealidad**]{.hl-yellow} tiene principalmente dos consecuencias:

. . .

* [**Reduce la precisi√≥n de los estimadores**]{.hl-red} ya que la matriz $X^{T} X$, seg√∫n aumenta la dependencia, tiene un determinante cada vez m√°s cercano a 0, por lo que $\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}$, que siempre depende de $1/det\left( \mathbf{X}^{T}\mathbf{X} \right)$, tendr√° valores cada vez m√°s grandes. Dado que $\widehat{\beta}_j\sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right)$, con $SE \left(\widehat{\beta}_j\right)^2 = \sigma_{\varepsilon}^2 v_j$ siendo $v_j$ el elemento $j$-√©simo de la diagonal de $\left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}$, implica que [**a mayor colinealidad, mayor varianza de las estimaciones**]{.hl_yellow}

. . .


* [**Distorsiona los efectos**]{.hl-red} de los predictores sobre la respuesta

---

## Multicolinealidad

¬øPero qu√© sucede cuando esa [**colinealidad no es tan obvia**]{.hl-yellow}? Veamos un ejemplo sencillo: simula el siguiente modelo (para $n = 100$)

$$Y = 1 + 0.5 X_1 + 2 X_2 - 3 X_3 - X_4 + \varepsilon, \quad \varepsilon \sim N(0, 1)$$


* $X_1 \sim N(0, 1)$, $X_2 = 0.5*X_1 + N(0, 1)$ y $X_3 = 0.5*X_2 + N(0, 1)$

* $X_4 = -X_1 + X_2 + N(0, 0.5)$

```{r}
#| code-fold: true
set.seed(12345)
n <- 100

x1 <- rnorm(n)
x2 <- 0.5 * x1 + rnorm(n)
x3 <- 0.5 * x2 + rnorm(n)
x4 <- -x1 + x2 + rnorm(n, sd = 0.5)
eps <- rnorm(100)

y <- 1 + 0.5 * x1 + 2 * x2 - 3 * x3 - x4 + eps
datos <- tibble(y, x1, x2, x3, x4)
```

¬øC√≥mo ser√°n sus **correlaciones (lineales)**?

---

## Multicolinealidad

A priori [**no se observa un problema obvio de colinealidad**]{.hl-yellow} ya que no hay dos predictoras altamente correladas.

```{r}
datos |> 
  correlate()
```

---

## Multicolinealidad

Por la definici√≥n realizada es obvio que hay una [**dependencia lineal entre todas las predictoras**]{.hl-yellow}, pero al ser una relaci√≥n lineal m√°s compleja (con ruido de por medio y distintas predictoras interactuando a la vez), una simple [**revisi√≥n de las correlaciones es necesaria pero no suficiente**]{.hl-yellow} ya que puede ocultar problemas de colinealidad que s√≠ existen.

```{r}
datos |> 
  cor() |> 
  corrplot(method = "color", addCoef.col = "#121212")
```


---

## Multicolinealidad

Necesitamos por tanto una forma de cuantificar dicha multicolinealidad, y para ello usaremos el conocido como [**factor de inflaci√≥n de la varianza (VIF)**]{.hl-yellow}

. . .

Para cada coeficiente $\beta_j$, y su estimador $\widehat{\beta}_j$, se define el VIF como

$$VIF\left(\widehat {\beta_j} \right) = \frac{1}{1 - R^2_{X_j | X_{-j}}}, \quad R_{j}^2:= R^2_{X_j | X_{-j}}$$

donde $R_{j}^2$ representa la [**bondad de ajuste $R^2$ cuando predecimos con una regresi√≥n lineal multivariante la predictora $X_j$**]{.hl-yellow} en funci√≥n del resto de predictoras (sin la variable objetivo)

$$X_j = \gamma_0 + \gamma_1 X_1 + \ldots + \gamma_{j-1} X_{j-1} + \gamma_{j+1} X_{j+1} + \ldots + \gamma_j X_j$$

---

## Multicolinealidad


$$VIF\left(\widehat {\beta_j} \right) = \frac{1}{1 - R^2_{X_j | X_{-j}}}, \quad R_{j}^2:= R^2_{X_j | X_{-j}}$$

La idea es [**cuantificar como de relacionado est√° cada predictor**]{.hl-yellow} respecto al resto (de manera conjunta, no dos a dos). Nota: **siempre es mayor que 1**

. . .


* Si $VIF\left(\widehat {\beta_j} \right)$ [**cercano a 1**]{.hl-yellow}, entonces $R_{j}^{2}$ cercano a 0 --> variabilidad del predictor $X_j$ no se puede explicar con la combinaci√≥n lineal de otras

. . .


* Si $VIF\left(\widehat {\beta_j} \right)$ [**mayor que 5**]{.hl-yellow}, entonces $1 - R_{j}^{2} < 0.2$, ergo $R_{j}^{2} > 0.8$ --> problema moderado de colinealidad

. . .


* Si adem√°s $VIF\left(\widehat {\beta_j} \right)$ [**mayor que 10**]{.hl-yellow}, entonces $R_{j}^{2} > 0.9$ --> problema grave de colinealidad 

---

## Multicolinealidad

Para calcular el $VIF$ podemos o bien hacer regresi√≥n con cada par√°metro, de manera manual, o bien hacer uso de `check_collinearity()` del paquete `{performance}`

```{r}
ajuste <- lm(data = datos, y ~ .)
performance::check_collinearity(ajuste)
```

---

## Multicolinealidad

Si te fijas $VIF \left(\widehat{\beta}_2 \right) = 10.41$, que es justamente el $1/(1-R^2)$ si hacemos la regresi√≥n $X_2$ vs el resto de predictoras

```{r}
check_collinearity(ajuste)$VIF
ajuste_X2 <- lm(data = datos |> select(-y), x2 ~ .)
ajuste_X2 |> summary()
```

---

## Multicolinealidad

El VIF podemos adem√°s [**visualizarlo en una sexta gr√°fica**]{.hl-model} cuando hacemos `check_model()`

```{r}
check_model(ajuste)
```

---

## Multicolinealidad

El VIF podemos adem√°s [**visualizarlo de manera manual**]{.hl-model}

```{r}
#| code-fold: true
VIF <- tibble("variable" = c("x1", "x2", "x3", "x4"),
              "VIF"= check_collinearity(ajuste)$VIF)
ggplot(VIF) +
  geom_col(aes(x = variable, y = VIF, fill = VIF),
           alpha = 0.75) +
  scale_fill_gradient2(low = "#1B8A54", mid = "#F7E865",
                       high = "#CE2424", midpoint = 5) +
  theme_minimal()
```


---

## Selecci√≥n de modelos

Por lo tanto parece que obvio que en un **modelo de regresi√≥n multivariante** vamos a tener que que [**seleccionar distintas variables**]{.hl-yellow} para solventar esos problemas de colinealidad

. . .

Llamaremos [**modelo saturado**]{.hl-yellow} al modelo con todas las $p$ predictoras, sin seleccionar.

. . .

Y como hemos visto, [**no podremos simplemente comparar 2 a 2 correlaciones o p-valores**]{.hl-red} ya que el efecto de una variable nos puede afectar en otras. ¬øQu√© hacer?


---

## Selecci√≥n de modelos

Un protocolo de actuaci√≥n habitual podr√≠a ser el siguiente:

1. Hacemos el [**ajuste del modelo saturado**]{.hl-yellow}

2. Calculamos el [**VIF de cada estimador**]{.hl-yellow}

3. Adoptamos un umbral (por ejemplo, $VIF > 10$), de manera que [**toda predictora que lo supere se elimina ya de antemano**]{.hl-yellow}

4. Volvemos a chequear el VIF. Si hay que eliminar, volver al paso 3.

5. Del resto de predictoras se hace una [**selecci√≥n m√°s fina**]{.hl-yellow}. Algunas opciones: BIC/AIC, regresi√≥n penalizada (LASSO, ridge, elastic net), PCA.

---

## Selecci√≥n de modelos

Lo que nos dice el VIF y lo visto hasta ahora es que, incluso aunque tengan efecto, [**a√±adir predictoras complejizando el modelo no es gratis**]{.hl-yellow}, ya que lo hacemos a costa de [**sobreajustar el modelo**]{.hl-yellow} (la varianza de los estimadores se incrementa considerablemente)

¬øCu√°l es el [**n√∫mero m√≠nimo/m√°ximo de predictoras**]{.hl-yellow} que podremos incluir?

. . .

* El n√∫mero m√≠nimo de predictoras ser√° $p = 1$
* El n√∫mero m√°ximo ser√° $p = n-2$, o dicho de otra forma, necesito al menos $n \geq p+2$ observaciones. Piensa en $p=1$: si quiero una recta, necesito al menos 2 puntos para que la recta existe y al menos 3 para poder calcular la varianza residual estimada (acu√©rdate que se divide entre $n-p-1$)

. . .

De manera resumida: dado un modelo con $p$ predictores, necesitaremos estimar [**$p+2$ inc√≥gnitas**]{.hl-yellow} ($p+1$ coeficientes y la varianza residual).

---

## BIC y AIC

[**¬øC√≥mo seleccionar los predictores m√°s adecuados para el CONJUNTO del ajuste?**]{.hl-yellow}

. . .

Los m√©todos m√°s conocidos son los conocidos como [**selecci√≥n de modelos stepwise (paso-a-paso)**]{.hl-yellow}, que de manera **iterativa**, va incluyendo y descartando distintos predictores, y comparando su calidad, para decidir que **combinaci√≥n de par√°metros** es la m√°s √≥ptima.

. . .

La idea es [**seleccionar el modelo m√°s √≥ptimo**]{.hl-yellow} en funci√≥n de un [**criterio de informaci√≥n**]{.hl-yellow} que combina la calidad del modelo con el n√∫mero de predictoras empleadas: vamos a [**penalizar el uso de variables que no mejore suficiente el modelo**]{.hl-yellow}

---


## BIC y AIC

Los dos criterios de informaci√≥n m√°s famosos son el [**Bayesian Information Criterion (BIC)**]{.hl-yellow} y el [**Akaike Information Criterion (AIC)**]{.hl-yellow} definidos ambos como

$$AIC/BIC_{modelo} = -Calidad + \underbrace{\text{npar(modelo)} * \text{penalizaci√≥n}}_{\text{Complejidad}}$$

* $\text{npar(modelo)}$ es el [**n√∫mero de par√°metros a estimar del modelo**]{.hl-yellow}, que en el caso que nos ocupa es $\text{npar(modelo)} = p+2$ (coeficientes + var residual)

* $\text{Calidad}$ una medida que nos diga como de bueno es nuestro ajuste: al tener signo negativo, buscamos el [**menor valor de AIC/BIC**]{.hl-yellow}

* $\text{penalizaci√≥n}$: cuando sube $p$, entonces suba el $AIC/BIC$

El objetivo ser√° probar [**distintos modelos y quedarnos con el que tenga AIC/BIC m√°s peque√±o**]{.hl-yellow}

---

## BIC y AIC

Es habitual que la [**calidad del modelo**]{.hl-yellow} venga cuantificada por $2\ell(\text{modelo})$, donde $\ell(\text{modelo})$ es lo que se conoce como [**log-verosimilitud del modelo**]{.hl-yellow} 

$$\ell(\text{modelo}) = \log \left(P \left(\text{datos} |  \left(\text{par√°metros}, \text{modelo} \right) \right) \right)$$

es decir, suponiendo que el **modelo fuese correcto** y los **par√°metros valiesen la estimaci√≥n obtenida**, ¬øc√≥mo de [**probable es que mis datos hayan sido los que han sido**]{.hl-yellow}?

. . .

As√≠ nuestros criterios quedan como

$$AIC/BIC_{modelo} = -2\ell(\text{modelo}) + \underbrace{(p+2) * \text{penalizaci√≥n}}_{\text{Complejidad}}$$

---

## BIC y AIC

La diferencia entre ambos est√° en la **penalizaci√≥n usada**


$$BIC(modelo_p) = -2\ell(\text{modelo}) + \underbrace{(p+2) * \log(n)}_{\text{Complejidad}}$$

$$AIC (modelo_p) = -2\ell(\text{modelo}) + \underbrace{(p+2) * 2}_{\text{Complejidad}}$$

El [**criterio BIC es m√°s agresivo**]{.hl-yellow} seleccionando variables ya que, si $n$ crece, $\log(n) >> 2$ (penaliza m√°s el sobreajuste) y, adem√°s, [**depende del tama√±o muestral**]{.hl-yellow}

---

## BIC y AIC

Vamos a ver un **peque√±o ejemplo en R** con `wine.csv`, aplicando un modelo saturado (habiendo hecho un filtro inicial por VIF), un modelo solo con `AGST`,  otro con `AGST + HarvestRain` y otro con `AGST + HarvestRain + Year`, y calcular su BIC/AIC con las funciones `BIC()` y `AIC()`

```{r}
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, formula = Price ~ .)
ajuste_1 <- lm(data = datos, formula = Price ~ AGST)
ajuste_2 <- lm(data = datos, formula = Price ~ AGST + HarvestRain)
ajuste_3 <- lm(data = datos, formula = Price ~ AGST + HarvestRain + Age)
```

:::: columns
::: {.column width="50%"}
```{r}
BIC(ajuste_saturado)
BIC(ajuste_1)
```
:::

::: {.column width="50%"}
```{r}
BIC(ajuste_2)
BIC(ajuste_3)
```
:::
::::

---

## BIC y AIC

Vamos a ver un **peque√±o ejemplo en R** con `wine.csv`, aplicando un modelo saturado (habiendo hecho un filtro inicial por VIF), un modelo solo con `AGST`,  otro con `AGST + HarvestRain` y otro con `AGST + HarvestRain + Year`, y calcular su BIC/AIC con las funciones `BIC()` y `AIC()`

:::: columns
::: {.column width="50%"}
```{r}
AIC(ajuste_saturado)
AIC(ajuste_1)
```
:::

::: {.column width="50%"}
```{r}
AIC(ajuste_2)
AIC(ajuste_3)
```
:::
::::

---

## BIC y AIC

Podemos usar `compare_performance()`  ($AICc = AIC +\frac{2(p+2)^{2} + 2(p+2)}{n-(p+2)-1}$ es una versi√≥n corregida para tama√±os muestrales peque√±os)

```{r}
performance::compare_performance(ajuste_saturado, ajuste_1, ajuste_2, ajuste_3)
```

. . .

En ambos criterios la conclusi√≥n es que de los 4 modelos, el [**"mejor" es el 3¬∫ ya que AIC/BIC m√°s bajos**]{.hl-yellow} aunque tenga un $R_{adj}^2$ menor: la mejora que produce meter todas las variables no es suficiente para lo que se complica el modelo.

---

## BIC y AIC

En realidad lo correcto ser√≠a antes chequear si [**podemos eliminar alguna variable**]{.hl-yellow} ya de manera preliminar usando el VIF

```{r}
check_collinearity(ajuste_saturado)
```

---

## BIC y AIC

Seg√∫n el VIF debemos ya eliminar de antemano `Year` y `FrancePop` as√≠ que lo hacemos y volvemos calcularlo para las restantes

```{r}
datos_VIF <- datos |> select(-Year, -FrancePop)
ajuste_saturado <- lm(data = datos_VIF, formula = Price ~ .)
check_collinearity(ajuste_saturado)
```

---

## BIC y AIC

Tras este primer **filtro grosero** volvemos a implementar los 3 modelos

```{r}
ajuste_saturado <- lm(data = datos_VIF, formula = Price ~ .)
ajuste_1 <- lm(data = datos_VIF, formula = Price ~ AGST)
ajuste_2 <- lm(data = datos_VIF, formula = Price ~ AGST + HarvestRain)
ajuste_3 <- lm(data = datos_VIF, formula = Price ~ AGST + HarvestRain + Age)
compare_performance(ajuste_saturado, ajuste_1, ajuste_2, ajuste_3)
```

Ahora **nos indica que el mejor modelo es el saturado** (pero que ya no tiene 6 variables sino 4): la mejora de bondad de ajuste compensa por incrementar una sola variable (entre ajuste3 y saturado)

---

## BIC y AIC

F√≠jate que adem√°s al haber quitado dos variables con alta dependenica lineal del resto las [**hip√≥tesis se cumplen**]{.hl-yellow} (antes no)

```{r}
check_model(ajuste_saturado)
```

---

## BIC y AIC

Vamos a repetirlo con un ejemplo simulado $Y = 0.01 + 1.5*X_1 -1.5X_2 + \varepsilon$, donde $X_1,X_2 \sim N(0,1)$ y $\varepsilon \sim N(0, 2)$, y al que vamos a√±adiendo predictoras basura $X_{2+j} \sim N(0, 2)$ con $\beta_{2+j} = 10^{-8}$, probando cuando $j=1$, $j = 5$, $j = 10$, $j=25$, $j=50$ y $j = 100$. El mejor modelo es el que solo tiene las **dos predictoras con un efecto real** sobre y.

```{r}
#| code-fold: true
n <- 200
p <- c(1, 5, 10, 25, 50, 100)

set.seed(12345)
x_1 <- rnorm(n)
x_2 <- rnorm(n)
eps <- rnorm(n, mean = 0, sd = 2)
y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
ajuste_0 <- lm(data = tibble("y" = y, "x_1" = x_1, "x_2" = x_2), y ~ .)

X <- matrix(rnorm(n * max(p), mean = 0, sd = 2), nrow = n)
ajuste <- list()
for (j in 1:length(p)) {
  
  ynew <- y + 1e-8 * sum(X[, 1:p[j]])
  datos <- tibble("y" = ynew, "x_1" = x_1, "x_2" = x_2, X[, 1:p[j]])
  ajuste[[j]] <- lm(data = datos, y ~ .)
}
compare_performance(ajuste_0, ajuste[[1]], ajuste[[2]],
                    ajuste[[3]], ajuste[[4]], ajuste[[5]])
```



---


## Sobreajuste

Algo importante a tener en cuenta es que, aunque ambos criterios nos ayudan a seleccionar modelos, ambos [**funcionan de manera aceptable bajo la hip√≥tesis**]{.hl-yellow} de que $n >> p +2$: en caso contrario, si $n$ se acerca a $p +2$, el sobreajuste seguir√° produci√©ndose

. . .

Dado que la penalizaci√≥n es m√°s grande en el BIC, el [**criterio BIC nos garantiza una m√°s temprana detecci√≥n del sobreajuste**]{.hl-yellow}

---

## Sobreajuste

Lo anterior se puede ilustrar calculando BIC e AIC del anterior estudio de simulaci√≥n de $R^2$ vs $R_{adj}^2$: f√≠jate como el [**BIC tarda m√°s en bajar**]{.hl-yellow}, tal que $BIC(p = 195) > BIC(p=2)$ (nos har√≠a quedarnos con el modelo sin sobreajustar) mientras que $AIC(p = 156) < AIC(p=2)$ (y para todos los que van detr√°s), por lo que acabar√≠amos eligiendo el modelo m√°s sobreajustado.

![](img/BIC-AIC.png)

```{r}
#| echo: false
#| eval: false
M <- 300
n <- 200
p <- 195
BIC_values <- AIC_values <- matrix(NA, M, p)

set.seed(12345)
for (i in 1:M) {
  
  x_1 <- rnorm(n)
  x_2 <- rnorm(n)
  eps <- rnorm(n, mean = 0, sd = 12)
  y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
  
  X <- matrix(NA, n, p)
  for (j in 1:p) {
    
    X[, j] <- rnorm(n, mean = 0, sd = 2)
    y <- y + 1e-10 * X[, j]
    datos <- tibble("y" = y, "x_1" = x_1, "x_2" = x_2, X[, 1:j])
    ajuste <- lm(data = datos, y ~ .)
    
    BIC_values[i, j] <- BIC(ajuste)
    AIC_values[i, j] <- AIC(ajuste)
  }
}

mean_BIC <- BIC_values |> colMeans()
mean_AIC <- AIC_values |> colMeans()

gg <- ggplot()
for (i in 1:M) {
  gg <-
    gg +
    geom_line(data =
                 tibble("p" = 1:p, "BIC" = BIC_values[i, ],
                        "AIC" = AIC_values[i, ]) |>
                 pivot_longer(cols = -p, names_to = "criterio",
                              values_to = "values"),
               aes(x = p, y = values, color = criterio),
              linewidth = 0.01, alpha = 0.1) +
    geom_line(data = tibble("p" = 1:p, "BIC" = mean_BIC,
                            "AIC" = mean_AIC) |>
                 pivot_longer(cols = -p, names_to = "criterio",
                              values_to = "values"),
              aes(x = p, y = values, color = criterio), linewidth = 1.5)
}
gg1 <- 
  gg +
  theme_minimal() +
  labs(x = "N√∫mero de predictoras basura")
```

---

## Consistencia

Otra [**enorme ventaja del BIC**]{.hl-yellow} es que se ha demostrado matem√°ticamente que es [**consistente**]{.hl-yellow}: si el tama√±o muestral fuese lo suficientemente grande, el [**BIC garantiza elegir el modelo correcto que genera los datos**]{.hl-green}. Matem√°ticamente se cumple que, si tenemos una serie de modelos $M_1, \ldots, M_m$, y el modelo real que genera los datos $M_0$ (que pretendemos estimar), entonces

$$P\left[\arg\min_{k=0,\ldots,m}\text{BIC}(\widehat{M}_k)=0\right]\to 1, \quad  n \to \infty$$

Esto solo suponiendo que el modelo subyacente sea lineal claro... Esto [**no sucede con el AIC**]{.hl-red} (para vuestro yo del futuro: en <https://doi.org/10.2307/2290328.> se prueba que el AIC es equivalente a usar validaci√≥n cruzada leave-one-out, el cual es inconsistente)

---

## stepAIC

Y aqu√≠ nos puede nacer una duda: si tengo muchos predictores, [**¬øtengo que calcular el BIC/AIC de todas las combinaciones posibles?**]{.hl-yellow}

. . .

S√≠, pero lo har√° por nosotros `MASS::stepAIC()`, donde en el par√°metro `k = ...` le indicamos la penalizaci√≥n (si `k = 2` es AIC y si `k = log(n)` es el BIC)

```{r}
#| eval: false
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, Price ~ .)
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```

---

## stepAIC

```{r}
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, Price ~ .)
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```


---

## stepAIC

Lo que obtendremos ser√° una [**selecci√≥n secuencial de modelos**]{.hl-yellow}, de manera que el ir√° probando las combinaciones, nos **muestra el ajuste y el valor del BIC/AIC** y se parar√° cuando el AIC/BIC no mejore.

```{r}
#| eval: false
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, Price ~ .)
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```


---

## stepAIC

```{r}
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "forward")
```

El argumento `direction = ...` puede tomar los valores `"both"` (por defecto), `"forward"` y `"backward"` que nos determina la [**direcci√≥n de b√∫squeda**]{.hl-yellow}:


* `direction = "forward"`: empieza con el modelo proporcionado y va [**a√±adiendo**]{.hl-yellow} predictoras haciendo modelos cada vez m√°s complejos.

---

## stepAIC

```{r}
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "forward")
```


F√≠jate que ahora  como le hemos dado el modelo saturado y `direction = "forward"`, entonces no hace nada. Para controlar esto podemos incluir la variable `scope = list(lower = mod_easy, upper = mod_complex)`, donde **podemos pasarle dos modelos**p: los modelos m√°s complejos y m√°s sencillo **pentre los que queremos que se mueva en la b√∫squeda**.

---

## stepAIC

```{r}
#| eval: false
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "backward")
```

* `direction = "backward"`: empieza con el modelo proporcionado y va [**eliminando**]{.hl-yellow} predictoras haciendo modelos cada vez m√°s sencillos.


```{r}
#| eval: false
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```

* `direction = "both"`: empieza con el modelo proporcionado y va [**a√±adiendo y eliminando**]{.hl-yellow} predictoras seg√∫n sea m√°s conveniente (pudiendo a√±adir/eliminar una variable que previamente e hab√≠a eliminado/a√±adido)




---

## stepAIC

```{r}
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "forward")
```

[**Aviso**]{.hl-red}: en realidad `MASS::stepAIC()` no calcula exactamente el mismo BIC/AIC que las funciones `BIC()` e `AIC()`, sino que les suma una constante $n(\log(2\pi) + 1) + \log(n)$ para BIC y $n(\log(2\pi) + 1) + 2$ para AIC

Dado que es una constante nos da igual para comparar modelos, pero hace que [**NO podamos comparar salidas**]{.hl-red} de `BIC()` y `AIC()` con salidas de `MASS::stepAIC()`.



# Clase 15: casos pr√°cticos {#clase-15}

[**Casos pr√°cticos: datos de viviendas de Boston y seatpos dataset**]{style="color:#444442;"}

---

## Casos pr√°cticos

Realiza todo el ajuste completo multivariante con los datasets:

* `seatpos` del paquete `{faraway}`: datos de 38 conductores donde el objetivo es predecir `hipcenter`, la posici√≥n del asiento del conductor, en funci√≥n de distintas variables (ver `? faraway::seatpos`)

* `Boston` del paquete `{MASS}`:  datos de 560 suburbios de Boston, en el que se han medido 14 variables en cada uno, con el objetivo de predecir `medv` el precio mediano de inmuebles (en millones de dolares), en funci√≥n de variables estructurales (`rm` y `age`), variables de vecindario (`crim`, `zn`, `indus`, `chas`, `tax`, `ptratio`, `black` y `lstat`), variables de accesibilidad (`dis` y `rad`) y variables de calidad del aire (`nox`)


# Clase 16: variables cualitativas {#clase-16}

[**¬øC√≥mo introducir predictoras cualitativas?**]{style="color:#444442;"}

---

## ...

# Clase 17: depuraci√≥n {#clase-17}

[**...**]{style="color:#444442;"}

---

## Ausentes

---

## Outliers

---

## ...

# Clase 18: ... {#clase-18}

[**...**]{style="color:#444442;"}

---

## ..


# El mundo Github

[**Trabajar ordenados, publicar resultados, replicabilidad de lo realizado**]{style="color:#444442;"}

---

## ¬øQu√© es Github?

[**GitHub**]{.hl-yellow} es la plataforma colaborativa m√°s conocida basada en el [**sistema de control de versiones Git**]{.hl-yellow}

. . .

-   [**¬øQu√© es Git?**]{.hl-purple} Git es un sistema de [**control de versiones**]{.hl-yellow}: una especie de [**Dropbox**]{.hl-yellow} para facilitar la [**programaci√≥n colaborativa**]{.hl-yellow} entre un grupo de personas, permitiendo llevar la [**trazabilidad de los cambios**]{.hl-yellow} realizados.

. . .

-   [**¬øQu√© es Github?**]{.hl-purple} Nuestra [**plataforma/interfaz**]{.hl-yellow} para ejecutar el control de versiones: nos servir√° no solo para trabajar colaborativamente sino para [**hacer transparente**]{.hl-yellow} el proceso de construcci√≥n de nuestros proyectos de c√≥digo.

. . .

::: callout-important
## Importante

Desde el 4 de junio de 2018 Github es de Microsoft (ergo el c√≥digo que subas tambi√©n)
:::

---

## Visi√≥n general

Tras hacernos una cuenta en Github, [**arriba a la derecha**]{.hl-purple} tendremos un c√≠rculo, y haciendo click en [**Your Profile**]{.hl-purple}, veremos algo similar a esto

::: columns
::: {.column width="55%"}
![](img/github_1.png)
:::

::: {.column width="45%"}
-   [**Edit profile**]{.hl-purple}: nos permite a√±adir una [**descripci√≥n y foto de perfil**]{.hl-yellow}.

-   [**Overview**]{.hl-purple}: en ese panel de cuadrados se [**visualizar√° nuestra actividad**]{.hl-yellow} a lo largo del tiempo.

-   [**Repositories**]{.hl-purple}: el c√≥dugo ser√° subido a [**repositorios**]{.hl-yellow}, el equivalente a nuestras carpetas compartidas en Dropbox.
:::
:::

---

## Primer uso: consumidor

Antes de aprender como crear repositorios, Github tambi√©n nos servir√° para

-   [**Acceder a c√≥digo**]{.hl-purple} ajeno
-   [**Proponer mejoras**]{.hl-purple} a otros usuarios, e incluso proponer [**correcciones de error que detectemos**]{.hl-yellow} de software que usemos

. . .

-   [**Instalar paquetes de R**]{.hl-purple}. En muchas ocasiones los desarrolladores de paquetes suben las actualizaciones a CRAN cada cierto tiempo, y en otras el software no es suficientemente ¬´amplio¬ª para poder ser subido como paquete.

El c√≥digo de paquetes que no tengamos subido en CRAN podremos [**instalarlo como c√≥digo desde Github**]{.hl-yellow}

---

## Instalar desde Github

Por ejemplo, vamos a instalar un paquete llamado [`{peRReo}`](https://github.com/jbgb13/peRReo), cuya √∫nica funci√≥n es darnos [**paletas de colores**]{.hl-yellow} basadas en portadas de [**√°lbumes de m√∫sica urbana**]{.hl-yellow}

::: columns
::: {.column width="50%"}
![](img/perrreo.png)
:::

::: {.column width="50%"}
![](img/rosalia.png){width="360"}
:::
:::

. . .

Para ello antes tendremos que instalar un [**conjunto de paquetes para desarrolladores**]{.hl-yellow} llamado `{devtools}`, que nos permitir√° la instalaci√≥n desde Github

```{r}
#| eval: false
install.packages("devtools")
```

---

## Instalar desde Github

Las [**instrucciones de instalaci√≥n**]{.hl-yellow} suelen venir detalladas en la portada del repositorio

::: columns
::: {.column width="50%"}
![](img/install_perreo.png)
:::

::: {.column width="50%"}
![](img/rosalia.png){width="310"}
:::
:::

. . .

En la mayor√≠a de casos bastar√° con la funci√≥n `install_github()` (del paquete que acabamos de instalar), pas√°ndole como argumento la [**ruta del repositorio**]{.hl-yellow} (sin "github.com/").

```{r}
#| eval: false
devtools::install_github("jbgb13/peRReo")
```

Ya puedes perrear con ggplot ;)

---

## Descargar desde Github

La mayor√≠a de veces lo que subamos no ser√° un paquete de R como tal sino que [**subiremos un c√≥digo m√°s o menos organizado**]{.hl-yellow} y comentado. En ese caso podremos [**descargar el repo entero**]{.hl-yellow} haciendo click [**Code**]{.hl-green} y luego Download ZIP.

Por ejemplo, vamos a descargarnos los scripts de dataviz que han subido desde el [Centre d'Estudis d'Opini√≥](https://github.com/ceopinio/bop-grafics)

![](img/ceo_github.png)

---

## Ideal

![](img/abogados_simpson.jpeg){width="600"}

[**¬øLo ideal en caso de RTVE?**]{.hl-purple} Tener dos tipos de repositorios

-   Una [**colecci√≥n de repositorios p√∫blicos (producci√≥n)**]{.hl-yellow} donde hacer transparente el c√≥digo y los datos ([**ya validados**]{.hl-purple}), coordinado por un n¬∫ reducido de personas.

-   Una [**colecci√≥n de repositorios privados (desarrollo)**]{.hl-yellow} donde est√© todo el equipo colaborando y donde se haga el [**trabajo del d√≠a**]{.hl-purple}, con trazabilidad interna.

---

## Nuestro primer repositorio

Vamos a [**crear nuestro primero repositorio**]{.hl-yellow} que servir√° adem√°s como [**carta de presentaci√≥n**]{.hl-yellow} de nuestro perfil en Github.

1.  [**Repositories**]{.hl-purple}: hacemos click en las pesta√±a de Repositories.

2.  [**New**]{.hl-purple}: hacemos click en el [**bot√≥n verde New**]{.hl-green} para crear un nuevo repositorio

![](img/new_repo.png)

---

## Nuestro primer repositorio

-   [**Repository name**]{.hl-purple}: el [**nombre del repositorio**]{.hl-yellow}. En este caso vamos a crear un repositorio muy concreto: el nombre debe [**coincidir exactamente con tu nombre de usuario**]{.hl-yellow}

-   [**Description**]{.hl-purple}: descripci√≥n de tu repositorio. En este caso ser√° un repo de presentaci√≥n.

![](img/repo_init_1.png)

---

## Nuestro primer repositorio

-   [**Public vs private**]{.hl-purple}: con cada repositorio tendremos la opci√≥n de hacer el repositorio

    -   [**p√∫blico**]{.hl-purple}: todos los usuarios podr√°n ver el c√≥digo as√≠ c√≥mo la trazabilidad de su desarrollo (qu√© se a√±ade y cu√°ndo). Es para m√≠ la opci√≥n m√°s recomendable cuando quieres darle [**visibilidad y transparencia**]{.hl-yellow} a tu trabajo
    -   [**privado**]{.hl-purple}: solo tendr√°n acceso al repositorio aquellos usuarios a los que se lo permitas. No se podr√° visualizar ni instalar nada de √©l fuera de Github.

![](img/repo_init_2.png)

En este caso concreto, dado que ser√° un repositorio de presentaci√≥n, lo [**haremos p√∫blico**]{.hl-yellow}.

---

## Nuestro primer repositorio

-   [**Add a README file**]{.hl-purple}: un README file ser√° el archivo donde incluiremos las [**instrucciones y detalles de uso**]{.hl-yellow} a los dem√°s (en el caso de `{peRReo}` era el archivo que conten√≠a los detalles de instalaci√≥n)

![](img/repo_init_3.png)

De momento [**ignoraremos los dem√°s campos**]{.hl-red} para este primer repositorio.

---

## Nuestro primer repositorio

![](img/repo_init_1.png)

Por defecto Github asume que este repositorio, con el [**mismo nombre que nuestro usuario**]{.hl-yellow} ser√° el repositorio que querremos que se presente de inicio cuando alguien entra en nuestro perfil, y ser√° el repositorio donde \[**incluir en el README.md**\] una presentaci√≥n de nosotros y un √≠ndice de tu trabajo (si quieres).

---

## Nuestro primer repositorio

![](img/profile_github_md.png)

F√≠jate que ahora en nuestra [**portada tenemos dicho README.md**]{.hl-yellow} que podemos personalizar a nuestro gusto haciendo uso de [**html y markdown**]{.hl-yellow}.

Aqu√≠ puedes ver [**algunos ejemplos de README.MD**](https://github.com/matiassingers/awesome-readme)

---

## Repo de c√≥digo

Una vez que tenemos nuestro README de presentaci√≥n (recuerda que puedes [**personalizar a tu gusto con html y markdown**]{.hl-yellow}) vamos a crear un [**repositorio de c√≥digo**]{.hl-yellow}.

. . .

Si ya era importante [**trabajar con proyectos**]{.hl-yellow} en `RStudio`, cuando lo combinamos con Github es a√∫n m√°s crucial que creemos un proyecto antes de subir el c√≥digo, as√≠ que vamos a crear uno de prueba que se llame `repo-github-1`.

. . .

En dicho proyecto vamos a [**crear un script**]{.hl-yellow} (en mi caso llamado **codigo.R**) en el que deber√°s hacer los siguientes pasos:

---

## Repo de c√≥digo

1.  [**Carga**]{.hl-yellow} directamente desde la p√°gina del [ISCIII](https://cnecovid.isciii.es/covid19/resources) el archivo llamado `casos_hosp_uci_def_sexo_edad_provres.csv`

```{r}
#| eval: false
#| code-fold: true
# Carga de datos desde ISCIII
datos_covid <- read_csv(file = "https://cnecovid.isciii.es/covid19/resources/casos_hosp_uci_def_sexo_edad_provres.csv")
```

. . .

2.  [**Filtra**]{.hl-yellow} datos de Madrid (`"M"`), de 2020 y con sexo conocido (hombre/mujer). Tras ello qu√©date con las columnas `fecha`, `sexo`, `grupo_edad`, `num_casos` (ese orden). Por √∫ltimo obt√©n la suma de casos diarios por fecha y sexo.

```{r}
#| eval: false
#| code-fold: true
# Depuraci√≥n
datos_madrid <-
  datos_covid |>
  # Filtrado por Madrid y fecha
  filter(provincia_iso == "M" & fecha <= "2020-12-31" & sexo != "NC") |> 
  # Selecci√≥n de columnas
  select(provincia_iso:fecha, num_casos) |> 
  # Resumen de casos diarios por fecha y sexo
  summarise(num_casos = sum(num_casos), .by = c(fecha, sexo))
  
```

---

## Repo de c√≥digo

3.  [**Exporta el dataset a un csv**]{.hl-yellow} en una carpeta que se llame `exportado`

```{r}
#| eval: false
#| code-fold: true
# Exportamos datos
write_csv(datos_madrid, file = "./exportado/datos_madrid.csv")
```

. . .

::: columns
::: {.column width="40%"}
4.  Crea una [**gr√°fica de l√≠neas**]{.hl-yellow} que tenga en el eje x fecha, en el eje y casos, con una curva por sexo (gr√°ficas separadas).

```{r}
#| eval: false
#| code-fold: true
# Gr√°fica
ggplot(datos_madrid) +
  geom_line(aes(x = fecha, y = num_casos, color = sexo),
            alpha = 0.6, linewidth = 0.7) +
  scale_color_manual(values = c("#85519D", "#278862")) +
  facet_wrap(~sexo) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
:::

::: {.column width="60%"}
![](./img/ggplot.png){width="380"}
:::
:::

. . .

5.  Tras ello [**exportamos la gr√°fica**]{.hl-yellow}

```{r}
#| eval: false
ggsave(filename = "./exportado/ggplot.png", plot = last_plot(),
       bg = "white", width = 12, height = 8)
```

---

## Repo de c√≥digo

¬øC√≥mo [**subimos el proyecto**]{.hl-yellow}? Vamos de nuevo a [**crear un proyecto de cero**]{.hl-yellow}. Antes no hemos hablado de dos campos importantes:

::: columns
::: {.column width="50%"}
![](./img/repo-1.png){width="460"}
:::

::: {.column width="50%"}
-   `Add .gitignore` nos permitir√° [**seleccionar el lenguaje**]{.hl-yellow} en el que estar√° nuestro proyecto para que Github lo entienda al sincronizar (y no actualice cosas que no deba).

-   `Choose a license` nos permitir√° [**seleccionar la licencia**]{.hl-yellow} que determinar√° las condiciones en las que otros podr√°n reusar tu c√≥digo.
:::
:::

---

## Repo de c√≥digo

::: columns
::: {.column width="50%"}
![](./img/repo-1-1.png)
:::

::: {.column width="50%"}
Si te fijas traer crearlo tenemos solo 3 archivos: el de licencia, el .gitignore y el readme.md (donde deber√≠amos escribir una gu√≠a de uso de lo que hayamos subido)
:::
:::

Para subir los archivos vamos a clickar en [**Add file \< Upload File**]{.hl-purple} y [**arrastraremos TODOS los archivos**]{.hl-yellow} de la carpeta de nuestro proyecto.

---

## Repo de c√≥digo

Tras la subida de archivos tendremos un cuadro llamado [**Commit changes**]{.hl-purple}

![](./img/commit-1.png)

Un [**commit**]{.hl-purple} es una [**modificaci√≥n del repositorio**]{.hl-yellow} con algo que se a√±ade/elimine/modifique, y dicho cuadro es recomendable usarlo para [**resumir en qu√© consiste la modificaci√≥n**]{.hl-yellow}, de manera que quede trazado el cambio.

---

## Repo de c√≥digo

Haciendo click en el reloj donde indica el [**n√∫mero de commits**]{.hl-yellow} accedemos al [**hist√≥rico de commits (cambios)**]{.hl-yellow} con hora, d√≠a, autor, comentarios, etc.

![](./img/commit-reloj.png)

---

## Repo de c√≥digo

Vamos a realizar un [**cambio en nuestro c√≥digo**]{.hl-yellow}: en tu c√≥digo local (local --\> tu ordenador), en lugar de filtrar por Madrid haz el [**filtro por Barcelona**]{.hl-yellow}, guarda el c√≥digo y sube en el repositorio el nuevo archivo (con el mismo nombre, Github har√° la sobrescritura)

```{r}
#| eval: false
#| code-line-numbers: "3"
datos_bcn <-
  datos_covid |>
  filter(provincia_iso == "B" & fecha <= "2020-12-31" & sexo != "NC") |> 
  select(fecha, sexo, grupo_edad, num_casos) |> 
  summarise(num_casos = sum(num_casos), .by = c(fecha, sexo))
```

---

## Consulta de commits

![](./img/barcelona_covid.png){width="550"}

::: columns
::: {.column width="40%"}
Si ahora [**consultamos el commit**]{.hl-yellow}, al lado hay un n√∫mero que lo identifica, y clickando en √©l nos resume los cambios: no solo [**almacena todas las versiones pasadas**]{.hl-yellow} sino que adem√°s nos [**muestra las diferencias entre los archivos cambiados**]{.hl-yellow}
:::

::: {.column width="60%"}
![](./img/commit_cambio.png)
:::
:::

---

## Trazabilidad de cambios

::: columns
::: {.column width="72%"}
![](./img/commit_split.png) ![](./img/commit_unified.png)
:::

::: {.column width="28%"}
Tenemos [**dos modos de visualizaci√≥n**]{.hl-yellow} de los cambios: el modo split nos muestra el antiguo y el nuevo, con las inclusiones en verde y lo que ya no est√° en rojo; y el modo unified nos muestra todo en un mismo documento.
:::
:::

---

## Recuperaci√≥n de commits

![](./img/browse-repo.png)

Github nos permite incluso [**recuperar una versi√≥n del pasado**]{.hl-yellow} de nuestro repositorio, haciendo click en el tercer icono del commit.

---

## Recuperaci√≥n de commits

![](./img/branch-commit.png)

Si te fijas ahora al lado de `1 branch` tenemos un [**men√∫ desplegable**]{.hl-yellow} en el que antes pon√≠a `main` y ahora un n√∫mero identificador del commit. Ya hablaremos de la idea de [**rama (branch)**]{.hl-yellow}

---

## Repo con rmd/qmd

::: columns
::: {.column width="60%"}
Vamos a poner en pr√°ctica lo aprendido:

1.  Crea un nuevo repositorio en Github (llamado `repo-github-2`) donde habr√° alojado con proyecto de R.

2.  Crea un proyecto en `RStudio` que se llame (por ejemplo) `proyecto-qmd`

3.  Una vez dentro del proyecto en `RStudio` haz click en `File < New File < Quarto Document`
:::

::: {.column width="40%"}
![](./img/primer-qmd.png)
:::
:::

Deber√°s tener un documento similar a este: un [**quarto markdown (.qmd)**]{.hl-yellow}, un documento que nos permitir√° incluir [**markdown + c√≥digo**]{.hl-yellow} (puede ser `R` o puede ser `Observable`, `D3`, etc).

---

## Repo con rmd/qmd

Este formato es ideal para:

-   [**Trabajar en equipo**]{.hl-yellow} construyendo el borrador de una pieza.
-   Tomar [**apuntes o informes**]{.hl-yellow} para uno mismo.
-   [**Presentar**]{.hl-yellow} tu trabajo a tus compa√±eros.

::: columns
::: {.column width="50%"}
![](./img/prueba-qmd-html.png)
:::

::: {.column width="50%"}
Si te fijas ahora nuestro repositorio tiene un archivo con formato `.html`...es decir...

[**¬°Es una web!**]{.hl-yellow}
:::
:::

---

## Github pages

¬øC√≥mo [**convertir nuestro repositorio en una web**]{.hl-yellow}?

![](./img/github-pages.png)

1.  Haz click en `Settings`
2.  Ve al apartado `Pages`
3.  En el subapartado `branch` selecciona la √∫nica rama que tenemos ahora (`main`)
4.  Selecciona la carpeta donde tengas el `.html` (en web complejas estar√° como en cualquier web en `docs`, en algo simple estar√° en la ruta raiz del repositorio)
5.  Haz click en `Save`

---

## Github pages

Si te fijas en la [**parte superior del repositorio**]{.hl-yellow} ahora tenemos un icono naranja, que nos indica que la [**web est√° en proceso de ser desplegada (deploy)**]{.hl-orange}

![](./img/github-naranja.png)

---

## Github pages

Pasados unos segundos (dependiendo del tama√±o de la web y tu conexi√≥n a internet) ese [**icono pasar√° a ser un check verde**]{.hl-green}: habemus web

![](./img/github-verde.png)

El [**link de la web por defecto**]{.hl-yellow} ser√° `{nombre_usuario}.github.io/{nombre_repo}`

---

## Github pages

![](./img/github-pages-deploy.png)

¬°Un momento! Ahora mismo nuestra web [**no nos est√° mostrando nuestro .qmd**]{.hl-red}, sino por defecto el [**README.md**]{.hl-yellow}.

. . .

Para que Github entienda que queremos visualizar ese `.html` que hemos generado a partir del `.qmd` vamos en nuestro proyecto local a [**borrar**]{.hl-yellow} todo lo que no sea nuestro archivo `.Rproj` y nuestro archivo `.qmd`, y vamos a [**cambiar el nombre**]{.hl-yellow} a este √∫ltimo llam√°ndolo `index.qmd`, y volvemos a compilarlo para [**generar un index.html**]{.hl-yellow}

---

## Github pages

Vamos a [**subir a Github ese nuevo proyecto**]{.hl-yellow} con el cambio de nombre (llamado `repo-github-3`) para ver luego las diferencias entre uno y otro

![](./img/repo-index-html.png)

---

## Github pages

Si [**repetimos el proceso para hacer una Page**]{.hl-yellow} y esperamos al tick verde...

::: columns
::: {.column width="50%"}
![](./img/index-html-qmd.png)
:::

::: {.column width="50%"}
Si a tu `.qmd` ya le llamas de inicio `index.qmd`, autom√°ticamente, al detectar Github un `index.html`, interpreta que ese [**archivo index.html**]{.hl-yellow} es el que define la web (y puedes personalizar a√±adiendo un archivo `css` de estilos)

[**Habemus web**]{.hl-green} simplemente clickando en Pages :)
:::
:::

---

## Repo con diapositivas

Vamos a crear el √∫ltimo repositorio que se llamar√° `repo-diapos`, y crear un proyecto en `RStudio` del mismo nombre (por ejemplo). Una vez creado le daremos a `File < New File < Quarto Presentation`.

::: columns
::: {.column width="45%"}
![](img/quarto-slides.png)
:::

::: {.column width="55%"}
La forma de escribir ser√° igual que un `.qmd` normal solo que ahora [**cada diapositiva la separaremos**]{.hl-yellow} con un `---` (usando archivos de estilos podemos personalizar lo que queramos)

Llama al archivo directamente `index.qmd`, s√∫belo a Github y con un click en Pages tienes una [**web con tus diapositivas**]{.hl-yellow}
:::
:::

---

## Uso de Gitkraken

La forma m√°s [**sencilla para trabajar de manera colaborativa**]{.hl-yellow} en Github, y tenerlo sincronizado con nuestro local, es hacer uso de [Gitkraken](https://www.gitkraken.com/download)

::: columns
::: {.column width="50%"}
![](img/gitkraken-repo.png)
:::

::: {.column width="50%"}
Una vez dentro clickamos en el icono de la carpeta (`Repo Management`) y si ya tenemos el repositorio en Github seleccionamos `Clone`, indicando donde queremos clonar (en nuestro local) y que [**repositorio de Github queremos clonar**]{.hl-yellow}.
:::
:::

---

## Uso de Gitkraken

::: columns
::: {.column width="50%"}
![](img/view-change.png)
:::

::: {.column width="50%"}
Una vez clonado, la idea es que cada [**cambio que hagamos en local nos aparecer√° en Gitkraken**]{.hl-yellow} como `View changes`.
:::
:::

---

## Uso de Gitkraken

Cuando tengas suficientes cambios como para [**actualizar el repositorio**]{.hl-yellow} (tampoco tiene sentido actualizar con cada edici√≥n), ver√°s algo similar a esto con todos los [**commits realizados**]{.hl-yellow}

::: columns
::: {.column width="50%"}
![](img/stage-all-changes.png)
:::

::: {.column width="50%"}
Podr√°s decidir cu√°les de los [**commits locales quieres incluir en remoto**]{.hl-yellow}, bien uno a uno o en `Stage all changes` (para todos)
:::
:::

---

## Uso de Gitkraken

Tras incluir los commits deber√°s incluir un [**t√≠tulo y descripci√≥n del commit**]{.hl-yellow}

![](img/titulo-commit.png)

---

## Uso de Gitkraken

Tras hacerlo ver√°s que ahora tenemos [**dos iconos separados en una especie de √°rbol**]{.hl-yellow} (¬øte acuerdas de la `branch` o rama?):

-   [**Ordenador**]{.hl-purple}: la versi√≥n del repositorio que tienes en tu [**ordenador**]{.hl-yellow}.

-   [**Logo**]{.hl-purple}: la versi√≥n del repositorio que tienes [**subida en remoto**]{.hl-yellow}

![](img/split-gitkraken.png)

---

## Uso de Gitkraken

Mientras eso suceda solo tendr√°s sincronizado tu ordenador con Gitkraken, pero no con Github. Para ello haremos [**click en Push**]{.hl-yellow} (con `Pull` podr√°s forzar a tener en local lo mismo que en remoto).

![](img/push-gitkraken.png)

---

## Branchs

Como hemos mencionado ya en varias ocasiones, hay un elefante en la habitaci√≥n que a√∫n no hemos mentado: las [**ramas o branchs**]{.hl-yellow} de un repositorio.

. . .

Imagina que est√°is trabajando varios en un proyecto y tene√≠s una versi√≥n que funciona pero que quer√©is [**modificar en paralelo a partir del estado actual**]{.hl-yellow} del repositorio.

. . .

Las [**ramas**]{.hl-yellow} nos permiten partir de una versi√≥n com√∫n del repositorio y hacer cambios que [**no afecten a los dem√°s**]{.hl-yellow}

---

## Branchs

Para [**crear una rama**]{.hl-yellow} a partir del estado actual de repositorio haremos click en `Branch` y le pondremos un nombre

![](img/branch-button.png)

Una vez creada ver√°s [**dos iconos**]{.hl-yellow} y un [**men√∫ desplegable**]{.hl-yellow} con las distintas ramas en las que quieres hacer el commit. Imagina que realizas un cambio pero [**no quieres a√±adirlo a la rama principal**]{.hl-yellow}: puedes hacer el [**commit en tu rama propia en LOCAL**]{.hl-yellow} (lo har√°s en la rama activa de tu men√∫ de branchs).

---

## Branchs

La primera vez te pedir√° que escribas la [**rama en REMOTO**]{.hl-yellow} con la quieres sincronizar tu rama en local. [**Consejo**]{.hl-green}: ponle el mismo nombre en remoto que en local.

![](img/name-branch.png)

---

## Branchs

F√≠jate que ahora tenemos el ordenador y el logo en el mismo sitio. Esto no significa que tengas ambas ramas en tu local, solo que [**Gitkraken tiene ambas sincronizadas**]{.hl-yellow}: clickando en cualquiera de ellas, tus archivos en tu ordenador cambiar√°n.

![](img/both-branchs.png)

---

## Pull request

Lo m√°s recomendable es que [**solo se incorpore de una rama secundaria**]{.hl-yellow} a la rama principal aquello que est√° [**validado por un/a coordinador/a**]{.hl-yellow} del repositorio, asegur√°ndose que todo funciona correctamente.

Cuando queramos incluirlo haremos [**click con bot√≥n derecho**]{.hl-yellow} en el icono de la rama secundaria y seleccionamos `Start a pull request to origin from...`

![](img/pull-request-menu.png)

. . .

Una [**pull request**]{.hl-yellow} ser√° una [**petici√≥n al responsable de la rama principal**]{.hl-yellow} para incluir los cambios

---

## Pull request

::: columns
::: {.column width="60%"}
![](img/create-pull-request.png)
:::

::: {.column width="40%"}
En el cuadro que no se abre deberemos escribir:

-   La [**rama**]{.hl-yellow} a la que hacer el `merge` (normalmente la `main`)
-   T√≠tulo y resumen de los cambios
-   Puedes incluso asignar un [**revisor**]{.hl-yellow} entre los colaboradores del repo.
-   Puedes asignar [**etiquetas**]{.hl-yellow}
:::
:::

---

## Pull request

Mientras no se acepte aparecer√° un [**icono de rama**]{.hl-yellow} y un +1 en Pull Requests

![](img/pending-branch.png){width="500"}

. . .

Si somos al mantenedor del repositorio, haciendo click en el men√∫ nos saldr√°n las ramas que nos quieren hacer hacer `merge`

![](img/menu-pr.png){width="500"}

---

## Pull request

Al hacer click se abrir√° un [**cuadro de Pull Request**]{.hl-yellow} para decidir si

-   [**Revisar**]{.hl-yellow} los cambios
-   [**Aprobar**]{.hl-yellow} el `merge`
-   [**A√±adir comentarios**]{.hl-yellow} al que ha solicitado el `merge` por si queremos solicitar alg√∫n cambio [**antes de ser aprobado**]{.hl-yellow}

## ![](img/menu-pr-2.png)

## Pull request

Tras revisar todo y aprobarlo clickaremos en `Confirm merge`, y tras ello podremos decidir si esa rama que era paralela a la principal la queremos [**eliminar**]{.hl-yellow} o dejar visible a todos (consejo: dejar visible para tene [**trazabilidad**]{.hl-yellow} del proyecto de trabajo)

::: columns
::: {.column width="50%"}
![](img/merge-branch.png)
:::

::: {.column width="50%"}
![](img/delete-branch.png)
:::
:::

