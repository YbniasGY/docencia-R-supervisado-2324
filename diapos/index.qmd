---
title: "Aprendizaje Supervisado I"
subtitle: "Métodos de predicción lineal"
title-slide-attributes:
  data-background-image: img/data-science-2.jpeg
  data-background-size: cover
  data-background-opacity: "0.2"
author: "Grado en Ciencia de Datos Aplicada • curso 2023-2024"
affiliation: Facultad de Estudios Estadísticos (UCM)
lang: es
language: custom_lang.yml
format: 
  revealjs:
    theme: [default, style.scss]
    chalkboard: true
    multiplex: true
    menu:
      side: left
      width: normal
    footer: "[<strong>Javier Álvarez Liébana</strong>](...) • Grado en Ciencia de Datos Aplicada (UCM) • curso 2023-2024"
    slide-number: c/t
execute:
  echo: true
---

# Empieza lo bueno...empieza la modelización

[**Vamos a juntar las piezas del puzzle para hacer «magia»**]{style="color:#444442;"}

```{r}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
```

---

## ¡Buenas!

[**Correo**]{.hl-green}: **<javalv09@ucm.es>**. [**Despacho**]{.hl-green}: 722 (3ª planta). [**Tutorías**]{.hl-green}: lunes (14:30-16:00), martes (13:00-15:00) y viernes (13:00-14:00).

::: columns
::: {.column width="30%"}
![](img/me.jpeg)
:::

::: {.column width="70%"}
::: incremental
-   [**Javier Álvarez Liébana**]{.hl-yellow}, de Carabanchel (Bajo).

-   Licenciado en Matemáticas (UCM). [**Doctorado en estadística**]{.hl-yellow} (UGR).

-   Encargado de la [**visualización y análisis de datos covid**]{.hl-yellow} del Principado de Asturias (2021-2022).

-   Miembro de la [**Sociedad Española de Estadística e IO**]{.hl-yellow} y la [**Real Sociedad Matemática Española**]{.hl-yellow}.
:::
:::
:::

. . .

Actualmente, [**investigador y docente en la Facultad de Estadística de la UCM**]{.hl-yellow}. Divulgando por [**Twitter**](https://twitter.com/dadosdelaplace) e [**Instagram**](https://instagram.com/javieralvarezliebana)


---

## Objetivos

::: columns
::: {.column width="37%"}
![](https://assets-global.website-files.com/6092cb6b4ac959f39728dd26/6188a97fa499b5fbfe410417_target%20(1).png)
:::

::: {.column width="63%"}
::: incremental
- Empezar a [**relacionar asignaturas**]{.hl-yellow} como matemáticas, inferencia y R.

- Aprender los fundamentos del [**aprendizaje estadístico**]{.hl-yellow} (ahora llamado Machine Learning o data science)

- Pasar de los descriptivo a lo predictivo: [**construir nuestros primeros modelos**]{.hl-yellow}

- Entender en profundidad el contexto de la [**predicción lineal**]{.hl-yellow}.


:::
:::
:::

---

## Evaluación

-   [**Asistencia**]{.hl-yellow}. Se [**valorará muy positivamente**]{.hl-purple} la participación. Si se [**restarán puntos si eres expulsado de clase**]{.hl-purple}: -0.3 la 1ª vez, -0.6 la 2ª, -1.2 la 3ª...

. . .

- [**Evaluación continua**]{.hl-yellow}: 1 **examen a papel** (25%), **2 entregas R en clase** (20%-30%) y **entrega grupal último día** (3-6p obligatoriamente, 25%)
  
. . .

- [**Examen final**]{.hl-yellow}:
  - [**Menos de un 6 (sin la grupal)**]{.hl-purple} -> examen de R en lugar de la grupal.
  - [**Más de un 6.5 (con la grupal)**]{.hl-purple} -> podrás decidir **peso del final entre un 20% y un 100%**.
  - [**Entre 5 y 6.5 (con la grupal)**]{.hl-purple} -> podrás decidir **peso del final entre un 40% y un 100%**.
  - [**Por debajo de 5 (con la grupal)**]{.hl-purple} -> podrás decidir **peso del final entre un 70% y un 100%**.
  

---

## Planificación


* [**Entrega I (20%)**]{.hl-yellow}: semana 27-29 de febrero de 2024

* [**Entrega II (30%)**]{.hl-yellow}: semana 2-4 de abril de 2024

* [**Parcial con papel y boli (25%)**]{.hl-yellow}: 16-18 de abril de 2024 

* [**Entrega grupal (25%)**]{.hl-yellow}: 9 de mayo de 2024 (incluyendo lo visto hasta el 30 de abril)

&nbsp;

(se admiten propuestas)

. . . 

[**Clase a recuperar otro día**]{.hl-yellow}: clase del 21 de marzo.

---

## Materiales

* [**Diapositivas**]{.hl-yellow}: las diapositivas que usaremos en el aula a lo largo del curso, estructuradas por clases, estarán disponibles y actualizadas en **<https://javieralvarezliebana.es/docencia-R-supervisado-2324/diapos>** 

En el menú de las diapositivas (abajo a la izquierda) tienes una [**opción para descargarlas en pdf**]{.hl-yellow} en `Tools` (consejo: no lo hagas hasta el final del curso ya que irán modificándose)
  
&nbsp;

* [**Material**]{.hl-yellow}: [**scripts de cada tema**](https://github.com/dadosdelaplace/docencia-R-supervisado-2324/tree/main/material) y materiales extras

* [**Resúmenes de paquetes**]{.hl-yellow}: [**chuletas de los paquetes**](https://github.com/dadosdelaplace/docencia-R-supervisado-2324/tree/main/fichas%20paquetes) en formato .pdf

---


## Requisitos

Para el curso los únicos requisitos serán:

1.  [**Conexión a internet**]{.hl-yellow} (para la descarga de algunos datos y paquetes).

2.  [**Instalar R y RStudio**]{.hl-yellow}: la descarga la haremos (gratuitamente) desde <https://cran.r-project.org/> y  <https://posit.co/download/rstudio-desktop/>

3. Se darán por asumido conocimientos aprendidos de [**R base, tidyverse y ggplot**]{.hl-yellow}

4. Se darán por asumido conocimientos aprendidos de [**Quarto, diapositivas en Quarto y Github**]{.hl-yellow}. Para las entregas [**SOLO SE VALORARÁ**]{.hl-purple} la salida html correspodiente.

5. [**Recomendable**]{.hl-yellow}: saber usar la calculadora en modo estadístico.


---

## Listado de clases

* [**Clase 1**](#clase-1): repaso de descriptiva
* [**Clase 2**](#clase-2): importancia de la visualización
* [**Clase 3**](#clase-3): intro al aprendizaje estadístico. Sesgo vs varianza. Supervisado vs no supervisado. Correlación vs dependencia vs causalidad.


# Clase 1: repaso {#clase-1}

[**Objetivo de la predicción lineal. Concepto de linealidad. Repaso de estadística descriptiva**]{style="color:#444442;"}

---

## ¿Qué es predecir?

Como veremos más adelante, en esta asignatura vamos a tratar principalmente lo que se conoce el aprendizaje estadístico como [**predicción (continua)**]{.hl-yellow}

. . .

Dada una [**variable objetivo (variable dependiente)**]{.hl-yellow}, y con la información aportada por un conjunto de [**variables predictoras (covariables)**]{.hl-yellow}, el objetivo será construir un modelo que consiga dar una estimación/predicción lo «mejor posible»

. . .

Es importante que - de momento - distingamos dos conceptos:

* [**Estimación**]{.hl-yellow}: el modelo aprende de unos datos e intenta estimar dichos valores que ha usado.
* [**Predicción**]{.hl-yellow}: el modelo aprende de unos datos e intenta estimar valores que el **modelo no conoce**.

Más adelante los llamaremos «predicción en train» y «predicción en test»

---

## ¿Qué es la linealidad?

En matemáticas decimos que una función $f(x)$ es [**lineal**]{.hl-yellow} cuando se cumple:

* [**Propiedad aditiva**]{.hl-purple}: $f(x + y) = f(x) + f(y)$

* [**Propiedad homogénea**]{.hl-purple}: $f(k*x) = k*f(x)$ (donde $k$ es una constante en $\mathbb{R}$).

Ambas se pueden resumir en $f(a*x + b*y) = a*f(x) + b*f(y)$

. . .

En estadística llamamos [**modelo de predicción lineal**]{.hl-yellow} a un modelo que usa la información de covariables $X_1, X_2, \ldots, X_p$, de manera que su información siempre [**se relacionen entre sí con sumas y restas**]{.hl-yellow}.

- [**Ejemplos lineales**]{.hl-green}: $y = 2*x_1 - 3$ o  $y = 4 - \frac{x_1}{2} + 3*x_2$

- [**Ejemplos no lineales**]{.hl-red}: $y = 2*\frac{1}{x_1}$ o  $y = 4 - x_{1}^{2} - x_2$ o $y = ln(x_1) + cos(x_2)$

En general son modelos que se pueden representar con rectas (una sola predictora), planos (dos predictoras), etc.

---

## Repaso: continua vs discreta

* **Caracteres**: cada una de las características o cualidades que se podrían medir o analizar para cada individuo de la población (y de los que disponemos el valor para cada individuo de la muestra).

* **Modalidades**: diferentes valores que puede adoptar una característica o variable.

&nbsp;

Como veremos más adelante, en el ámbito del aprendizaje estadístico va a ser fundamental tener clara la diferencia entre una [**variable cualitativa y cuantitativa**]{.hl-purple} y entre [**variable continua y otra discreta**]{.hl-yellow}. ¿Cuál es la diferencia?


---

## Repaso: continua vs discreta

Imagina las siguientes variables:

- ¿Tienes hermanos? (si/no)
- Color de zapatillas
- Nivel de estudios
- Número de hermanos
- Número de pelos en la cabeza
- Resultado de un dado
- Temperatura ºC
- Estatura o peso

&nbsp;

[**¿Cuál es la diferencia entre ellas**]{.hl-yellow}


---

## Repaso: continua vs discreta



* [**Cualitativas**]{.hl-yellow}: **cualidades o categorías**. Ejemplos: sexo, estado civil, estudios, etc.
  * [**Ordinales**]{.hl-purple}: admiten relación jerarquica (suspenso-aprobado-bien, sano-leve-grave).
  * [**Nominales**]{.hl-purple}: no tienen asociada una jerarquía (sexo, religión, color de zapatillas, etc).

. . .

* [**Cuantitativas**]{.hl-yellow}: cuantificación numérica.
  * [**Discretas**]{.hl-purple} (barras): se pueden contar y enumerar (aunque sean infinitos) (granos de arena, nº hermanos, etc) → se **suman**
  * [**Continuas**]{.hl-purple}: toman infinitos valores y entre dos valores cualesquiera hay a su vez infinitas opciones (estatura, peso, etc) → se **integran**

---

## Repaso: continua vs discreta

![](img/vitro-fuego-discretas.jpg)


---

## Repaso: población vs muestra

En estadística llamaremos [**población**]{.hl-yellow} al universo teórico, al colectivo de individuos a estudiar, o de posibles elementos/eventos de los podríamos tener observaciones (ejemplo: 47 millones de españoles).

. . .

Cada uno de esos elementos o eventos se llaman **individuos**.

. . .

Aunque nuestro objetivo será conocer algunas de las propiedades de la población, la [**población suele ser inaccesible**]{.hl-red} en su totalidad → [**SELECCIÓN**]{.hl-green} de un conjunto de individuos

---

## Repaso: población vs muestra

Para ello en estadística usamos lo que se conoce como [**muestra**]{.hl-yellow}: un subconjunto finito, de tamaño $n$, «representativo» de la población (en estudio estadístico realizado sobre la totalidad de una población se denomina **censo**).

. . .

Para ello existe una rama conocida como [**muestreo**]{.hl-yellow}

* [**Probabilístico**]{.hl-purple}: todos los individuos tienen oportunidad de ser seleccionados, y la probabilidad de que suceda puede ser modelizada (al azar, por estratos, etc).

* [**No probabilístico**]{.hl-purple}: algunos elementos de la población no tienen posibilidad de selección (sesgo de exclusión), o su probabilidad no puede ser conocida.

. . .

🤔 ¿Sería adecuado hacer una encuesta sobre el streamer favorito de los jóvenes a través de una encuesta realizada por teléfono fijo?


---

## Sesgos en el muestreo

![](img/sampling-bias.jpg)

. . .

[**Sesgo de selección**]{.hl-yellow}: aparece cuando no se tiene en cuenta la forma en la que se han recogido los datos.

--- 


## Sesgos en el muestreo


![](img/dewey.jpg)

El ejemplo más famoso es el caso [**«Dewey defeats Truman» (Dewer derrota a Truman)**]{.hl-yellow}, el titular con el que abrió el Chicago Tribune en 1948, el mismo día en el que Truman ganó al repúblicano Dewer en las elecciones de 1948: sin esperar a los resultados, se basaron en una encuesta telefónica (sin contar con el sesgo que, en aquella época, solo la clase alta tenía teléfono).

---



## Sesgos en el muestreo

![](img/superviviente.jpg)

¿Dónde reforzarías los aviones?

---

## Sesgos en el muestreo

![](img/superviviente.jpg)



El [**sesgo del superviviente**]{.hl-yellow} (un tipo de sesgo de selección) aparece cuando se toma una muestra de un fenómeno ignorando si los individuos elegidos tienen las mismas opciones respecto al mismo.


---

## Repaso: medidas de centralización

* [**Media**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la media muestral $\overline{x}$ se define como la **suma de todos los valores dividida por el tamaño muestral**

$$\overline{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$

. . .

[**Geométricamente**]{.hl-purple}: es el **valor «más cercano» de todos los datos a la vez** (minimiza las distancias al cuadrado)

---

## Media muestral


[**VENTAJAS**]{.hl-green}

* Fácil de calcular y entender
* Fácil y eficiente de programar
* Siempre existe (para cuantitativas)

. . .

[**DESVENTAJAS**]{.hl-red}

* No es un valor de los datos (la media de {1, 2, 3, 4} es 2.5)
* **Poco robusta** (valores atípicos le afectan mucho)
* Solo se puede definir para variables cuantitativas


---

## Repaso: medidas de centralización

* [**Mediana**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la mediana muestral se define como el **valor que es mayor o igual que al menos el 50%**, y menor igual que al menos el 50% de los datos

$$Me_{x} = \arg \min_{x_i} \left\lbrace F_i > 0.5 \right\rbrace, \quad Me_x = e_{i-1} + \frac{0.5 - F_{i-1}}{F_i - F_{i-1} }a_i$$

La mediana es el [**valor de en medio**]{.hl-purple} si ordenamos los datos (y si se pueden ordenar...)

---

## Mediana muestral


[**VENTAJAS**]{.hl-green}

* Suele ser un valor de la muestra
* Un poco más robusta que la media


. . .

[**DESVENTAJAS**]{.hl-red}

* Muy ineficiente (requiere un algoritmo de ordenación)
* Solo definida para  cuantitativas o cualitativas ordinales

---


## Repaso: medidas de centralización

* [**Moda**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la moda muestral se define como el **valor o valores más repetidos** (en caso de que existan).

$$Mo_x = \arg \max_{x_i} f_i, \quad Mo_x = e_{i-1} + \frac{d_i - d_{i-1}}{\left(d_i - d_{i-1} \right) + \left(d_i - d_{i+1} \right)}a_i$$


. . .

[**Gráficamente**]{.hl-purple}: representa el «pico» de un diagrama de barras o un histograma

---

## Moda muestral


[**VENTAJAS**]{.hl-green}

* Es un valor de la muestra
* Muy robusta
* **Se puede calcular para cualquier** cuanti o cuali



. . .

[**DESVENTAJAS**]{.hl-red}

* No siempre existe (amodal) y pueden existir varias (bimodal, trimodal, etc)
* Poco usada en inferencia


---

## Repaso: medidas de centralización

![](img/ine-salarios-oculto.jpg)

**¿Cuál es la mediana, la media y la moda?**

---

## Repaso: medidas de centralización

![](img/ine-salarios.jpg)

---

## Repaso: medidas de dispersión

![](img/iker-jimenez.jpg)

¿Qué tiene que ver la imagen con la dispersión?


---

## Repaso: medidas de dispersión

![](img/extremos.jpg)

El cambio climático no solo es porque aumente la [**temperatura media (centralización)**]{.hl-yellow} sino por la aparición cada vez más frecuente de fenómenos extremos 


Aumento de la [**variabilidad**]{.hl-yellow} → aumento de la [**DISPERSIÓN**]{.hl-yellow}

---

## Repaso: medidas de dispersión

[**¿Cómo medir lo que se alejan los datos de la media?**]{.hl-yellow}

. . .

![](img/primera-idea-varianza.jpg)

Una **primera idea** podría ser [**medir la distancia de cada dato al centro**]{.hl-yellow}, es decir, restar cada dato de la media, y después realizar su promedio.

---

![](img/wait-for-it.jpg)

---

## Repaso: medidas de dispersión

Imagina que tenemos la siguiente muestra $X = \left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$.

**¿Cuánto vale la media?**

. . .

La media vale 0 y la distancia a ella es...la propia muestra $\left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$. **¿Cuál es el promedio de dichas distancias?**

. . .

Pues...de nuevo vale 0.

. . .

Si la dispersión es 0...[**¿no hay dispersión?**]{.hl-red} ¿No debería de dar 0 solo cuando los datos sean constantes?

---

## Repaso: medidas de dispersión

Para **evitar que se cancelen** los signos lo que haremos será calcular el [**promedio PERO de las distancias al cuadrado**]{.hl-yellow}, la conocida como [**varianza**]{.hl-yellow}

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 $$

. . .


::: callout-warning
# Cuidado

Tomar el valor absoluto (para evitar que se cancelen los signos) suele ser una mala idea en matemáticas (no es derivable como función).
:::

---

## Repaso: medidas de dispersión


[**Problema**]{.hl-red}: si los datos están en metros, la varianza estará en...metros cuadrados


. . .

:::: columns
::: {.column width="50%"}

¿Tiene sentido medir la dispersión de nuestra estatura en baldosas?

:::

::: {.column width="50%"}

![](img/albert-rivera.jpg)
:::

::::

---

## Repaso: medidas de dispersión

Para tener una [**medida de dispersión en las unidades de los datos**]{.hl-yellow} calcularemos la [**desviación típica**]{.hl-yellow}, como la raíz cuadrada de la varianza

$$s_{x} = \sqrt{s_{x}^{2}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2} = \sqrt{\overline{x^2} - \overline{x}^2}$$


---

## Repaso: medidas de dispersión

Todavía tenemos un pequeño problema.

Imagina que queremos **comparar la dispersión de dos conjuntos** de datos, estaturas de personas y diámetros de núcleos de células. Y Supongamos que las medias son 170 cm y 5 micrómetros, y la desviación típica de 1 cm y 1.5 micrómetros.

[**¿Qué conjunto de datos es más disperso?**]{.hl-yellow}

. . .

Para tener una  **medida de dispersión adimensional** definiremos el [**coeficiente de variación**]{.hl-yellow} 

$$CV_{x} = \frac{s_{x}}{\left| \overline{x} \right|}$$

---

## Repaso: medidas de localización

Las [**medidas de posición o localización**]{.hl-yellow} nos localizan los datos, siendo valores que **nos dividen un conjunto ordenado** en subconjuntos del mismo tamaño (ejemplo: mediana es percentil 50).

* **Percentil**: valores $P_{\alpha}$ del conjunto ordenado que dejan por debajo, al menos, el $\alpha$% de datos y $\alpha$% por encima.       
* **Decil**: valores $D_{\alpha}$ que dividen los datos en 10 partes iguales.

* **Cuartil**: valores $C_{\alpha}$ o $q_{\alpha}$ que dividen los datos en 4 partes iguales.

---

## Repaso: covarianza y correlación

[**¿Qué es en realidad la varianza?**]{.hl-yellow}

. . .

La [**varianza es el promedio de las desviaciones al cuadrado**]{.hl-yellow} (respecto a la media), apareciendo dos veces dicha desviación: puede ser entendida como una [**medida que cuantifica la relación de una variable CONSIGO MISMA**]{.hl-yellow}

. . .

[**¿Y si quiésemos medir la relación de una variable X respecto a otra variable Y (en lugar de consigo misma)?**]{.hl-purple}


---

## Repaso: covarianza y correlación

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 \quad \text{(varianza)}$$


La idea detrás de la [**covarianza**]{.hl-yellow} es justo esa: sustituir una de esas desviaciones de la X por la desviación de la Y.


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

---

## Repaso: covarianza y correlación

Es importante entender algunas [**propiedades de la covarianza**]{.hl-yellow}

. . .

* [**Signo**]{.hl-purple}: la covarianza puede ser tanto positiva como negativa como 0: al eliminar el cuadrado de la varianza, ya no es necesario que sea positiva


. . .

* [**¿Qué cuantifica?**]{.hl-purple} La covarianza mide la [**relación LINEAL**]{.hl-red} (en torno a una recta) entre dos variables

. . .

* [**¿Qué dice su signo?**]{.hl-purple} El signo de la covarianza nos indicará la [**dirección de la dependencia lineal**]{.hl-yellow}: si es positiva, la relación será creciente (cuando X crece, Y crece); si es negativa, la relación será decreciente (cuando X crece, Y decrece)


---

## Repaso: covarianza y correlación

Al igual que pasaba con la varianza, la [**covarianza depende de las unidades y magnitudes**]{.hl-yellow} de los datos, así que lo que haremos será [**estandarizar la covarianza**]{.hl-yellow}. Definiremos la [**coeficiente correlación lineal (de Pearson)**]{.hl-yellow} como la covarianza dividida entre el producto de las desviaciones típicas (adimensional)


$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

. . .

Tiene el [**mismo signo que la covarianza**]{.hl-yellow} (el denominador es siempre positivo) y sus [**valores siempre están entre -1 y 1**]{.hl-yellow}

* más cerca de -1 o 1 → relación lineal más fuerte
* más cerca de 0 → ausencia de relación **LINEAL**

---

## Repaso: covarianza y correlación

![](img/correlaciones.jpg)


# Clase 2: importancia del dataviz {#clase-2}

[**¿Basta con calcular la correlación para cuantificar si un ajuste es adecuado?**]{style="color:#444442;"}


---

## Conjunto anscombe

En el paquete `{datasets}` se encuentra el dataset conocido como [**cuarteto de Anscombe**]{.hl-yellow}, un dataset que cuenta con 4 conjuntos de datos.

```{r}
anscombe_tb <- as_tibble(datasets::anscombe)
anscombe_tb
```

. . .

[**Primera tarea: convierte a tidydata**]{.hl-yellow}

---

## Conjunto anscombe

```{r}
anscombe_x <- 
  anscombe_tb |>
  pivot_longer(cols = x1:x4, names_to = "dataset", values_to = "x") |> 
  select(-contains("y")) |> 
  mutate(dataset = str_remove_all(dataset, "x"))

anscombe_y <- 
  anscombe_tb |>
  pivot_longer(cols = y1:y4, names_to = "dataset", values_to = "y") |> 
  select(-contains("x"), -dataset)

anscombe_tidy <-
  anscombe_x |> mutate("y" = anscombe_y$y)
anscombe_tidy
```

---

## Conjunto anscombe

[**¿Qué características muestrales tienen?**]{.hl-yellow} Calcula la media, varianza, desv. típica, covarianza y correlación en cada dataset

. . .

```{r}
anscombe_tidy |> 
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

. . .

Si te fijas [**todos los datasets tienen los mismos momentos**]{.hl-yellow} muestrales, incluso tendrían el mismo ajuste de regresión...¿serán el mismo dataset desordenado?

---

## Conjunto anscombe

[**Visualiza los 4 datasets**]{.hl-yellow}

. . .

```{r}
#| code-fold: true
ggplot(anscombe_tidy) +
  geom_point(aes(x = x, y = y, color = dataset), size = 3, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~dataset) + theme_minimal()
```

---

## Conjunto anscombe

```{r}
#| echo: false
ggplot(anscombe_tidy) +
  geom_point(aes(x = x, y = y, color = dataset), size = 3, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~dataset) + theme_minimal()
```


Por suerte o por desgracia [**no todo son matemáticas**]{.hl-red}: antes de pensar que modelo es mejor para nuestros datos, es importantísimo realizar un [**análisis exploratorio**]{.hl-yellow} de los datos (incluyendo visualización)

---

## Datasaurus

Podemos visualizarlo de manera aún más extrema con el dataset `datasaurus_dozen` del paquete `{datasauRus}` (ver más en <https://www.research.autodesk.com/publications/same-stats-different-graphs/>)

```{r}
library(datasauRus)
datasaurus_dozen |>
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

---

## Datasaurus

```{r}
#| code-fold: true
ggplot(datasaurus_dozen |> filter(dataset != "wide_lines"),
       aes(x = x, y = y, color = dataset)) +
  geom_point(size = 2.5, alpha = 0.75) +
  geom_smooth(method = "lm", se = FALSE)  +
  facet_wrap(~dataset, ncol = 3) +
  theme_minimal()
```

---

# Clase 3: profundizando en la correlación {#clase-3}

[**Matrices de correlación y covarianza. Correlación vs causalidad vs dependencia**]{style="color:#444442;"}


---

## Correlación lineal: sin agrupar


Como decíamos, la idea detrás de la [**covarianza**]{.hl-yellow} es una "varianza" entre dos variales (la varianza es una covarianza de una variable consigo misma), midiendo el **promedio de lo que se desvía cada una respecto a su media**

$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

. . .

Para calcularla en `R` basta con usar la función `cov()`

```{r}
starwars |> 
  drop_na(mass, height) |> 
  summarise(cov(mass, height))
```

---

## Correlación lineal: sin agrupar

Vamos a practicar una vez más como [**hacerlo a mano con el siguiente ejercicio**]{.hl-yellow}.

---

## Cor lineal: sin agrupar

En la tabla inferior se han recopilado (del 2013 al 2022) la **temperatura media en el mes de abril en Madrid (variable X, en ºC)** y el **número de días (variable Y) en el que el nivel de ozono superó las 0.20 ppm (partes por millón)**

* ¿Cuál fue media de días en los que se superó umbral de ozono de 0.20 ppm?
* ¿Cuál fue media de días en los que se superó umbral de ozono en los años que la temperatura media en marzo superó los 17.4ºC?
* ¿Cuál es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)

---


## Correlación lineal: sin agrupar

Repite el ejercicio con pocas líneas de código `R`

* ¿Cuál fue la media de días en los que se superó el umbral de ozono de 0.20 ppm?
* ¿Cuál fue la media de días en los que se superó el umbral de ozono en los años que la temperatura media en marzo superó los 17.4ºC?
* ¿Cuál es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)


---


## Correlación lineal: sin agrupar

Realiza lo que consideres tanto a mano como en `R`

* ¿Existe alguna **relación de dependencia entre las variables**? ¿De qué tipo? ¿Cómo de fuerte o débil es dicha relación? ¿En qué dirección es dicha relación?


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

---

## Correlación lineal: sin agrupar

No sé si te has fijado qué sucede cuando intentamos [**calcular la covarianza/correlación de varias variables**]{.hl-yellow}, por ejemplo vamos a calcular la (cuasi)covarianza de todas las variables numéricas de starwars.

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

. . .

Podemos usar la función `cov()` sin más, fuera de un resumen, obteniendo lo que se conoce como [**matriz de (cuasi)covarianzas**]{.hl-yellow} y que tendrá un papel fundamental en estadística ya que contiene la información (= varianza) del dataset.

---

## Matriz de covarianzas


```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

Además de ser [**simétrica**]{.hl-yellow}...¿qué tenemos en la [**diagonal**]{.hl-yellow}?

. . .

La [**matriz de (cuasi)covarianzas**]{.hl-yellow} se denota como $\Sigma$ y sus elementos se define como $\Sigma_{ii} = s_{x_i}^{2}$ para la diagonal y $\Sigma_{ij} = \Sigma_{ji} = s_{x_i x_j}$ fuera de ella.

. . .

::: callout-important
## Importante

Recuerda que los softwares estadísticos nos devuelven siempre la [**cuasi covarianza**]{.hl-purple}, dividido entre $n-1$ y no entre $n$. La cuasivarianza y la cuasicovarianza son los [**mejores estimadores muestrales (insesgados)**]{.hl-yellow} de los respectivos parámetros poblaciones

:::

---

## Matriz de correlaciones

De la misma manera con `cor()` podemos [**calcular la matriz de correlaciones**]{.hl-yellow} (en este caso sin el `cuasi` ya que se cancelan denominadores)

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cor()
```

. . .

La [**matriz de correlaciones**]{.hl-yellow} se denota como $R$ y sus elementos se define como $r_{ii} = 1$ para la diagonal y $r_{ij} = r_{x_ix_j}$ fuera de ella, y nos proporciona la dependencia lineal entre variables ya de manera **estandarizada**.

---

## Matriz de correlaciones

[**¿Se te ocurre alguna manera de calcular la matriz de correlaciones a partir de la de covarianzas?**]{.hl-yellow}

---

## Correlación lineal: datos agrupados

Imagina ahora que queremos calcular la covarianza/correlación de los siguientes datos que representan 24 tiradas de dados

![](img/dados.png)

¿Cómo calcular la covarianza/correlación agrupando los datos?

---

## Correlación lineal: datos agrupados

![](img/tabla-bi-freq.png)
---

## Correlación lineal: datos agrupados

---

## Correlación vs dependencia


Podemos tener [**variables incorreladas**]{.hl-red}, con correlación nula, pero que [**exista dependencia entre ellas**]{.hl-green}: la covarianza/correlación [**SOLO CAPTURA relaciones lineales**]{.hl-yellow}, nada más.

. . .

Veamos un ejemplo sencillo con $X = \left\lbrace -1, 0, 1 \right\rbrace$ y $Y = X^2 =  \left\lbrace 1, 0, 1 \right\rbrace$. 

* La media de ambas es nula
* La media del producto es la media de $XY = \left\lbrace -1, 0, 1 \right\rbrace$, que es de nuevo nula
* Así la covarianza $\overline{x*y} - \overline{x}*\overline{y}$ es nula a pesar de tener la mayor dependencia posible (dependencia funcional)

---

## Correlación vs dependencia

![](img/covarianza-no-lineal.png)

En relaciones no lineales como la de la imagen, la **correlación estará cercana a cero** (ya que no hay relación lineal) pero existe una [**dependencia**]{.hl-yellow}. Diremos que [**dos variables son dependientes entre sí**]{.hl-yellow} cuando existe un **patrón numérico que las relaciona**

. . .

* [**Independencia implica incorrelación**]{.hl-green}
* [**Incorrelación NO implica independencia**]{.hl-red}

---

## Correlación vs dependencia

![](img/escenarios-covarianza.png)

---

## Dependencia vs causalidad

![](img/helados-moreno.jpg)

Si $X$ e $Y$ fuesen variables dependientes (lineales o no)...[**¿implicaría que el ascenso/descenso de una provoca el ascenso/descenso de la otra?**]{.hl-yellow}

. . .

Imagina por ejemplo dos variables: nivel de bronceado (X) y consumo de helados (Y). ¿Son dependientes? Aparentemente sí ya que su comportamiento es similar. **¿Una causa la otra?**

---

## Dependencia vs causalidad

![](img/helados-moreno.jpg)

Diremos que dos variables tienen una [**relación causal o de causalidad**]{.hl-yellow} cuando una variable es la CAUSA del comportamiento de la otra, algo que no sabremos solo con estadística (sino con conocimiento experto, en este caso de nutricionistas y médicos)

. . .

[**Correlación NO IMPLICA causalidad**]{.hl-red} (al menos no tiene por qué)

---

## Dependencia vs causalidad


![](img/chocolate-correlacion.jpg)


Este fenómeno es conocido como [**correlaciones espúreas**]{.hl-yellow}, y aparece cuando dos variables presentan una [**relación matemática**]{.hl-green} pero sin [**ningún tipo de relación causal o lógica**]{.hl-red}. Puedes ver más en <https://www.tylervigen.com/spurious-correlations>

---

## Dependencia vs causalidad

![](img/confounder.jpg)

Dicho patrón matemático puede deberse al mero azar o la existencia de lo que se conoce como [**variables de confusión**]{.hl-yellow} (en el caso del helado, el verano es el que causa ambas).

La rama que se dedica al [**estudio de la causalidad**]{.hl-yellow}, combinando las ramas de la **estadística, filosofía, sociología y psicología** se le conoce como [**inferencia causal**]{.hl-yellow} y es fundamental en un análisis más profunde de las relaciones entre las variables (sobre todo en campos como la economía o la sociología)


# Clase 4: primeras regresiones {#clase-4}

[**Historia regresión. Aprendizaje supervisado. Regresión lineal univariante**]{style="color:#444442;"}

---

## Historia de la reg

> Hay una regla universal: cualquier pariente tuyo es probablemente más mediocre que tú»

La [**historia de regresión**]{.hl-yellow} se remonta a **Francis Galton**, primo de Charles Darwin, que además de estadístico fue psicólogo, geógrafo y, por desgracia, el primer eugenésico (de hecho acuñó el termino)

. . .

También fue el primero en proponer métodos de clasificación de huellas en medicina forense e incluso se le atribuye el primer mapa meteorológico de la historia

---

## Regresión y Darwin

Galton mostró fascinación por «El origen de la especies» de su primo. Sin embargo, [**Galton no se centraba en los mejor adaptados sino en los que él llama mediocres**]{.hl-yellow}

. . .

Según Galton, las sociedades estaban fomentando la mediocridad, interfiriendo en la selección natural, así que empezó a **estudiar si el talento era o no hereditario**.


. . .

¿Su conclusión? El [**talento se disipaba**]{.hl-yellow} a lo largo de las generaciones

---

## Regresión a la mediocridad

En 1886 publicó «Regression towards mediocrity in hereditary stature», un artículo que cambiaría la estadística: fue el [**primer uso documentado de lo que hoy conocemos como recta de regresión**]{.hl-yellow} 

. . .

:::: columns
::: {.column width="38%"}

![](img/reg_dalton.jpg)

:::

::: {.column width="62%"}

Galton analizó la estatura de 205 hijos y padres, observando que, de nuevo, los valores extremos se disipaban: a lo largo de las generaciones había una [**regresión (un retroceso) a la mediocridad (entendida como la media)**]{.hl-yellow} 


:::
::::

Hijos de altos eran algo más bajitos, e hijos de bajitos eran algo más altos.

---

## Regresión a la mediocridad

:::: columns
::: {.column width="60%"}

![](img/reg-galton-coef.jpg)

:::

::: {.column width="40%"}

Galton no solo observó que las [**estaturas «regresaban» a un valor medio sino que lo hacían con un patrón**]{.hl-yellow}, con un factor constante de $2/3$: si los padres se desviaban $+3$ por encima de la media, los hijos se desviaban solo $(2/3)*3 = +2$ por encima de la media.

:::
::::

---

## Aprendizaje estadístico

La regresión lineal es el modelo más simple de lo que se conoce como [**aprendizaje estadístico (Machine Learning)**]{.hl-yellow}, en concreto del conocido como [**aprendizaje supervisado**]{.hl-yellow}

![](img/ml.jpg)


---

## Ciencia de Datos


La [**ciencia de datos**]{.hl-yellow} es precisamente la rama que integra las matemáticas, la estadística, la probabilidad, el Machine Learning e incluso el Big Data

![](img/stats-ml.jpg)

---

## Aprendizaje ¿supervisado?

![](img/unsupervised-learning.jpg)

---

## Aprendizaje ¿supervisado?

En el campo del Machine Learning hay principalmente dos tipos de metodologías:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje supervisado**]{.hl-purple}: tendremos dos tipos de variables, la [**variable dependiente (output/target)**]{.hl-yellow} que se quiere predecir/clasificar, normalmente denotada como $Y$, y las [**variables independientes (inputs) o explicativas o predictoras**]{.hl-yellow}, que contienen la información disponible. Ejemplos: regresión, knn, árboles, etc.
 

:::

::: {.column width="50%"}

![](img/supervised.jpg)
:::
::::

---

## Aprendizaje ¿supervisado?

En el campo del Machine Learning hay principalmente dos tipos de metodologías:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje no supervisado**]{.hl-purple}: no existe la distinción entre target y variables explicativas ya que [**no tenemos etiquetados los datos**]{.hl-yellow}, no sabemos a priori la respuesta correcta. El aprendizaje no supervisado [**buscará patrones**]{.hl-yellow} basados en similitudes/diferencias. Ejemplos: PCA, clustering, redes neuronales, etc.

:::


::: {.column width="50%"}

![](img/unsupervised.jpg)

:::
::::

---

## Clasificación vs predicción

Como hemos comentado, la [**regresión lineal**]{.hl-yellow} se enmarca dentro del [**predicción supervisada**]{.hl-yellow}

* [**Predicción**]{.hl-purple}: la [**variable objetivo es una variable cuantitativa continua**]{.hl-yellow} (por ejemplo, precio, glucosa, peso, etc).

* [**Clasificación**]{.hl-purple}: la [**variable objetivo es una variable cualitativa**]{.hl-yellow} (por ejemplo, especie de flor, ausencia/presencia de enfermedad, si/no, etc) o **cuantitativa discreta** (por ejemplo, número de accidentes). La etiqueta tomará un valor dentro del conjunto de modalidades permitidas, pudiendo ser **binaria** (si/no) o **multiclase** (A, B, C, D).

&nbsp;

📚 Ver «The elements of Statistical Learning» (Hastie et al., 2008)

# Clase 5: ajuste de regresión {#clase-5}

[**Interpretación de coeficientes. Método mínimos cuadrados**]{style="color:#444442;"}

---

## Modelo predictivo

Dentro del marco de la [**predicción supervisada**]{.hl-yellow} un modelo tendrá siempre la siguiente forma:

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad {\rm E} \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$

* $X$ serán los [**datos**]{.hl-yellow}

* $f(\cdot)$ será nuestro [**modelo**]{.hl-yellow}, es decir, el [**valor esperado de $Y$**]{.hl-yellow} con la información que tenemos.

* $\mathbf{X} = \left(X_1, \ldots, X_p \right)$ serán nuestras [**predictoras o variables independientes**]{.hl-yellow}

* $\varepsilon$ será el [**error o ruido**]{.hl-yellow}, una [**variable aleatoria de media 0**]{.hl-yellow} ${\rm E} \left[\varepsilon | \boldsymbol{X} = x \right] = 0$ (el error debería ser reducido a **algo aleatorio (irreducible)**, aunque en estadística SIEMPRE nos vamos a equivocar).

---

## Modelo predictivo

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad {\rm E} \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$


El objetivo es intentar [**estimar dicha variable objetivo**]{.hl-yellow} minimizando al máximo el error cometido (la parte que podemos evitar).

. . .

El [**modelo estimado**]{.hl-yellow} se definirá como

$$\widehat{Y} := \widehat{{\rm E} \left[Y | \boldsymbol{X} = x \right]}  = \widehat{f}\left(X_1, \ldots, X_p \right), \quad \widehat{\varepsilon} = \widehat{Y} - Y$$

* $\widehat{Y}$ serán las [**estimaciones**]{.hl-yellow}, definidas como la estimación del [**valor esperado de $Y$**]{.hl-yellow} con la información que tenemos.

* $\widehat{f}$ será el [**modelo estimado**]{.hl-yellow}

* $\widehat{\varepsilon}$ los [**errores estimados**]{.hl-yellow} (con media muestral igual a cero).


---

## Reg. lineal univariante

En el caso concreto de la [**regresión lineal**]{.hl-yellow} nuestro modelo será un **hiperplano lineal** (en el caso de una variable, una simple recta):

$${\rm E} \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p$$

. . .

Su estimación será por tanto

$$\widehat{Y} := \widehat{{\rm E} \left[Y | \boldsymbol{X} = x \right]} = \widehat{f}(\mathbf{X}) = \widehat{\beta_0} + \widehat{\beta_1} X_1 + \ldots + \widehat{\beta_p} X_p $$

* El modelo $\widehat{f}(\cdot)$ será una [**recta/hiperplano lineal**]{.hl-yellow}

* En el caso de la regresión lineal [**univariante**]{.hl-yellow} tendremos por tanto ${\rm E} \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1X$ ($p = 1$)

* El objetivo será obtener la estimación de los $\widehat{\beta}$ tal que **minimicemos el error**

---

## Varianza residual

Como ya hemos comentado, una manera de cuantificar lo que nuestro modelo se equivoca es lo que llamamos [**varianza residual o error cuadrático medio**]{.hl-yellow} (varianza del error), que podemos estimar como sigue (ya que la [**media muestral de los errores será cero**]{.hl-purple})

$$s_{r}^{2} = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \left( \widehat{\varepsilon}_{i} - \overline{\widehat{\varepsilon}}_{i}\right)^2 = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} $$
¿Cómo quedaría la fórmula si **desarrollo lo que vale los errores estimados** $\widehat{\varepsilon}$?

. . .


$$s_{r}^{2} := s_{r}^{2}\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)= \frac{1}{n} \sum_{i=1}^{n} \left(Y_i - \widehat{Y}_i \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\widehat{\beta}_0 + \widehat{\beta}_1X_i \right)\right]^2$$

---

## Método mínimos cuadrados

El [**método de los mínimos cuadrados**]{.hl-yellow}, originariamente planteado a la vez por Legendre y Gauss, consiste en establecer que los [**parámetros óptimos**]{.hl-yellow} serán aquellos $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$ que minimicen dicha suma de cuadrados (que minimicen la varianza residual)

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} s_{r}^{2}\left(\beta_0, \beta_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} n*s_{r}^{2}\left(\beta_0, \beta_1 \right)$$

Vamos a desarrollar ese cuadrado...

--- 


## Método mínimos cuadrados

$$s_{r}^{2}\left(\beta_0, \beta_1 \right) = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\beta_0 + \beta_1 X_i \right)\right]^2$$ 
Vamos a desarrollar ese cuadrado...

. . .

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$




---

## Método mínimos cuadrados

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$

¿Cómo encontrar el mínimo de una función?

. . .

Efectivamente, derivando.

$$\frac{\partial s_{r}^{2}}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{0} + 2 \beta_1 X_i - 2Y_i \right] = 2 \left( \beta_{0} + \beta_{1} \overline{x} - \overline{y} \right)$$ 

$$\frac{\partial s_{r}^{2}}{\partial \beta_1} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{1}X_{i}^{2} + 2 \beta_0 X_i - 2Y_iX_i \right] = 2 \left( \beta_{1} \overline{x^2}+\beta_{0}\overline{x} - \overline{xy} \right)$$ 

---

## Método mínimos cuadrados


Si lo agrupamos en un sistema y lo [**igualamos a cero**]{.hl-yellow} para encontrar el óptimo tenemos

$$\left\{\beta_{0} + \beta_{1} \overline{x}   = \overline{y} \atop \overline{x} \beta_{0} + \overline{x^2} \beta_{1} = \overline{xy} \right.$$ 

¿Cómo resolver un sistema?

. . .

Haciendo la regla de Cramer tenemos que

$$\widehat{\beta}_1 = \frac{\overline{xy} - \overline{x}*\overline{y}}{\overline{x^2} - \overline{x}^2} = \frac{s_{xy}}{s_{x}^{2}}$$

$$\widehat{\beta}_0 = \frac{\overline{y}*\overline{x^2} - \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2}  = \overline{y} + \frac{\overline{y}*\overline{x}^2- \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2} = \overline{y} - \widehat{\beta}_1 \overline{x}$$


---

## Estimación reg. univariante


Los [**estimadores de los parámetros de una regresión lineal univariante**]{.hl-yellow} serán por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

. . .

Es importante [**distinguir lo población de lo muestral**]{.hl-yellow}

* Los parámetros $\left( \beta_0, \beta_1 \right)$ son desconocidos, los [**parámetros poblacionales**]{.hl-yellow}

* Los parámetros $\left( \widehat{\beta}_0, \widehat{\beta}_1 \right)$ son estimados a partir de los datos, son [**variables aleatorias**]{.hl-yellow} ya que han sido calculados en función de una muestra aleatoria $\left\lbrace \left(x_i, y_i \right) \right\rbrace_{i=1}^{n}$ de la población $\left(X, Y \right) $

---

## Estimación reg. univariante

Los [**estimadores de los parámetros de una regresión lineal univariante**]{.hl-yellow} serán por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

¿Cuál es su [**interpretación**]{.hl-yellow}?

. . .

* [**Ordenada en el origen**]{.hl-yellow}: también llamado **intercepto**, y denotado como $\beta_0$ su valor real, es el valor de $Y$ cuando $X=0$. Es decir, $\widehat{\beta}_0$ se puede interpretar como la estimación $\widehat{Y}$ cuando $X = 0$ (cuando dicha estimación tenga sentido).


. . .

* [**Pendiente**]{.hl-yellow}: denotado com $\beta_1$ su valor real, cuantifica el incremento de $Y$ cuando $X$ aumenta una unidad. Es decir, $\widehat{\beta}_1$ se puede interpretar como la variación de la **estimación $\widehat{Y}$ cuando $X$ tiene un incremento unitario**.

---

## Regresión en R

Para hacer un [**ajuste de regresión lineal univarainte en R**]{.hl-yellow} solo necesitamos utilizar la función `lm()` (de linear model) con dos argumentos:

* `data = ...`: los datos de los que queremos sacar el ajuste (sin ausentes de momento)
* `formula = ...`: la fórmula del ajuste (y frente a x, expresado como `y ~ x`)

```{r}
datos <- 
  starwars |>
  drop_na(mass, height) 
ajuste_lineal <- lm(data = datos, formula = mass ~ height)
```

---

## Regresión en R

Una vez realizado el ajuste podemos [**resumirlo**]{.hl-yellow} con `summary()`

```{r}
ajuste_lineal |> summary()
```

. . .

¿Qué representa cada bloque de la salida?

---

## Regresión en R

```{r}
ajuste_lineal |> summary()
```

* `Call: ...`: la orden que hemos ejecutado

* `Residuals: ...`: un resumen en forma de [**cuartiles de los residuales**]{.hl-yellow}, es decir, de los [**errores**]{.hl-yellow} (fíjate que aunque la media de los errores siempre es 0, no tiene porque serlo la mediana, de hecho que no lo sea ya nos indica que normales no van a ser seguramente).


---

## Regresión en R

```{r}
ajuste_lineal |> summary()
```


* `Coefficients: ...`: de momento solo nos interesa la columna [**Estimate**]{.hl-yellow} que nos da las estimaciones de los parámetros. En la fila `Intercept` siempre irá $\widehat{\beta}_0$, y el resto de filas tendrá el nombre de la variable predictora a la que multiplica el parámetro (en este caso la fila `height` corresponde a la estimación $\widehat{\beta}_1$).

---


## Regresión en R

```{r}
ajuste_lineal |> summary()
```

En nuestro caso particular, el [**ajuste de regresión**]{.hl-yellow} viene dado por $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1 X = -11.4868 + 0.6240*X$, donde $X$ es la estatura: por cada cm extra que mida un personaje, el modelo estima que el peso aumenta 0.6240 kg, y la estimación de peso para un persona que mida 0 cm es de -11.4868 kg (obviamente esta estimación mucho sentido no tiene)

---

## Regresión en R

```{r}
ajuste_lineal |> summary()
```

* `Multiple R-squared: ...` (de momento lo demás lo ignoramos): es el conocido como $R^2$ o bondad de ajuste. Hablaremos de él en las próximas clases, pero de momento, nos basta saber que es una [**métrica de calidad del ajuste**]{.hl-yellow} que va de 0 (peor) a 1 (mejor).

---

## Predicción en R

Una vez estimado el modelo (sus coeficientes) podemos usarlo, bien para  [**estimar valores que el modelo conoce**]{.hl-yellow} (de los que aprendió) o para [**predecir nuevos valores**]{.hl-yellow} de los que el modelo no ha aprendido

. . .

Simplemente necesitamos la función `predict(modelo, nuevos_datos)`, usando como argumentos el **modelo entrenado** y un [**nuevo dataset que contenga una/s columna/s llamadas igual que nuestras variables predictores**]{.hl-yellow} con los valores de la predictora para los que queremos predecir

```{r}
predicciones <- 
  tibble("height" = c(15, 81.3, 153.1, 189.3, 210.3),
         "mass_predict" = predict(ajuste_lineal, tibble("height" = height)))
predicciones
```

---

## Rango de fiabilidad

Aunque hablaremos en profundidad de la bondad de ajuste, supongamos que tenemos un **modelo extremadamente bueno**, con una bondad de ajuste $R^2 = 0.99$. ¿Sería fiable nuestro modelo anterior para predecir el peso de un personaje con $X = 15$ cm?

. . .

```{r}
predicciones
```

Si te fijas la [**¡predicción del peso es negativa!**]{.hl-red}: por muy bebé que sea, algo pesará. ¿Por qué sucede esto?

---

## Rango de fiabilidad


![](img/rango_1.png)
![](img/rango_2.png)

Nuestro [**modelo solo puede aprender de los datos que conoce**]{.hl-yellow}: solo podremos predecir para valores nuevos de las predictoras [**dentro del rango de las predictoras con las que entrenamos**]{.hl-yellow} el modelo

Da igual lo bueno que sea el modelo: las [**predicciones fuera del rango de las predictoras de entrenamiento**]{.hl-red} no serán fiables ya que el modelo no sabe lo que sucede fuera.

# Clase 6: diagnosis  {#clase-6}

[**Hipótesis e inferencia del modelo**]{style="color:#444442;"}

---

## Repaso


* [**Modelo lineal**]{.hl-yellow}: descrito como $Y = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p X_p + \varepsilon$, buscamos estimar el **valor esperado**  ${\rm E} \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p x_p$, donde $\boldsymbol{X}$ es la población y $x$ la muestra.

. . .

* [**Caso univariante**]{.hl-yellow}: $p = 1$ tal que ${\rm E} \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 x$.

. . .

* [**Estimación**]{.hl-yellow}: $\widehat{Y}$ en función de $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$. De todas las rectas posibles, queremos la [**recta de regresión mínimos-cuadrados**]{.hl-yellow}: **minimiza la varianza residual** (calculamos sus derivadas parciales, igualamos a cero y resolvemos el sistema). Esos estimadores serán $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$  y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$.

. . .

* [**Predicción**]{.hl-yellow}: podemos estimar o predecir valores con la recta $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_0 X$, siempre y cuando $X = x$ esté dentro del rango de valores de entrenamiento del modelo.

---

## Diagnosis

Hasta ahora no hemos pedido nada a la muestra $\left\lbrace \left( X_i, Y_i \right) \right\rbrace_{i=1}^{n}$, ¿para qué necesitaríamos [**hipótesis**]{.hl-yellow} entonces?

. . .

La razón es que, hasta ahora, lo único que hemos podido realizar es una [**estimación puntual**]{.hl-yellow} de los parámetros, pero dado que dichos estimadores serán variables aleatorias, necesitaremos realizar [**inferencia estadística**]{.hl-yellow} sobre ellos (recuerda: los parámetros son simpleme estimaciones para esa muestra de la población, de forma que dada otra muestra, la recta será distinta).

. . .

Para poder cuantificar la [**variabilidad y precisión de nuestras estimaciones**]{.hl-yellow} necesitaremos que los datos cumplan ciertas [**hipótesis probabilísticas**]{.hl-purple}: lo interesante no es la estimación puntual de los parámetros a partir de la muestra sino lo que [**podamos inferir de ellos a la población**]{.hl-green}

---

## Diagnosis

En el caso de la regresión lineal univariante pediremos [**4 hipótesis**]{.hl-yellow}

. . .

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es ${\rm E} \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1 x$

. . .

2. [**Homocedasticidad**]{.hl-green}: el modelo no podrá explicar toda la información (a veces se equivocará por arriba y otras por abajo), pero necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow}, tal que $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \boldsymbol{X} = x \right] = cte < \infty$, que no varíe según aumenta o disminuye la $X$.

. . .

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$

. . .

4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser independientes entre sí (el error en una observación no depende de otras). En particular, serán **incorrelados**

$${\rm Cor}_{\varepsilon_i \varepsilon_j} = {\rm E} \left[\varepsilon_i \varepsilon_j \right] - {\rm E} \left[\varepsilon_i \right] {\rm E} \left[\varepsilon_j \right] = {\rm E} \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$$

---


## Diagnosis


Las 4 hipótesis se pueden [**resumir de manera teórica**]{.hl-yellow} en

$$Y | \boldsymbol{X} = x \sim N \left(\beta_0 + \beta_1x, \sigma_{\varepsilon}^2 \right)$$

. . .

Su [**versión muestral**]{.hl-purple} sería simplemente

$$y_i | \boldsymbol{X} = x_i \sim N \left(\beta_0 + \beta_1x_i, \sigma_{\varepsilon}^2 \right), \quad i=1,\ldots,n$$

---

## Inferencia de los parámetros

Las hipótesis nos permiten decir (lo demostraremos más adelante) que los [**parámetros estimados siguen una distribución (condicionada) normal**]{.hl-yellow} de [**media el parámetro**]{.hl-purple}  a estimar y de [**varianza el conocido como standard error**]{.hl-purple}

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

. . .

Por las propiedades de la normal y el teorema central del límite 

$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)}| \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$

---

## Inferencia de los parámetros


$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

Veremos más adelante porqué pero de momento...estos son los $SE$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

¿Qué **propiedades** tienen estos estimadores?


---

## Inferencia de los parámetros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

* [**Insesgados**]{.hl-yellow}: ambos son [**estimadores insesgados**]{.hl-green} ya que la esperanza de la estimación es el valor a estimar. ${\rm E} \left[ \widehat{\beta}_j \right] = \beta_j$

. . .
 
* [**Precisión vs tamaño muestral**]{.hl-yellow}: cuando $n \to \infty$, sus varianzas tienden a cero, es decir, $SE \left(\widehat{\beta}_j\right)^2 \to^{n \to \infty} 0$ ya que $n$ está dividiendo. Traducción: [**a más datos, mayor precision**]{.hl-green} (menos varianza tendrán los estimadores si repetimos la toma de muestras)


---

## Inferencia de los parámetros


$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$

. . .

* [**Precisión vs var residual**]{.hl-yellow}: cuanto más grande sea la varianza del error $\sigma_{\varepsilon}^{2}$, $SE \left(\widehat{\beta}_j\right)^2$ crecerá (es decir, [**más ruido implicará más imprecisión**]{.hl-red})

. . .

* [**Precisión vs varianza de X**]{.hl-yellow}: cuanto más grande sea la varianza de la predictora $s_{x}^2$, $SE \left(\widehat{\beta}_j\right)^2$ decrecerá, tal que $SE \left(\widehat{\beta}_j\right)^2 \to^{s_{x}^2 \to \infty} 0$. Dicho de otra forma: cuánta [**más información (varianza) contenga nuestra tabla, mayor precisión**]{.hl-green}.

. . .

* [**Precisión vs media X**]{.hl-yellow}: solo afecta a la estimación de $\beta_0$, cuya [**precisión decrece cuando la media aumenta**]{.hl-red}. Esto tiene sentido ya que cuánto más grande en media sean los datos, menos fiable será la predicción para $X=0$.

---

## Inferencia de los parámetros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


Pero tenemos un problema: la [**varianza residual poblacional**]{.hl-yellow} (varianza población del error) $\sigma_{\varepsilon}^{2}$ es [**desconocida**]{.hl-yellow}. Como suele pasar, la [**sustituiremos por un estimador (insesgado)**]{.hl-yellow} de la misma (ya demostaremos...), donde $p$ es el número de variables (lo que R llama `std error` en la salida)

. . .

$$\widehat{\sigma_{\varepsilon}^{2}} = \frac{n}{n-p-1} s_{r}^{2} = \frac{1}{n-p-1} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 =_{p=1} \frac{1}{n-2} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2$$
---

## Inferencia de los parámetros



$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$


¿Qué suele suceder cuando en una normal **desconocemos la varianza real y la sustituimos por su estimador insesgado**?

. . .

Si [**sustituimos el error standard por su estimador**]{.hl-yellow} (sustituyendo la varianza residual por su estimador insesgado), obtenemos que sigue una **t-Student**

$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$
donde $\widehat{SE} \left(\widehat{\beta}_0\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right]$ y $\widehat{SE} \left(\widehat{\beta}_1\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n s_{x}^2}$

---

## Inferencia de los parámetros


```{r}
ajuste_lineal |> summary()
```

* `std error`: es lo que hemos denotado como $\widehat{SE} \left(\widehat{\beta}_j\right)^2$

. . .

* `t value`: es el estadístico $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ que hemos dicho que sigue una t-Student (fíjate que hemos puesto $\beta_j = 0$)

## Inferencia de los parámetros


```{r}
ajuste_lineal |> summary()
```

* `Pr(>|t|)`: representa un p-valor. ¿De qué contraste se te ocurre?

. . .

* `Residual standard error`: es el estimador insesgado de la varianza residual, que hemos denotado como $\widehat{\sigma_{\varepsilon}^{2}}$

---

## Inferencia de los parámetros

:::: columns
::: {.column width="55%"}

$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$

:::

::: {.column width="45%"}

![](img/tablas-intervalos.png)

:::
::::

Si te acuerdas de inferencia, esto implica que los [**intervalos de confianza de nuestros parámetros**]{.hl-yellow} son

$$IC \left(\alpha, \widehat{\beta}_j \right) = \left[\widehat{\beta}_j - t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right), \widehat{\beta}_j + t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right) \right]$$
donde $t_{n-p-1; \alpha/2}$ es el cuantil $1-\alpha/2$ (deja a la izquierda una probabilidad de $1-\alpha/2$).

---


## Paréntesis: intervalos de confianza
confint(
---


## Inferencia de los parámetros

Y si tenemos inferencia, tenemos contrastes: ¿te acuerdas de los p-valores que devuelve la tabla para cada parámetro?

. . .

Para cada parámetro se realiza un [**contraste de significancia**]{.hl-yellow}: ¿cuánta evidencia hay en mis datos para poder decir que el [**valor estimado de mi parámetro es distinto de 0**]{.hl-yellow}?

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

. . .

El estadístico usado es el [**t value**]{.hl-yellow} que devuelve `R` definido como $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ y que, [**BAJO EL SUPUESTO DE QUE LA HIPÓTESIS NULA SEA CIERTA**]{.hl-purple}, sigue una t-Student $t_{n-p-1}$

---

## Inferencia de los parámetros

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

Este contraste significancia en realidad **contesta a la siguiente pregunta**

¿Tiene mi variable $X_j$ un [**efecto lineal SIGNIFICATIVO**]{.hl-yellow} en mi variable $Y$?

. . .

&nbsp;

Esa pregunta puede ser respondida desde un punto de vista **frecuentista** haciendo uso de los [**p-valores**]{.hl-yellow}: si $p-value < \alpha$, entonces rechazamos la nula (rechazamos que la variable no tenga efecto lineal significativo, es decir, aceptamos que sí lo tiene); en caso contrario no podemos rechazar (que **no es lo mismo que aceptarla**, solo que no hay evidencias suficientes en los datos para decir que haya un efecto significativo)

---

## Paréntesis: p-valor

---

## Inferencia de los parámetros



```{r}
ajuste_lineal |> summary()
```

Si te fijas ambos p-valores están por encima de $\alpha = 0.05$ (valor adoptado habitualmente) los que nos dice que [**no hay evidencias en los datos para afirmar que haya efectos lineales significativos**]{.hl-yellow}

. . .

¿Y si probamos a quitar $\beta_0$ (es decir, la respuesta está centrada)?

---

## Inferencia de los parámetros

Para ello basta añadir un `-1` antes de las variables predictoras.

```{r}
lm(data = datos, formula = mass ~ -1 + height) |> summary()
```

Fíjate que ahora, amén que [**hay un efecto lineal significativo de la altura**]{.hl-yellow} y de tener $R^2$ mayor, $\beta_1$ tiene una precisión mayor. En este caso [**solo podía quitar uno**]{.hl-yellow} (perderíamos $X$), pero veremos más adelante cómo decidir cuál quitar si tuviésemos varias variables.

---

siguiente clase 13/02: caso práctico

* Análisis exploratorio (numérico y visualización, corrplot)

* Estimación

* Interpretación coeficientes

* Diagnosis (herramientas en R)


---

siguiente clase 15/02:

* otro caso real

* evaluación (métricas, etc), significado R2

* intervalos predicción (se)

---

siguiente clase 20/02:


ANOVA

caso con todo

---

siguiente clase 22/02:


caso con todo a mano y en R


---

siguiente clase 27/02:

entrega

---

siguiente clase 29/02:

Regresión lineal multivariante

---

# El mundo Github

[**Trabajar ordenados, publicar resultados, replicabilidad de lo realizado**]{style="color:#444442;"}

---

## ¿Qué es Github?

[**GitHub**]{.hl-yellow} es la plataforma colaborativa más conocida basada en el [**sistema de control de versiones Git**]{.hl-yellow}

. . .

-   [**¿Qué es Git?**]{.hl-purple} Git es un sistema de [**control de versiones**]{.hl-yellow}: una especie de [**Dropbox**]{.hl-yellow} para facilitar la [**programación colaborativa**]{.hl-yellow} entre un grupo de personas, permitiendo llevar la [**trazabilidad de los cambios**]{.hl-yellow} realizados.

. . .

-   [**¿Qué es Github?**]{.hl-purple} Nuestra [**plataforma/interfaz**]{.hl-yellow} para ejecutar el control de versiones: nos servirá no solo para trabajar colaborativamente sino para [**hacer transparente**]{.hl-yellow} el proceso de construcción de nuestros proyectos de código.

. . .

::: callout-important
## Importante

Desde el 4 de junio de 2018 Github es de Microsoft (ergo el código que subas también)
:::

---

## Visión general

Tras hacernos una cuenta en Github, [**arriba a la derecha**]{.hl-purple} tendremos un círculo, y haciendo click en [**Your Profile**]{.hl-purple}, veremos algo similar a esto

::: columns
::: {.column width="55%"}
![](img/github_1.png)
:::

::: {.column width="45%"}
-   [**Edit profile**]{.hl-purple}: nos permite añadir una [**descripción y foto de perfil**]{.hl-yellow}.

-   [**Overview**]{.hl-purple}: en ese panel de cuadrados se [**visualizará nuestra actividad**]{.hl-yellow} a lo largo del tiempo.

-   [**Repositories**]{.hl-purple}: el códugo será subido a [**repositorios**]{.hl-yellow}, el equivalente a nuestras carpetas compartidas en Dropbox.
:::
:::

---

## Primer uso: consumidor

Antes de aprender como crear repositorios, Github también nos servirá para

-   [**Acceder a código**]{.hl-purple} ajeno
-   [**Proponer mejoras**]{.hl-purple} a otros usuarios, e incluso proponer [**correcciones de error que detectemos**]{.hl-yellow} de software que usemos

. . .

-   [**Instalar paquetes de R**]{.hl-purple}. En muchas ocasiones los desarrolladores de paquetes suben las actualizaciones a CRAN cada cierto tiempo, y en otras el software no es suficientemente «amplio» para poder ser subido como paquete.

El código de paquetes que no tengamos subido en CRAN podremos [**instalarlo como código desde Github**]{.hl-yellow}

---

## Instalar desde Github

Por ejemplo, vamos a instalar un paquete llamado [`{peRReo}`](https://github.com/jbgb13/peRReo), cuya única función es darnos [**paletas de colores**]{.hl-yellow} basadas en portadas de [**álbumes de música urbana**]{.hl-yellow}

::: columns
::: {.column width="50%"}
![](img/perrreo.png)
:::

::: {.column width="50%"}
![](img/rosalia.png){width="360"}
:::
:::

. . .

Para ello antes tendremos que instalar un [**conjunto de paquetes para desarrolladores**]{.hl-yellow} llamado `{devtools}`, que nos permitirá la instalación desde Github

```{r}
#| eval: false
install.packages("devtools")
```

---

## Instalar desde Github

Las [**instrucciones de instalación**]{.hl-yellow} suelen venir detalladas en la portada del repositorio

::: columns
::: {.column width="50%"}
![](img/install_perreo.png)
:::

::: {.column width="50%"}
![](img/rosalia.png){width="310"}
:::
:::

. . .

En la mayoría de casos bastará con la función `install_github()` (del paquete que acabamos de instalar), pasándole como argumento la [**ruta del repositorio**]{.hl-yellow} (sin "github.com/").

```{r}
#| eval: false
devtools::install_github("jbgb13/peRReo")
```

Ya puedes perrear con ggplot ;)

---

## Descargar desde Github

La mayoría de veces lo que subamos no será un paquete de R como tal sino que [**subiremos un código más o menos organizado**]{.hl-yellow} y comentado. En ese caso podremos [**descargar el repo entero**]{.hl-yellow} haciendo click [**Code**]{.hl-green} y luego Download ZIP.

Por ejemplo, vamos a descargarnos los scripts de dataviz que han subido desde el [Centre d'Estudis d'Opinió](https://github.com/ceopinio/bop-grafics)

![](img/ceo_github.png)

---

## Ideal

![](img/abogados_simpson.jpeg){width="600"}

[**¿Lo ideal en caso de RTVE?**]{.hl-purple} Tener dos tipos de repositorios

-   Una [**colección de repositorios públicos (producción)**]{.hl-yellow} donde hacer transparente el código y los datos ([**ya validados**]{.hl-purple}), coordinado por un nº reducido de personas.

-   Una [**colección de repositorios privados (desarrollo)**]{.hl-yellow} donde esté todo el equipo colaborando y donde se haga el [**trabajo del día**]{.hl-purple}, con trazabilidad interna.

---

## Nuestro primer repositorio

Vamos a [**crear nuestro primero repositorio**]{.hl-yellow} que servirá además como [**carta de presentación**]{.hl-yellow} de nuestro perfil en Github.

1.  [**Repositories**]{.hl-purple}: hacemos click en las pestaña de Repositories.

2.  [**New**]{.hl-purple}: hacemos click en el [**botón verde New**]{.hl-green} para crear un nuevo repositorio

![](img/new_repo.png)

---

## Nuestro primer repositorio

-   [**Repository name**]{.hl-purple}: el [**nombre del repositorio**]{.hl-yellow}. En este caso vamos a crear un repositorio muy concreto: el nombre debe [**coincidir exactamente con tu nombre de usuario**]{.hl-yellow}

-   [**Description**]{.hl-purple}: descripción de tu repositorio. En este caso será un repo de presentación.

![](img/repo_init_1.png)

---

## Nuestro primer repositorio

-   [**Public vs private**]{.hl-purple}: con cada repositorio tendremos la opción de hacer el repositorio

    -   [**público**]{.hl-purple}: todos los usuarios podrán ver el código así cómo la trazabilidad de su desarrollo (qué se añade y cuándo). Es para mí la opción más recomendable cuando quieres darle [**visibilidad y transparencia**]{.hl-yellow} a tu trabajo
    -   [**privado**]{.hl-purple}: solo tendrán acceso al repositorio aquellos usuarios a los que se lo permitas. No se podrá visualizar ni instalar nada de él fuera de Github.

![](img/repo_init_2.png)

En este caso concreto, dado que será un repositorio de presentación, lo [**haremos público**]{.hl-yellow}.

---

## Nuestro primer repositorio

-   [**Add a README file**]{.hl-purple}: un README file será el archivo donde incluiremos las [**instrucciones y detalles de uso**]{.hl-yellow} a los demás (en el caso de `{peRReo}` era el archivo que contenía los detalles de instalación)

![](img/repo_init_3.png)

De momento [**ignoraremos los demás campos**]{.hl-red} para este primer repositorio.

---

## Nuestro primer repositorio

![](img/repo_init_1.png)

Por defecto Github asume que este repositorio, con el [**mismo nombre que nuestro usuario**]{.hl-yellow} será el repositorio que querremos que se presente de inicio cuando alguien entra en nuestro perfil, y será el repositorio donde \[**incluir en el README.md**\] una presentación de nosotros y un índice de tu trabajo (si quieres).

---

## Nuestro primer repositorio

![](img/profile_github_md.png)

Fíjate que ahora en nuestra [**portada tenemos dicho README.md**]{.hl-yellow} que podemos personalizar a nuestro gusto haciendo uso de [**html y markdown**]{.hl-yellow}.

Aquí puedes ver [**algunos ejemplos de README.MD**](https://github.com/matiassingers/awesome-readme)

---

## Repo de código

Una vez que tenemos nuestro README de presentación (recuerda que puedes [**personalizar a tu gusto con html y markdown**]{.hl-yellow}) vamos a crear un [**repositorio de código**]{.hl-yellow}.

. . .

Si ya era importante [**trabajar con proyectos**]{.hl-yellow} en `RStudio`, cuando lo combinamos con Github es aún más crucial que creemos un proyecto antes de subir el código, así que vamos a crear uno de prueba que se llame `repo-github-1`.

. . .

En dicho proyecto vamos a [**crear un script**]{.hl-yellow} (en mi caso llamado **codigo.R**) en el que deberás hacer los siguientes pasos:

---

## Repo de código

1.  [**Carga**]{.hl-yellow} directamente desde la página del [ISCIII](https://cnecovid.isciii.es/covid19/resources) el archivo llamado `casos_hosp_uci_def_sexo_edad_provres.csv`

```{r}
#| eval: false
#| code-fold: true
# Carga de datos desde ISCIII
datos_covid <- read_csv(file = "https://cnecovid.isciii.es/covid19/resources/casos_hosp_uci_def_sexo_edad_provres.csv")
```

. . .

2.  [**Filtra**]{.hl-yellow} datos de Madrid (`"M"`), de 2020 y con sexo conocido (hombre/mujer). Tras ello quédate con las columnas `fecha`, `sexo`, `grupo_edad`, `num_casos` (ese orden). Por último obtén la suma de casos diarios por fecha y sexo.

```{r}
#| eval: false
#| code-fold: true
# Depuración
datos_madrid <-
  datos_covid |>
  # Filtrado por Madrid y fecha
  filter(provincia_iso == "M" & fecha <= "2020-12-31" & sexo != "NC") |> 
  # Selección de columnas
  select(provincia_iso:fecha, num_casos) |> 
  # Resumen de casos diarios por fecha y sexo
  summarise(num_casos = sum(num_casos), .by = c(fecha, sexo))
  
```

---

## Repo de código

3.  [**Exporta el dataset a un csv**]{.hl-yellow} en una carpeta que se llame `exportado`

```{r}
#| eval: false
#| code-fold: true
# Exportamos datos
write_csv(datos_madrid, file = "./exportado/datos_madrid.csv")
```

. . .

::: columns
::: {.column width="40%"}
4.  Crea una [**gráfica de líneas**]{.hl-yellow} que tenga en el eje x fecha, en el eje y casos, con una curva por sexo (gráficas separadas).

```{r}
#| eval: false
#| code-fold: true
# Gráfica
ggplot(datos_madrid) +
  geom_line(aes(x = fecha, y = num_casos, color = sexo),
            alpha = 0.6, linewidth = 0.7) +
  scale_color_manual(values = c("#85519D", "#278862")) +
  facet_wrap(~sexo) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
:::

::: {.column width="60%"}
![](./img/ggplot.png){width="380"}
:::
:::

. . .

5.  Tras ello [**exportamos la gráfica**]{.hl-yellow}

```{r}
#| eval: false
ggsave(filename = "./exportado/ggplot.png", plot = last_plot(),
       bg = "white", width = 12, height = 8)
```

---

## Repo de código

¿Cómo [**subimos el proyecto**]{.hl-yellow}? Vamos de nuevo a [**crear un proyecto de cero**]{.hl-yellow}. Antes no hemos hablado de dos campos importantes:

::: columns
::: {.column width="50%"}
![](./img/repo-1.png){width="460"}
:::

::: {.column width="50%"}
-   `Add .gitignore` nos permitirá [**seleccionar el lenguaje**]{.hl-yellow} en el que estará nuestro proyecto para que Github lo entienda al sincronizar (y no actualice cosas que no deba).

-   `Choose a license` nos permitirá [**seleccionar la licencia**]{.hl-yellow} que determinará las condiciones en las que otros podrán reusar tu código.
:::
:::

---

## Repo de código

::: columns
::: {.column width="50%"}
![](./img/repo-1-1.png)
:::

::: {.column width="50%"}
Si te fijas traer crearlo tenemos solo 3 archivos: el de licencia, el .gitignore y el readme.md (donde deberíamos escribir una guía de uso de lo que hayamos subido)
:::
:::

Para subir los archivos vamos a clickar en [**Add file \< Upload File**]{.hl-purple} y [**arrastraremos TODOS los archivos**]{.hl-yellow} de la carpeta de nuestro proyecto.

---

## Repo de código

Tras la subida de archivos tendremos un cuadro llamado [**Commit changes**]{.hl-purple}

![](./img/commit-1.png)

Un [**commit**]{.hl-purple} es una [**modificación del repositorio**]{.hl-yellow} con algo que se añade/elimine/modifique, y dicho cuadro es recomendable usarlo para [**resumir en qué consiste la modificación**]{.hl-yellow}, de manera que quede trazado el cambio.

---

## Repo de código

Haciendo click en el reloj donde indica el [**número de commits**]{.hl-yellow} accedemos al [**histórico de commits (cambios)**]{.hl-yellow} con hora, día, autor, comentarios, etc.

![](./img/commit-reloj.png)

---

## Repo de código

Vamos a realizar un [**cambio en nuestro código**]{.hl-yellow}: en tu código local (local --\> tu ordenador), en lugar de filtrar por Madrid haz el [**filtro por Barcelona**]{.hl-yellow}, guarda el código y sube en el repositorio el nuevo archivo (con el mismo nombre, Github hará la sobrescritura)

```{r}
#| eval: false
#| code-line-numbers: "3"
datos_bcn <-
  datos_covid |>
  filter(provincia_iso == "B" & fecha <= "2020-12-31" & sexo != "NC") |> 
  select(fecha, sexo, grupo_edad, num_casos) |> 
  summarise(num_casos = sum(num_casos), .by = c(fecha, sexo))
```

---

## Consulta de commits

![](./img/barcelona_covid.png){width="550"}

::: columns
::: {.column width="40%"}
Si ahora [**consultamos el commit**]{.hl-yellow}, al lado hay un número que lo identifica, y clickando en él nos resume los cambios: no solo [**almacena todas las versiones pasadas**]{.hl-yellow} sino que además nos [**muestra las diferencias entre los archivos cambiados**]{.hl-yellow}
:::

::: {.column width="60%"}
![](./img/commit_cambio.png)
:::
:::

---

## Trazabilidad de cambios

::: columns
::: {.column width="72%"}
![](./img/commit_split.png) ![](./img/commit_unified.png)
:::

::: {.column width="28%"}
Tenemos [**dos modos de visualización**]{.hl-yellow} de los cambios: el modo split nos muestra el antiguo y el nuevo, con las inclusiones en verde y lo que ya no está en rojo; y el modo unified nos muestra todo en un mismo documento.
:::
:::

---

## Recuperación de commits

![](./img/browse-repo.png)

Github nos permite incluso [**recuperar una versión del pasado**]{.hl-yellow} de nuestro repositorio, haciendo click en el tercer icono del commit.

---

## Recuperación de commits

![](./img/branch-commit.png)

Si te fijas ahora al lado de `1 branch` tenemos un [**menú desplegable**]{.hl-yellow} en el que antes ponía `main` y ahora un número identificador del commit. Ya hablaremos de la idea de [**rama (branch)**]{.hl-yellow}

---

## Repo con rmd/qmd

::: columns
::: {.column width="60%"}
Vamos a poner en práctica lo aprendido:

1.  Crea un nuevo repositorio en Github (llamado `repo-github-2`) donde habrá alojado con proyecto de R.

2.  Crea un proyecto en `RStudio` que se llame (por ejemplo) `proyecto-qmd`

3.  Una vez dentro del proyecto en `RStudio` haz click en `File < New File < Quarto Document`
:::

::: {.column width="40%"}
![](./img/primer-qmd.png)
:::
:::

Deberás tener un documento similar a este: un [**quarto markdown (.qmd)**]{.hl-yellow}, un documento que nos permitirá incluir [**markdown + código**]{.hl-yellow} (puede ser `R` o puede ser `Observable`, `D3`, etc).

---

## Repo con rmd/qmd

Este formato es ideal para:

-   [**Trabajar en equipo**]{.hl-yellow} construyendo el borrador de una pieza.
-   Tomar [**apuntes o informes**]{.hl-yellow} para uno mismo.
-   [**Presentar**]{.hl-yellow} tu trabajo a tus compañeros.

::: columns
::: {.column width="50%"}
![](./img/prueba-qmd-html.png)
:::

::: {.column width="50%"}
Si te fijas ahora nuestro repositorio tiene un archivo con formato `.html`...es decir...

[**¡Es una web!**]{.hl-yellow}
:::
:::

---

## Github pages

¿Cómo [**convertir nuestro repositorio en una web**]{.hl-yellow}?

![](./img/github-pages.png)

1.  Haz click en `Settings`
2.  Ve al apartado `Pages`
3.  En el subapartado `branch` selecciona la única rama que tenemos ahora (`main`)
4.  Selecciona la carpeta donde tengas el `.html` (en web complejas estará como en cualquier web en `docs`, en algo simple estará en la ruta raiz del repositorio)
5.  Haz click en `Save`

---

## Github pages

Si te fijas en la [**parte superior del repositorio**]{.hl-yellow} ahora tenemos un icono naranja, que nos indica que la [**web está en proceso de ser desplegada (deploy)**]{.hl-orange}

![](./img/github-naranja.png)

---

## Github pages

Pasados unos segundos (dependiendo del tamaño de la web y tu conexión a internet) ese [**icono pasará a ser un check verde**]{.hl-green}: habemus web

![](./img/github-verde.png)

El [**link de la web por defecto**]{.hl-yellow} será `{nombre_usuario}.github.io/{nombre_repo}`

---

## Github pages

![](./img/github-pages-deploy.png)

¡Un momento! Ahora mismo nuestra web [**no nos está mostrando nuestro .qmd**]{.hl-red}, sino por defecto el [**README.md**]{.hl-yellow}.

. . .

Para que Github entienda que queremos visualizar ese `.html` que hemos generado a partir del `.qmd` vamos en nuestro proyecto local a [**borrar**]{.hl-yellow} todo lo que no sea nuestro archivo `.Rproj` y nuestro archivo `.qmd`, y vamos a [**cambiar el nombre**]{.hl-yellow} a este último llamándolo `index.qmd`, y volvemos a compilarlo para [**generar un index.html**]{.hl-yellow}

---

## Github pages

Vamos a [**subir a Github ese nuevo proyecto**]{.hl-yellow} con el cambio de nombre (llamado `repo-github-3`) para ver luego las diferencias entre uno y otro

![](./img/repo-index-html.png)

---

## Github pages

Si [**repetimos el proceso para hacer una Page**]{.hl-yellow} y esperamos al tick verde...

::: columns
::: {.column width="50%"}
![](./img/index-html-qmd.png)
:::

::: {.column width="50%"}
Si a tu `.qmd` ya le llamas de inicio `index.qmd`, automáticamente, al detectar Github un `index.html`, interpreta que ese [**archivo index.html**]{.hl-yellow} es el que define la web (y puedes personalizar añadiendo un archivo `css` de estilos)

[**Habemus web**]{.hl-green} simplemente clickando en Pages :)
:::
:::

---

## Repo con diapositivas

Vamos a crear el último repositorio que se llamará `repo-diapos`, y crear un proyecto en `RStudio` del mismo nombre (por ejemplo). Una vez creado le daremos a `File < New File < Quarto Presentation`.

::: columns
::: {.column width="45%"}
![](img/quarto-slides.png)
:::

::: {.column width="55%"}
La forma de escribir será igual que un `.qmd` normal solo que ahora [**cada diapositiva la separaremos**]{.hl-yellow} con un `---` (usando archivos de estilos podemos personalizar lo que queramos)

Llama al archivo directamente `index.qmd`, súbelo a Github y con un click en Pages tienes una [**web con tus diapositivas**]{.hl-yellow}
:::
:::

---

## Uso de Gitkraken

La forma más [**sencilla para trabajar de manera colaborativa**]{.hl-yellow} en Github, y tenerlo sincronizado con nuestro local, es hacer uso de [Gitkraken](https://www.gitkraken.com/download)

::: columns
::: {.column width="50%"}
![](img/gitkraken-repo.png)
:::

::: {.column width="50%"}
Una vez dentro clickamos en el icono de la carpeta (`Repo Management`) y si ya tenemos el repositorio en Github seleccionamos `Clone`, indicando donde queremos clonar (en nuestro local) y que [**repositorio de Github queremos clonar**]{.hl-yellow}.
:::
:::

---

## Uso de Gitkraken

::: columns
::: {.column width="50%"}
![](img/view-change.png)
:::

::: {.column width="50%"}
Una vez clonado, la idea es que cada [**cambio que hagamos en local nos aparecerá en Gitkraken**]{.hl-yellow} como `View changes`.
:::
:::

---

## Uso de Gitkraken

Cuando tengas suficientes cambios como para [**actualizar el repositorio**]{.hl-yellow} (tampoco tiene sentido actualizar con cada edición), verás algo similar a esto con todos los [**commits realizados**]{.hl-yellow}

::: columns
::: {.column width="50%"}
![](img/stage-all-changes.png)
:::

::: {.column width="50%"}
Podrás decidir cuáles de los [**commits locales quieres incluir en remoto**]{.hl-yellow}, bien uno a uno o en `Stage all changes` (para todos)
:::
:::

---

## Uso de Gitkraken

Tras incluir los commits deberás incluir un [**título y descripción del commit**]{.hl-yellow}

![](img/titulo-commit.png)

---

## Uso de Gitkraken

Tras hacerlo verás que ahora tenemos [**dos iconos separados en una especie de árbol**]{.hl-yellow} (¿te acuerdas de la `branch` o rama?):

-   [**Ordenador**]{.hl-purple}: la versión del repositorio que tienes en tu [**ordenador**]{.hl-yellow}.

-   [**Logo**]{.hl-purple}: la versión del repositorio que tienes [**subida en remoto**]{.hl-yellow}

![](img/split-gitkraken.png)

---

## Uso de Gitkraken

Mientras eso suceda solo tendrás sincronizado tu ordenador con Gitkraken, pero no con Github. Para ello haremos [**click en Push**]{.hl-yellow} (con `Pull` podrás forzar a tener en local lo mismo que en remoto).

![](img/push-gitkraken.png)

---

## Branchs

Como hemos mencionado ya en varias ocasiones, hay un elefante en la habitación que aún no hemos mentado: las [**ramas o branchs**]{.hl-yellow} de un repositorio.

. . .

Imagina que estáis trabajando varios en un proyecto y teneís una versión que funciona pero que queréis [**modificar en paralelo a partir del estado actual**]{.hl-yellow} del repositorio.

. . .

Las [**ramas**]{.hl-yellow} nos permiten partir de una versión común del repositorio y hacer cambios que [**no afecten a los demás**]{.hl-yellow}

---

## Branchs

Para [**crear una rama**]{.hl-yellow} a partir del estado actual de repositorio haremos click en `Branch` y le pondremos un nombre

![](img/branch-button.png)

Una vez creada verás [**dos iconos**]{.hl-yellow} y un [**menú desplegable**]{.hl-yellow} con las distintas ramas en las que quieres hacer el commit. Imagina que realizas un cambio pero [**no quieres añadirlo a la rama principal**]{.hl-yellow}: puedes hacer el [**commit en tu rama propia en LOCAL**]{.hl-yellow} (lo harás en la rama activa de tu menú de branchs).

---

## Branchs

La primera vez te pedirá que escribas la [**rama en REMOTO**]{.hl-yellow} con la quieres sincronizar tu rama en local. [**Consejo**]{.hl-green}: ponle el mismo nombre en remoto que en local.

![](img/name-branch.png)

---

## Branchs

Fíjate que ahora tenemos el ordenador y el logo en el mismo sitio. Esto no significa que tengas ambas ramas en tu local, solo que [**Gitkraken tiene ambas sincronizadas**]{.hl-yellow}: clickando en cualquiera de ellas, tus archivos en tu ordenador cambiarán.

![](img/both-branchs.png)

---

## Pull request

Lo más recomendable es que [**solo se incorpore de una rama secundaria**]{.hl-yellow} a la rama principal aquello que está [**validado por un/a coordinador/a**]{.hl-yellow} del repositorio, asegurándose que todo funciona correctamente.

Cuando queramos incluirlo haremos [**click con botón derecho**]{.hl-yellow} en el icono de la rama secundaria y seleccionamos `Start a pull request to origin from...`

![](img/pull-request-menu.png)

. . .

Una [**pull request**]{.hl-yellow} será una [**petición al responsable de la rama principal**]{.hl-yellow} para incluir los cambios

---

## Pull request

::: columns
::: {.column width="60%"}
![](img/create-pull-request.png)
:::

::: {.column width="40%"}
En el cuadro que no se abre deberemos escribir:

-   La [**rama**]{.hl-yellow} a la que hacer el `merge` (normalmente la `main`)
-   Título y resumen de los cambios
-   Puedes incluso asignar un [**revisor**]{.hl-yellow} entre los colaboradores del repo.
-   Puedes asignar [**etiquetas**]{.hl-yellow}
:::
:::

---

## Pull request

Mientras no se acepte aparecerá un [**icono de rama**]{.hl-yellow} y un +1 en Pull Requests

![](img/pending-branch.png){width="500"}

. . .

Si somos al mantenedor del repositorio, haciendo click en el menú nos saldrán las ramas que nos quieren hacer hacer `merge`

![](img/menu-pr.png){width="500"}

---

## Pull request

Al hacer click se abrirá un [**cuadro de Pull Request**]{.hl-yellow} para decidir si

-   [**Revisar**]{.hl-yellow} los cambios
-   [**Aprobar**]{.hl-yellow} el `merge`
-   [**Añadir comentarios**]{.hl-yellow} al que ha solicitado el `merge` por si queremos solicitar algún cambio [**antes de ser aprobado**]{.hl-yellow}

## ![](img/menu-pr-2.png)

## Pull request

Tras revisar todo y aprobarlo clickaremos en `Confirm merge`, y tras ello podremos decidir si esa rama que era paralela a la principal la queremos [**eliminar**]{.hl-yellow} o dejar visible a todos (consejo: dejar visible para tene [**trazabilidad**]{.hl-yellow} del proyecto de trabajo)

::: columns
::: {.column width="50%"}
![](img/merge-branch.png)
:::

::: {.column width="50%"}
![](img/delete-branch.png)
:::
:::

