---
title: "Aprendizaje Supervisado I"
subtitle: "M√©todos de predicci√≥n lineal"
title-slide-attributes:
  data-background-image: img/data-science-2.jpeg
  data-background-size: cover
  data-background-opacity: "0.2"
author: "Grado en Ciencia de Datos Aplicada ‚Ä¢ curso 2023-2024"
affiliation: Facultad de Estudios Estad√≠sticos (UCM)
lang: es
language: custom_lang.yml
format: 
  revealjs:
    theme: [default, style.scss]
    chalkboard: true
    multiplex: true
    menu:
      side: left
      width: normal
    footer: "[<strong>Javier √Ålvarez Li√©bana</strong>](...) ‚Ä¢ Grado en Ciencia de Datos Aplicada (UCM) ‚Ä¢ curso 2023-2024"
    slide-number: c/t
execute:
  echo: true
---

# Empieza lo bueno...empieza la modelizaci√≥n

[**Vamos a juntar las piezas del puzzle para hacer ¬´magia¬ª**]{style="color:#444442;"}

```{r}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
```

---

## ¬°Buenas!

[**Correo**]{.hl-green}: **<javalv09@ucm.es>**. [**Despacho**]{.hl-green}: 722 (3¬™ planta). [**Tutor√≠as**]{.hl-green}: lunes (14:30-16:00), martes (13:00-15:00) y viernes (13:00-14:00).

::: columns
::: {.column width="30%"}
![](img/me.jpeg)
:::

::: {.column width="70%"}
::: incremental
-   [**Javier √Ålvarez Li√©bana**]{.hl-yellow}, de Carabanchel (Bajo).

-   Licenciado en Matem√°ticas (UCM). [**Doctorado en estad√≠stica**]{.hl-yellow} (UGR).

-   Encargado de la [**visualizaci√≥n y an√°lisis de datos covid**]{.hl-yellow} del Principado de Asturias (2021-2022).

-   Miembro de la [**Sociedad Espa√±ola de Estad√≠stica e IO**]{.hl-yellow} y la [**Real Sociedad Matem√°tica Espa√±ola**]{.hl-yellow}.
:::
:::
:::

. . .

Actualmente, [**investigador y docente en la Facultad de Estad√≠stica de la UCM**]{.hl-yellow}. Divulgando por [**Twitter**](https://twitter.com/dadosdelaplace) e [**Instagram**](https://instagram.com/javieralvarezliebana)


---

## Objetivos

::: columns
::: {.column width="37%"}
![](https://assets-global.website-files.com/6092cb6b4ac959f39728dd26/6188a97fa499b5fbfe410417_target%20(1).png)
:::

::: {.column width="63%"}
::: incremental
- Empezar a [**relacionar asignaturas**]{.hl-yellow} como matem√°ticas, inferencia y R.

- Aprender los fundamentos del [**aprendizaje estad√≠stico**]{.hl-yellow} (ahora llamado Machine Learning o data science)

- Pasar de los descriptivo a lo predictivo: [**construir nuestros primeros modelos**]{.hl-yellow}

- Entender en profundidad el contexto de la [**predicci√≥n lineal**]{.hl-yellow}.


:::
:::
:::

---

## Evaluaci√≥n

-   [**Asistencia**]{.hl-yellow}. Se [**valorar√° muy positivamente**]{.hl-purple} la participaci√≥n. Si se [**restar√°n puntos si eres expulsado de clase**]{.hl-purple}: -0.3 la 1¬™ vez, -0.6 la 2¬™, -1.2 la 3¬™...

. . .

- [**Evaluaci√≥n continua**]{.hl-yellow}: 1 **examen a papel** (25%), **2 entregas R en clase** (20%-30%) y **entrega grupal √∫ltimo d√≠a** (3-6p obligatoriamente, 25%)
  
. . .

- [**Examen final**]{.hl-yellow}:
  - [**Menos de un 6 (sin la grupal)**]{.hl-purple} -> examen de R en lugar de la grupal.
  - [**M√°s de un 6.5 (con la grupal)**]{.hl-purple} -> podr√°s decidir **peso del final entre un 20% y un 100%**.
  - [**Entre 5 y 6.5 (con la grupal)**]{.hl-purple} -> podr√°s decidir **peso del final entre un 40% y un 100%**.
  - [**Por debajo de 5 (con la grupal)**]{.hl-purple} -> podr√°s decidir **peso del final entre un 70% y un 100%**.
  

---

## Planificaci√≥n


* [**Entrega I (20%)**]{.hl-yellow}: semana 27-29 de febrero de 2024

* [**Entrega II (30%)**]{.hl-yellow}: semana 2-4 de abril de 2024

* [**Parcial con papel y boli (25%)**]{.hl-yellow}: 16-18 de abril de 2024 

* [**Entrega grupal (25%)**]{.hl-yellow}: 9 de mayo de 2024 (incluyendo lo visto hasta el 30 de abril)

&nbsp;

(se admiten propuestas)

. . . 

[**Clase a recuperar otro d√≠a**]{.hl-yellow}: clase del 21 de marzo.

---

## Materiales

* [**Diapositivas**]{.hl-yellow}: las diapositivas que usaremos en el aula a lo largo del curso, estructuradas por clases, estar√°n disponibles y actualizadas en **<https://javieralvarezliebana.es/docencia-R-supervisado-2324/diapos>** 

En el men√∫ de las diapositivas (abajo a la izquierda) tienes una [**opci√≥n para descargarlas en pdf**]{.hl-yellow} en `Tools` (consejo: no lo hagas hasta el final del curso ya que ir√°n modific√°ndose)
  
&nbsp;

* [**Material**]{.hl-yellow}: [**scripts de cada tema**](https://github.com/dadosdelaplace/docencia-R-supervisado-2324/tree/main/material) y materiales extras

* [**Res√∫menes de paquetes**]{.hl-yellow}: [**chuletas de los paquetes**](https://github.com/dadosdelaplace/docencia-R-supervisado-2324/tree/main/fichas%20paquetes) en formato .pdf

---


## Requisitos

Para el curso los √∫nicos requisitos ser√°n:

1.  [**Conexi√≥n a internet**]{.hl-yellow} (para la descarga de algunos datos y paquetes).

2.  [**Instalar R y RStudio**]{.hl-yellow}: la descarga la haremos (gratuitamente) desde <https://cran.r-project.org/> y  <https://posit.co/download/rstudio-desktop/>

3. Se dar√°n por asumido conocimientos aprendidos de [**R base, tidyverse y ggplot**]{.hl-yellow}

4. Se dar√°n por asumido conocimientos aprendidos de [**Quarto, diapositivas en Quarto y Github**]{.hl-yellow}. Para las entregas [**SOLO SE VALORAR√Å**]{.hl-purple} la salida html correspodiente.

5. [**Recomendable**]{.hl-yellow}: saber usar la calculadora en modo estad√≠stico.


---

## Listado de clases

* [**Clase 1**](#clase-1): repaso de descriptiva
* [**Clase 2**](#clase-2): importancia de la visualizaci√≥n
* [**Clase 3**](#clase-3): intro al aprendizaje estad√≠stico. Sesgo vs varianza. Supervisado vs no supervisado. Correlaci√≥n vs dependencia vs causalidad.


# Clase 1: repaso {#clase-1}

[**Objetivo de la predicci√≥n lineal. Concepto de linealidad. Repaso de estad√≠stica descriptiva**]{style="color:#444442;"}

---

## ¬øQu√© es predecir?

Como veremos m√°s adelante, en esta asignatura vamos a tratar principalmente lo que se conoce el aprendizaje estad√≠stico como [**predicci√≥n (continua)**]{.hl-yellow}

. . .

Dada una [**variable objetivo (variable dependiente)**]{.hl-yellow}, y con la informaci√≥n aportada por un conjunto de [**variables predictoras (covariables)**]{.hl-yellow}, el objetivo ser√° construir un modelo que consiga dar una estimaci√≥n/predicci√≥n lo ¬´mejor posible¬ª

. . .

Es importante que - de momento - distingamos dos conceptos:

* [**Estimaci√≥n**]{.hl-yellow}: el modelo aprende de unos datos e intenta estimar dichos valores que ha usado.
* [**Predicci√≥n**]{.hl-yellow}: el modelo aprende de unos datos e intenta estimar valores que el **modelo no conoce**.

M√°s adelante los llamaremos ¬´predicci√≥n en train¬ª y ¬´predicci√≥n en test¬ª

---

## ¬øQu√© es la linealidad?

En matem√°ticas decimos que una funci√≥n $f(x)$ es [**lineal**]{.hl-yellow} cuando se cumple:

* [**Propiedad aditiva**]{.hl-purple}: $f(x + y) = f(x) + f(y)$

* [**Propiedad homog√©nea**]{.hl-purple}: $f(k*x) = k*f(x)$ (donde $k$ es una constante en $\mathbb{R}$).

Ambas se pueden resumir en $f(a*x + b*y) = a*f(x) + b*f(y)$

. . .

En estad√≠stica llamamos [**modelo de predicci√≥n lineal**]{.hl-yellow} a un modelo que usa la informaci√≥n de covariables $X_1, X_2, \ldots, X_p$, de manera que su informaci√≥n siempre [**se relacionen entre s√≠ con sumas y restas**]{.hl-yellow}.

- [**Ejemplos lineales**]{.hl-green}: $y = 2*x_1 - 3$ o  $y = 4 - \frac{x_1}{2} + 3*x_2$

- [**Ejemplos no lineales**]{.hl-red}: $y = 2*\frac{1}{x_1}$ o  $y = 4 - x_{1}^{2} - x_2$ o $y = ln(x_1) + cos(x_2)$

En general son modelos que se pueden representar con rectas (una sola predictora), planos (dos predictoras), etc.

---

## Repaso: continua vs discreta

* **Caracteres**: cada una de las caracter√≠sticas o cualidades que se podr√≠an medir o analizar para cada individuo de la poblaci√≥n (y de los que disponemos el valor para cada individuo de la muestra).

* **Modalidades**: diferentes valores que puede adoptar una caracter√≠stica o variable.

&nbsp;

Como veremos m√°s adelante, en el √°mbito del aprendizaje estad√≠stico va a ser fundamental tener clara la diferencia entre una [**variable cualitativa y cuantitativa**]{.hl-purple} y entre [**variable continua y otra discreta**]{.hl-yellow}. ¬øCu√°l es la diferencia?


---

## Repaso: continua vs discreta

Imagina las siguientes variables:

- ¬øTienes hermanos? (si/no)
- Color de zapatillas
- Nivel de estudios
- N√∫mero de hermanos
- N√∫mero de pelos en la cabeza
- Resultado de un dado
- Temperatura ¬∫C
- Estatura o peso

&nbsp;

[**¬øCu√°l es la diferencia entre ellas**]{.hl-yellow}


---

## Repaso: continua vs discreta



* [**Cualitativas**]{.hl-yellow}: **cualidades o categor√≠as**. Ejemplos: sexo, estado civil, estudios, etc.
  * [**Ordinales**]{.hl-purple}: admiten relaci√≥n jerarquica (suspenso-aprobado-bien, sano-leve-grave).
  * [**Nominales**]{.hl-purple}: no tienen asociada una jerarqu√≠a (sexo, religi√≥n, color de zapatillas, etc).

. . .

* [**Cuantitativas**]{.hl-yellow}: cuantificaci√≥n num√©rica.
  * [**Discretas**]{.hl-purple} (barras): se pueden contar y enumerar (aunque sean infinitos) (granos de arena, n¬∫ hermanos, etc) ‚Üí se **suman**
  * [**Continuas**]{.hl-purple}: toman infinitos valores y entre dos valores cualesquiera hay a su vez infinitas opciones (estatura, peso, etc) ‚Üí se **integran**

---

## Repaso: continua vs discreta

![](img/vitro-fuego-discretas.jpg)


---

## Repaso: poblaci√≥n vs muestra

En estad√≠stica llamaremos [**poblaci√≥n**]{.hl-yellow} al universo te√≥rico, al colectivo de individuos a estudiar, o de posibles elementos/eventos de los podr√≠amos tener observaciones (ejemplo: 47 millones de espa√±oles).

. . .

Cada uno de esos elementos o eventos se llaman **individuos**.

. . .

Aunque nuestro objetivo ser√° conocer algunas de las propiedades de la poblaci√≥n, la [**poblaci√≥n suele ser inaccesible**]{.hl-red} en su totalidad ‚Üí [**SELECCI√ìN**]{.hl-green} de un conjunto de individuos

---

## Repaso: poblaci√≥n vs muestra

Para ello en estad√≠stica usamos lo que se conoce como [**muestra**]{.hl-yellow}: un subconjunto finito, de tama√±o $n$, ¬´representativo¬ª de la poblaci√≥n (en estudio estad√≠stico realizado sobre la totalidad de una poblaci√≥n se denomina **censo**).

. . .

Para ello existe una rama conocida como [**muestreo**]{.hl-yellow}

* [**Probabil√≠stico**]{.hl-purple}: todos los individuos tienen oportunidad de ser seleccionados, y la probabilidad de que suceda puede ser modelizada (al azar, por estratos, etc).

* [**No probabil√≠stico**]{.hl-purple}: algunos elementos de la poblaci√≥n no tienen posibilidad de selecci√≥n (sesgo de exclusi√≥n), o su probabilidad no puede ser conocida.

. . .

ü§î ¬øSer√≠a adecuado hacer una encuesta sobre el streamer favorito de los j√≥venes a trav√©s de una encuesta realizada por tel√©fono fijo?


---

## Sesgos en el muestreo

![](img/sampling-bias.jpg)

. . .

[**Sesgo de selecci√≥n**]{.hl-yellow}: aparece cuando no se tiene en cuenta la forma en la que se han recogido los datos.

--- 


## Sesgos en el muestreo


![](img/dewey.jpg)

El ejemplo m√°s famoso es el caso [**¬´Dewey defeats Truman¬ª (Dewer derrota a Truman)**]{.hl-yellow}, el titular con el que abri√≥ el Chicago Tribune en 1948, el mismo d√≠a en el que Truman gan√≥ al rep√∫blicano Dewer en las elecciones de 1948: sin esperar a los resultados, se basaron en una encuesta telef√≥nica (sin contar con el sesgo que, en aquella √©poca, solo la clase alta ten√≠a tel√©fono).

---



## Sesgos en el muestreo

![](img/superviviente.jpg)

¬øD√≥nde reforzar√≠as los aviones?

---

## Sesgos en el muestreo

![](img/superviviente.jpg)



El [**sesgo del superviviente**]{.hl-yellow} (un tipo de sesgo de selecci√≥n) aparece cuando se toma una muestra de un fen√≥meno ignorando si los individuos elegidos tienen las mismas opciones respecto al mismo.


---

## Repaso: medidas de centralizaci√≥n

* [**Media**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la media muestral $\overline{x}$ se define como la **suma de todos los valores dividida por el tama√±o muestral**

$$\overline{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$

. . .

[**Geom√©tricamente**]{.hl-purple}: es el **valor ¬´m√°s cercano¬ª de todos los datos a la vez** (minimiza las distancias al cuadrado)

---

## Media muestral


[**VENTAJAS**]{.hl-green}

* F√°cil de calcular y entender
* F√°cil y eficiente de programar
* Siempre existe (para cuantitativas)

. . .

[**DESVENTAJAS**]{.hl-red}

* No es un valor de los datos (la media de {1, 2, 3, 4} es 2.5)
* **Poco robusta** (valores at√≠picos le afectan mucho)
* Solo se puede definir para variables cuantitativas


---

## Repaso: medidas de centralizaci√≥n

* [**Mediana**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la mediana muestral se define como el **valor que es mayor o igual que al menos el 50%**, y menor igual que al menos el 50% de los datos

$$Me_{x} = \arg \min_{x_i} \left\lbrace F_i > 0.5 \right\rbrace, \quad Me_x = e_{i-1} + \frac{0.5 - F_{i-1}}{F_i - F_{i-1} }a_i$$

La mediana es el [**valor de en medio**]{.hl-purple} si ordenamos los datos (y si se pueden ordenar...)

---

## Mediana muestral


[**VENTAJAS**]{.hl-green}

* Suele ser un valor de la muestra
* Un poco m√°s robusta que la media


. . .

[**DESVENTAJAS**]{.hl-red}

* Muy ineficiente (requiere un algoritmo de ordenaci√≥n)
* Solo definida para  cuantitativas o cualitativas ordinales

---


## Repaso: medidas de centralizaci√≥n

* [**Moda**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la moda muestral se define como el **valor o valores m√°s repetidos** (en caso de que existan).

$$Mo_x = \arg \max_{x_i} f_i, \quad Mo_x = e_{i-1} + \frac{d_i - d_{i-1}}{\left(d_i - d_{i-1} \right) + \left(d_i - d_{i+1} \right)}a_i$$


. . .

[**Gr√°ficamente**]{.hl-purple}: representa el ¬´pico¬ª de un diagrama de barras o un histograma

---

## Moda muestral


[**VENTAJAS**]{.hl-green}

* Es un valor de la muestra
* Muy robusta
* **Se puede calcular para cualquier** cuanti o cuali



. . .

[**DESVENTAJAS**]{.hl-red}

* No siempre existe (amodal) y pueden existir varias (bimodal, trimodal, etc)
* Poco usada en inferencia


---

## Repaso: medidas de centralizaci√≥n

![](img/ine-salarios-oculto.jpg)

**¬øCu√°l es la mediana, la media y la moda?**

---

## Repaso: medidas de centralizaci√≥n

![](img/ine-salarios.jpg)

---

## Repaso: medidas de dispersi√≥n

![](img/iker-jimenez.jpg)

¬øQu√© tiene que ver la imagen con la dispersi√≥n?


---

## Repaso: medidas de dispersi√≥n

![](img/extremos.jpg)

El cambio clim√°tico no solo es porque aumente la [**temperatura media (centralizaci√≥n)**]{.hl-yellow} sino por la aparici√≥n cada vez m√°s frecuente de fen√≥menos extremos 


Aumento de la [**variabilidad**]{.hl-yellow} ‚Üí aumento de la [**DISPERSI√ìN**]{.hl-yellow}

---

## Repaso: medidas de dispersi√≥n

[**¬øC√≥mo medir lo que se alejan los datos de la media?**]{.hl-yellow}

. . .

![](img/primera-idea-varianza.jpg)

Una **primera idea** podr√≠a ser [**medir la distancia de cada dato al centro**]{.hl-yellow}, es decir, restar cada dato de la media, y despu√©s realizar su promedio.

---

![](img/wait-for-it.jpg)

---

## Repaso: medidas de dispersi√≥n

Imagina que tenemos la siguiente muestra $X = \left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$.

**¬øCu√°nto vale la media?**

. . .

La media vale 0 y la distancia a ella es...la propia muestra $\left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$. **¬øCu√°l es el promedio de dichas distancias?**

. . .

Pues...de nuevo vale 0.

. . .

Si la dispersi√≥n es 0...[**¬øno hay dispersi√≥n?**]{.hl-red} ¬øNo deber√≠a de dar 0 solo cuando los datos sean constantes?

---

## Repaso: medidas de dispersi√≥n

Para **evitar que se cancelen** los signos lo que haremos ser√° calcular el [**promedio PERO de las distancias al cuadrado**]{.hl-yellow}, la conocida como [**varianza**]{.hl-yellow}

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 $$

. . .


::: callout-warning
# Cuidado

Tomar el valor absoluto (para evitar que se cancelen los signos) suele ser una mala idea en matem√°ticas (no es derivable como funci√≥n).
:::

---

## Repaso: medidas de dispersi√≥n


[**Problema**]{.hl-red}: si los datos est√°n en metros, la varianza estar√° en...metros cuadrados


. . .

:::: columns
::: {.column width="50%"}

¬øTiene sentido medir la dispersi√≥n de nuestra estatura en baldosas?

:::

::: {.column width="50%"}

![](img/albert-rivera.jpg)
:::

::::

---

## Repaso: medidas de dispersi√≥n

Para tener una [**medida de dispersi√≥n en las unidades de los datos**]{.hl-yellow} calcularemos la [**desviaci√≥n t√≠pica**]{.hl-yellow}, como la ra√≠z cuadrada de la varianza

$$s_{x} = \sqrt{s_{x}^{2}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2} = \sqrt{\overline{x^2} - \overline{x}^2}$$


---

## Repaso: medidas de dispersi√≥n

Todav√≠a tenemos un peque√±o problema.

Imagina que queremos **comparar la dispersi√≥n de dos conjuntos** de datos, estaturas de personas y di√°metros de n√∫cleos de c√©lulas. Y Supongamos que las medias son 170 cm y 5 micr√≥metros, y la desviaci√≥n t√≠pica de 1 cm y 1.5 micr√≥metros.

[**¬øQu√© conjunto de datos es m√°s disperso?**]{.hl-yellow}

. . .

Para tener una  **medida de dispersi√≥n adimensional** definiremos el [**coeficiente de variaci√≥n**]{.hl-yellow} 

$$CV_{x} = \frac{s_{x}}{\left| \overline{x} \right|}$$

---

## Repaso: medidas de localizaci√≥n

Las [**medidas de posici√≥n o localizaci√≥n**]{.hl-yellow} nos localizan los datos, siendo valores que **nos dividen un conjunto ordenado** en subconjuntos del mismo tama√±o (ejemplo: mediana es percentil 50).

* **Percentil**: valores $P_{\alpha}$ del conjunto ordenado que dejan por debajo, al menos, el $\alpha$% de datos y $\alpha$% por encima.       
* **Decil**: valores $D_{\alpha}$ que dividen los datos en 10 partes iguales.

* **Cuartil**: valores $C_{\alpha}$ o $q_{\alpha}$ que dividen los datos en 4 partes iguales.

---

## Repaso: covarianza y correlaci√≥n

[**¬øQu√© es en realidad la varianza?**]{.hl-yellow}

. . .

La [**varianza es el promedio de las desviaciones al cuadrado**]{.hl-yellow} (respecto a la media), apareciendo dos veces dicha desviaci√≥n: puede ser entendida como una [**medida que cuantifica la relaci√≥n de una variable CONSIGO MISMA**]{.hl-yellow}

. . .

[**¬øY si qui√©semos medir la relaci√≥n de una variable X respecto a otra variable Y (en lugar de consigo misma)?**]{.hl-purple}


---

## Repaso: covarianza y correlaci√≥n

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 \quad \text{(varianza)}$$


La idea detr√°s de la [**covarianza**]{.hl-yellow} es justo esa: sustituir una de esas desviaciones de la X por la desviaci√≥n de la Y.


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

---

## Repaso: covarianza y correlaci√≥n

Es importante entender algunas [**propiedades de la covarianza**]{.hl-yellow}

. . .

* [**Signo**]{.hl-purple}: la covarianza puede ser tanto positiva como negativa como 0: al eliminar el cuadrado de la varianza, ya no es necesario que sea positiva


. . .

* [**¬øQu√© cuantifica?**]{.hl-purple} La covarianza mide la [**relaci√≥n LINEAL**]{.hl-red} (en torno a una recta) entre dos variables

. . .

* [**¬øQu√© dice su signo?**]{.hl-purple} El signo de la covarianza nos indicar√° la [**direcci√≥n de la dependencia lineal**]{.hl-yellow}: si es positiva, la relaci√≥n ser√° creciente (cuando X crece, Y crece); si es negativa, la relaci√≥n ser√° decreciente (cuando X crece, Y decrece)


---

## Repaso: covarianza y correlaci√≥n

Al igual que pasaba con la varianza, la [**covarianza depende de las unidades y magnitudes**]{.hl-yellow} de los datos, as√≠ que lo que haremos ser√° [**estandarizar la covarianza**]{.hl-yellow}. Definiremos la [**coeficiente correlaci√≥n lineal (de Pearson)**]{.hl-yellow} como la covarianza dividida entre el producto de las desviaciones t√≠picas (adimensional)


$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

. . .

Tiene el [**mismo signo que la covarianza**]{.hl-yellow} (el denominador es siempre positivo) y sus [**valores siempre est√°n entre -1 y 1**]{.hl-yellow}

* m√°s cerca de -1 o 1 ‚Üí relaci√≥n lineal m√°s fuerte
* m√°s cerca de 0 ‚Üí ausencia de relaci√≥n **LINEAL**

---

## Repaso: covarianza y correlaci√≥n

![](img/correlaciones.jpg)


# Clase 2: importancia del dataviz {#clase-2}

[**¬øBasta con calcular la correlaci√≥n para cuantificar si un ajuste es adecuado?**]{style="color:#444442;"}


---

## Conjunto anscombe

En el paquete `{datasets}` se encuentra el dataset conocido como [**cuarteto de Anscombe**]{.hl-yellow}, un dataset que cuenta con 4 conjuntos de datos.

```{r}
anscombe_tb <- as_tibble(datasets::anscombe)
anscombe_tb
```

. . .

[**Primera tarea: convierte a tidydata**]{.hl-yellow}

---

## Conjunto anscombe

```{r}
anscombe_x <- 
  anscombe_tb |>
  pivot_longer(cols = x1:x4, names_to = "dataset", values_to = "x") |> 
  select(-contains("y")) |> 
  mutate(dataset = str_remove_all(dataset, "x"))

anscombe_y <- 
  anscombe_tb |>
  pivot_longer(cols = y1:y4, names_to = "dataset", values_to = "y") |> 
  select(-contains("x"), -dataset)

anscombe_tidy <-
  anscombe_x |> mutate("y" = anscombe_y$y)
anscombe_tidy
```

---

## Conjunto anscombe

[**¬øQu√© caracter√≠sticas muestrales tienen?**]{.hl-yellow} Calcula la media, varianza, desv. t√≠pica, covarianza y correlaci√≥n en cada dataset

. . .

```{r}
anscombe_tidy |> 
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

. . .

Si te fijas [**todos los datasets tienen los mismos momentos**]{.hl-yellow} muestrales, incluso tendr√≠an el mismo ajuste de regresi√≥n...¬øser√°n el mismo dataset desordenado?

---

## Conjunto anscombe

[**Visualiza los 4 datasets**]{.hl-yellow}

. . .

```{r}
#| code-fold: true
ggplot(anscombe_tidy) +
  geom_point(aes(x = x, y = y, color = dataset), size = 3, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~dataset) + theme_minimal()
```

---

## Conjunto anscombe

```{r}
#| echo: false
ggplot(anscombe_tidy) +
  geom_point(aes(x = x, y = y, color = dataset), size = 3, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~dataset) + theme_minimal()
```


Por suerte o por desgracia [**no todo son matem√°ticas**]{.hl-red}: antes de pensar que modelo es mejor para nuestros datos, es important√≠simo realizar un [**an√°lisis exploratorio**]{.hl-yellow} de los datos (incluyendo visualizaci√≥n)

---

## Datasaurus

Podemos visualizarlo de manera a√∫n m√°s extrema con el dataset `datasaurus_dozen` del paquete `{datasauRus}` (ver m√°s en <https://www.research.autodesk.com/publications/same-stats-different-graphs/>)

```{r}
library(datasauRus)
datasaurus_dozen |>
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

---

## Datasaurus

```{r}
#| code-fold: true
ggplot(datasaurus_dozen |> filter(dataset != "wide_lines"),
       aes(x = x, y = y, color = dataset)) +
  geom_point(size = 2.5, alpha = 0.75) +
  geom_smooth(method = "lm", se = FALSE)  +
  facet_wrap(~dataset, ncol = 3) +
  theme_minimal()
```

---

# Clase 3: profundizando en la correlaci√≥n {#clase-3}

[**Matrices de correlaci√≥n y covarianza. Correlaci√≥n vs causalidad vs dependencia**]{style="color:#444442;"}


---

## Correlaci√≥n lineal: sin agrupar


Como dec√≠amos, la idea detr√°s de la [**covarianza**]{.hl-yellow} es una "varianza" entre dos variales (la varianza es una covarianza de una variable consigo misma), midiendo el **promedio de lo que se desv√≠a cada una respecto a su media**

$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

. . .

Para calcularla en `R` basta con usar la funci√≥n `cov()`

```{r}
starwars |> 
  drop_na(mass, height) |> 
  summarise(cov(mass, height))
```

---

## Correlaci√≥n lineal: sin agrupar

Vamos a practicar una vez m√°s como [**hacerlo a mano con el siguiente ejercicio**]{.hl-yellow}.

---

## Cor lineal: sin agrupar

En la tabla inferior se han recopilado (del 2013 al 2022) la **temperatura media en el mes de abril en Madrid (variable X, en ¬∫C)** y el **n√∫mero de d√≠as (variable Y) en el que el nivel de ozono super√≥ las 0.20 ppm (partes por mill√≥n)**

* ¬øCu√°l fue media de d√≠as en los que se super√≥ umbral de ozono de 0.20 ppm?
* ¬øCu√°l fue media de d√≠as en los que se super√≥ umbral de ozono en los a√±os que la temperatura media en marzo super√≥ los 17.4¬∫C?
* ¬øCu√°l es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)

---


## Correlaci√≥n lineal: sin agrupar

Repite el ejercicio con pocas l√≠neas de c√≥digo `R`

* ¬øCu√°l fue la media de d√≠as en los que se super√≥ el umbral de ozono de 0.20 ppm?
* ¬øCu√°l fue la media de d√≠as en los que se super√≥ el umbral de ozono en los a√±os que la temperatura media en marzo super√≥ los 17.4¬∫C?
* ¬øCu√°l es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)


---


## Correlaci√≥n lineal: sin agrupar

Realiza lo que consideres tanto a mano como en `R`

* ¬øExiste alguna **relaci√≥n de dependencia entre las variables**? ¬øDe qu√© tipo? ¬øC√≥mo de fuerte o d√©bil es dicha relaci√≥n? ¬øEn qu√© direcci√≥n es dicha relaci√≥n?


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

---

## Correlaci√≥n lineal: sin agrupar

No s√© si te has fijado qu√© sucede cuando intentamos [**calcular la covarianza/correlaci√≥n de varias variables**]{.hl-yellow}, por ejemplo vamos a calcular la (cuasi)covarianza de todas las variables num√©ricas de starwars.

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

. . .

Podemos usar la funci√≥n `cov()` sin m√°s, fuera de un resumen, obteniendo lo que se conoce como [**matriz de (cuasi)covarianzas**]{.hl-yellow} y que tendr√° un papel fundamental en estad√≠stica ya que contiene la informaci√≥n (= varianza) del dataset.

---

## Matriz de covarianzas


```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

Adem√°s de ser [**sim√©trica**]{.hl-yellow}...¬øqu√© tenemos en la [**diagonal**]{.hl-yellow}?

. . .

La [**matriz de (cuasi)covarianzas**]{.hl-yellow} se denota como $\Sigma$ y sus elementos se define como $\Sigma_{ii} = s_{x_i}^{2}$ para la diagonal y $\Sigma_{ij} = \Sigma_{ji} = s_{x_i x_j}$ fuera de ella.

. . .

::: callout-important
## Importante

Recuerda que los softwares estad√≠sticos nos devuelven siempre la [**cuasi covarianza**]{.hl-purple}, dividido entre $n-1$ y no entre $n$. La cuasivarianza y la cuasicovarianza son los [**mejores estimadores muestrales (insesgados)**]{.hl-yellow} de los respectivos par√°metros poblaciones

:::

---

## Matriz de correlaciones

De la misma manera con `cor()` podemos [**calcular la matriz de correlaciones**]{.hl-yellow} (en este caso sin el `cuasi` ya que se cancelan denominadores)

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cor()
```

. . .

La [**matriz de correlaciones**]{.hl-yellow} se denota como $R$ y sus elementos se define como $r_{ii} = 1$ para la diagonal y $r_{ij} = r_{x_ix_j}$ fuera de ella, y nos proporciona la dependencia lineal entre variables ya de manera **estandarizada**.

---

## Matriz de correlaciones

[**¬øSe te ocurre alguna manera de calcular la matriz de correlaciones a partir de la de covarianzas?**]{.hl-yellow}

---

## Correlaci√≥n lineal: datos agrupados

Imagina ahora que queremos calcular la covarianza/correlaci√≥n de los siguientes datos que representan 24 tiradas de dados

![](img/dados.png)

¬øC√≥mo calcular la covarianza/correlaci√≥n agrupando los datos?

---

## Correlaci√≥n lineal: datos agrupados

![](img/tabla-bi-freq.png)
---

## Correlaci√≥n lineal: datos agrupados

---

## Correlaci√≥n vs dependencia


Podemos tener [**variables incorreladas**]{.hl-red}, con correlaci√≥n nula, pero que [**exista dependencia entre ellas**]{.hl-green}: la covarianza/correlaci√≥n [**SOLO CAPTURA relaciones lineales**]{.hl-yellow}, nada m√°s.

. . .

Veamos un ejemplo sencillo con $X = \left\lbrace -1, 0, 1 \right\rbrace$ y $Y = X^2 =  \left\lbrace 1, 0, 1 \right\rbrace$. 

* La media de ambas es nula
* La media del producto es la media de $XY = \left\lbrace -1, 0, 1 \right\rbrace$, que es de nuevo nula
* As√≠ la covarianza $\overline{x*y} - \overline{x}*\overline{y}$ es nula a pesar de tener la mayor dependencia posible (dependencia funcional)

---

## Correlaci√≥n vs dependencia

![](img/covarianza-no-lineal.png)

En relaciones no lineales como la de la imagen, la **correlaci√≥n estar√° cercana a cero** (ya que no hay relaci√≥n lineal) pero existe una [**dependencia**]{.hl-yellow}. Diremos que [**dos variables son dependientes entre s√≠**]{.hl-yellow} cuando existe un **patr√≥n num√©rico que las relaciona**

. . .

* [**Independencia implica incorrelaci√≥n**]{.hl-green}
* [**Incorrelaci√≥n NO implica independencia**]{.hl-red}

---

## Correlaci√≥n vs dependencia

![](img/escenarios-covarianza.png)

---

## Dependencia vs causalidad

![](img/helados-moreno.jpg)

Si $X$ e $Y$ fuesen variables dependientes (lineales o no)...[**¬øimplicar√≠a que el ascenso/descenso de una provoca el ascenso/descenso de la otra?**]{.hl-yellow}

. . .

Imagina por ejemplo dos variables: nivel de bronceado (X) y consumo de helados (Y). ¬øSon dependientes? Aparentemente s√≠ ya que su comportamiento es similar. **¬øUna causa la otra?**

---

## Dependencia vs causalidad

![](img/helados-moreno.jpg)

Diremos que dos variables tienen una [**relaci√≥n causal o de causalidad**]{.hl-yellow} cuando una variable es la CAUSA del comportamiento de la otra, algo que no sabremos solo con estad√≠stica (sino con conocimiento experto, en este caso de nutricionistas y m√©dicos)

. . .

[**Correlaci√≥n NO IMPLICA causalidad**]{.hl-red} (al menos no tiene por qu√©)

---

## Dependencia vs causalidad


![](img/chocolate-correlacion.jpg)


Este fen√≥meno es conocido como [**correlaciones esp√∫reas**]{.hl-yellow}, y aparece cuando dos variables presentan una [**relaci√≥n matem√°tica**]{.hl-green} pero sin [**ning√∫n tipo de relaci√≥n causal o l√≥gica**]{.hl-red}. Puedes ver m√°s en <https://www.tylervigen.com/spurious-correlations>

---

## Dependencia vs causalidad

![](img/confounder.jpg)

Dicho patr√≥n matem√°tico puede deberse al mero azar o la existencia de lo que se conoce como [**variables de confusi√≥n**]{.hl-yellow} (en el caso del helado, el verano es el que causa ambas).

La rama que se dedica al [**estudio de la causalidad**]{.hl-yellow}, combinando las ramas de la **estad√≠stica, filosof√≠a, sociolog√≠a y psicolog√≠a** se le conoce como [**inferencia causal**]{.hl-yellow} y es fundamental en un an√°lisis m√°s profunde de las relaciones entre las variables (sobre todo en campos como la econom√≠a o la sociolog√≠a)


# Clase 4: primeras regresiones {#clase-4}

[**Historia regresi√≥n. Aprendizaje supervisado. Regresi√≥n lineal univariante**]{style="color:#444442;"}

---

## Historia de la reg

> Hay una regla universal: cualquier pariente tuyo es probablemente m√°s mediocre que t√∫¬ª

La [**historia de regresi√≥n**]{.hl-yellow} se remonta a **Francis Galton**, primo de Charles Darwin, que adem√°s de estad√≠stico fue psic√≥logo, ge√≥grafo y, por desgracia, el primer eugen√©sico (de hecho acu√±√≥ el termino)

. . .

Tambi√©n fue el primero en proponer m√©todos de clasificaci√≥n de huellas en medicina forense e incluso se le atribuye el primer mapa meteorol√≥gico de la historia

---

## Regresi√≥n y Darwin

Galton mostr√≥ fascinaci√≥n por ¬´El origen de la especies¬ª de su primo. Sin embargo, [**Galton no se centraba en los mejor adaptados sino en los que √©l llama mediocres**]{.hl-yellow}

. . .

Seg√∫n Galton, las sociedades estaban fomentando la mediocridad, interfiriendo en la selecci√≥n natural, as√≠ que empez√≥ a **estudiar si el talento era o no hereditario**.


. . .

¬øSu conclusi√≥n? El [**talento se disipaba**]{.hl-yellow} a lo largo de las generaciones

---

## Regresi√≥n a la mediocridad

En 1886 public√≥ ¬´Regression towards mediocrity in hereditary stature¬ª, un art√≠culo que cambiar√≠a la estad√≠stica: fue el [**primer uso documentado de lo que hoy conocemos como recta de regresi√≥n**]{.hl-yellow} 

. . .

:::: columns
::: {.column width="38%"}

![](img/reg_dalton.jpg)

:::

::: {.column width="62%"}

Galton analiz√≥ la estatura de 205 hijos y padres, observando que, de nuevo, los valores extremos se disipaban: a lo largo de las generaciones hab√≠a una [**regresi√≥n (un retroceso) a la mediocridad (entendida como la media)**]{.hl-yellow} 


:::
::::

Hijos de altos eran algo m√°s bajitos, e hijos de bajitos eran algo m√°s altos.

---

## Regresi√≥n a la mediocridad

:::: columns
::: {.column width="60%"}

![](img/reg-galton-coef.jpg)

:::

::: {.column width="40%"}

Galton no solo observ√≥ que las [**estaturas ¬´regresaban¬ª a un valor medio sino que lo hac√≠an con un patr√≥n**]{.hl-yellow}, con un factor constante de $2/3$: si los padres se desviaban $+3$ por encima de la media, los hijos se desviaban solo $(2/3)*3 = +2$ por encima de la media.

:::
::::

---

## Aprendizaje estad√≠stico

La regresi√≥n lineal es el modelo m√°s simple de lo que se conoce como [**aprendizaje estad√≠stico (Machine Learning)**]{.hl-yellow}, en concreto del conocido como [**aprendizaje supervisado**]{.hl-yellow}

![](img/ml.jpg)


---

## Ciencia de Datos


La [**ciencia de datos**]{.hl-yellow} es precisamente la rama que integra las matem√°ticas, la estad√≠stica, la probabilidad, el Machine Learning e incluso el Big Data

![](img/stats-ml.jpg)

---

## Aprendizaje ¬øsupervisado?

![](img/unsupervised-learning.jpg)

---

## Aprendizaje ¬øsupervisado?

En el campo del Machine Learning hay principalmente dos tipos de metodolog√≠as:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje supervisado**]{.hl-purple}: tendremos dos tipos de variables, la [**variable dependiente (output/target)**]{.hl-yellow} que se quiere predecir/clasificar, normalmente denotada como $Y$, y las [**variables independientes (inputs) o explicativas o predictoras**]{.hl-yellow}, que contienen la informaci√≥n disponible. Ejemplos: regresi√≥n, knn, √°rboles, etc.
 

:::

::: {.column width="50%"}

![](img/supervised.jpg)
:::
::::

---

## Aprendizaje ¬øsupervisado?

En el campo del Machine Learning hay principalmente dos tipos de metodolog√≠as:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje no supervisado**]{.hl-purple}: no existe la distinci√≥n entre target y variables explicativas ya que [**no tenemos etiquetados los datos**]{.hl-yellow}, no sabemos a priori la respuesta correcta. El aprendizaje no supervisado [**buscar√° patrones**]{.hl-yellow} basados en similitudes/diferencias. Ejemplos: PCA, clustering, redes neuronales, etc.

:::


::: {.column width="50%"}

![](img/unsupervised.jpg)

:::
::::

---

## Clasificaci√≥n vs predicci√≥n

Como hemos comentado, la [**regresi√≥n lineal**]{.hl-yellow} se enmarca dentro del [**predicci√≥n supervisada**]{.hl-yellow}

* [**Predicci√≥n**]{.hl-purple}: la [**variable objetivo es una variable cuantitativa continua**]{.hl-yellow} (por ejemplo, precio, glucosa, peso, etc).

* [**Clasificaci√≥n**]{.hl-purple}: la [**variable objetivo es una variable cualitativa**]{.hl-yellow} (por ejemplo, especie de flor, ausencia/presencia de enfermedad, si/no, etc) o **cuantitativa discreta** (por ejemplo, n√∫mero de accidentes). La etiqueta tomar√° un valor dentro del conjunto de modalidades permitidas, pudiendo ser **binaria** (si/no) o **multiclase** (A, B, C, D).

&nbsp;

üìö Ver ¬´The elements of Statistical Learning¬ª (Hastie et al., 2008)

# Clase 5: ajuste de regresi√≥n {#clase-5}

[**Interpretaci√≥n de coeficientes. M√©todo m√≠nimos cuadrados**]{style="color:#444442;"}

---

## Modelo predictivo

Dentro del marco de la [**predicci√≥n supervisada**]{.hl-yellow} un modelo tendr√° siempre la siguiente forma:

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$

* $X$ ser√°n los [**datos**]{.hl-yellow}

* $f(\cdot)$ ser√° nuestro [**modelo**]{.hl-yellow}, es decir, el [**valor esperado de $Y$**]{.hl-yellow} con la informaci√≥n que tenemos.

* $\mathbf{X} = \left(X_1, \ldots, X_p \right)$ ser√°n nuestras [**predictoras o variables independientes**]{.hl-yellow}

* $\varepsilon$ ser√° el [**error o ruido**]{.hl-yellow}, una [**variable aleatoria de media 0**]{.hl-yellow} $E \left[\varepsilon | \boldsymbol{X} = x \right] = 0$ (el error deber√≠a ser reducido a **algo aleatorio (irreducible)**, aunque en estad√≠stica SIEMPRE nos vamos a equivocar).

---

## Modelo predictivo

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$


El objetivo es intentar [**estimar dicha variable objetivo**]{.hl-yellow} minimizando al m√°ximo el error cometido (la parte que podemos evitar).

. . .

El [**modelo estimado**]{.hl-yellow} se definir√° como

$$\widehat{Y} := \widehat{E \left[Y | \boldsymbol{X} = x \right]}  = \widehat{f}\left(X_1, \ldots, X_p \right), \quad \widehat{\varepsilon} = \widehat{Y} - Y$$

* $\widehat{Y}$ ser√°n las [**estimaciones**]{.hl-yellow}, definidas como la estimaci√≥n del [**valor esperado de $Y$**]{.hl-yellow} con la informaci√≥n que tenemos.

* $\widehat{f}$ ser√° el [**modelo estimado**]{.hl-yellow}

* $\widehat{\varepsilon}$ los [**errores estimados**]{.hl-yellow} (con media muestral igual a cero).


---

## Reg. lineal univariante

En el caso concreto de la [**regresi√≥n lineal**]{.hl-yellow} nuestro modelo ser√° un **hiperplano lineal** (en el caso de una variable, una simple recta):

$$E \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p$$

. . .

Su estimaci√≥n ser√° por tanto

$$\widehat{Y} := \widehat{E \left[Y | \boldsymbol{X} = x \right]} = \widehat{f}(\mathbf{X}) = \widehat{\beta_0} + \widehat{\beta_1} X_1 + \ldots + \widehat{\beta_p} X_p $$

* El modelo $\widehat{f}(\cdot)$ ser√° una [**recta/hiperplano lineal**]{.hl-yellow}

* En el caso de la regresi√≥n lineal [**univariante**]{.hl-yellow} tendremos por tanto $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1X$ ($p = 1$)

* El objetivo ser√° obtener la estimaci√≥n de los $\widehat{\beta}$ tal que **minimicemos el error**

---

## Varianza residual

Como ya hemos comentado, una manera de cuantificar lo que nuestro modelo se equivoca es lo que llamamos [**varianza residual o error cuadr√°tico medio**]{.hl-yellow} (varianza del error), que podemos estimar como sigue (ya que la [**media muestral de los errores ser√° cero**]{.hl-purple})

$$s_{r}^{2} = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \left( \widehat{\varepsilon}_{i} - \overline{\widehat{\varepsilon}}_{i}\right)^2 = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} $$
¬øC√≥mo quedar√≠a la f√≥rmula si **desarrollo lo que vale los errores estimados** $\widehat{\varepsilon}$?

. . .


$$s_{r}^{2} := s_{r}^{2}\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)= \frac{1}{n} \sum_{i=1}^{n} \left(Y_i - \widehat{Y}_i \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\widehat{\beta}_0 + \widehat{\beta}_1X_i \right)\right]^2$$

---

## M√©todo m√≠nimos cuadrados

El [**m√©todo de los m√≠nimos cuadrados**]{.hl-yellow}, originariamente planteado a la vez por Legendre y Gauss, consiste en establecer que los [**par√°metros √≥ptimos**]{.hl-yellow} ser√°n aquellos $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$ que minimicen dicha suma de cuadrados (que minimicen la varianza residual)

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} s_{r}^{2}\left(\beta_0, \beta_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} n*s_{r}^{2}\left(\beta_0, \beta_1 \right)$$

Vamos a desarrollar ese cuadrado...

--- 


## M√©todo m√≠nimos cuadrados

$$s_{r}^{2}\left(\beta_0, \beta_1 \right) = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\beta_0 + \beta_1 X_i \right)\right]^2$$ 
Vamos a desarrollar ese cuadrado...

. . .

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$




---

## M√©todo m√≠nimos cuadrados

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$

¬øC√≥mo encontrar el m√≠nimo de una funci√≥n?

. . .

Efectivamente, derivando.

$$\frac{\partial s_{r}^{2}}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{0} + 2 \beta_1 X_i - 2Y_i \right] = 2 \left( \beta_{0} + \beta_{1} \overline{x} - \overline{y} \right)$$ 

$$\frac{\partial s_{r}^{2}}{\partial \beta_1} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{1}X_{i}^{2} + 2 \beta_0 X_i - 2Y_iX_i \right] = 2 \left( \beta_{1} \overline{x^2}+\beta_{0}\overline{x} - \overline{xy} \right)$$ 

---

## M√©todo m√≠nimos cuadrados


Si lo agrupamos en un sistema y lo [**igualamos a cero**]{.hl-yellow} para encontrar el √≥ptimo tenemos

$$\left\{\beta_{0} + \beta_{1} \overline{x}   = \overline{y} \atop \overline{x} \beta_{0} + \overline{x^2} \beta_{1} = \overline{xy} \right.$$ 

¬øC√≥mo resolver un sistema?

. . .

Haciendo la regla de Cramer tenemos que

$$\widehat{\beta}_1 = \frac{\overline{xy} - \overline{x}*\overline{y}}{\overline{x^2} - \overline{x}^2} = \frac{s_{xy}}{s_{x}^{2}}$$

$$\widehat{\beta}_0 = \frac{\overline{y}*\overline{x^2} - \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2}  = \overline{y} + \frac{\overline{y}*\overline{x}^2- \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2} = \overline{y} - \widehat{\beta}_1 \overline{x}$$


---

## Estimaci√≥n reg. univariante


Los [**estimadores de los par√°metros de una regresi√≥n lineal univariante**]{.hl-yellow} ser√°n por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

. . .

Es importante [**distinguir lo poblaci√≥n de lo muestral**]{.hl-yellow}

* Los par√°metros $\left( \beta_0, \beta_1 \right)$ son desconocidos, los [**par√°metros poblacionales**]{.hl-yellow}

* Los par√°metros $\left( \widehat{\beta}_0, \widehat{\beta}_1 \right)$ son estimados a partir de los datos, son [**variables aleatorias**]{.hl-yellow} ya que han sido calculados en funci√≥n de una muestra aleatoria $\left\lbrace \left(x_i, y_i \right) \right\rbrace_{i=1}^{n}$ de la poblaci√≥n $\left(X, Y \right) $

---

## Estimaci√≥n reg. univariante

Los [**estimadores de los par√°metros de una regresi√≥n lineal univariante**]{.hl-yellow} ser√°n por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

¬øCu√°l es su [**interpretaci√≥n**]{.hl-yellow}?

. . .

* [**Ordenada en el origen**]{.hl-yellow}: tambi√©n llamado **intercepto**, y denotado como $\beta_0$ su valor real, es el valor de $Y$ cuando $X=0$. Es decir, $\widehat{\beta}_0$ se puede interpretar como la estimaci√≥n $\widehat{Y}$ cuando $X = 0$ (cuando dicha estimaci√≥n tenga sentido).


. . .

* [**Pendiente**]{.hl-yellow}: denotado com $\beta_1$ su valor real, cuantifica el incremento de $Y$ cuando $X$ aumenta una unidad. Es decir, $\widehat{\beta}_1$ se puede interpretar como la variaci√≥n de la **estimaci√≥n $\widehat{Y}$ cuando $X$ tiene un incremento unitario**.

---

## Regresi√≥n en R

Para hacer un [**ajuste de regresi√≥n lineal univarainte en R**]{.hl-yellow} solo necesitamos utilizar la funci√≥n `lm()` (de linear model) con dos argumentos:

* `data = ...`: los datos de los que queremos sacar el ajuste (sin ausentes de momento)
* `formula = ...`: la f√≥rmula del ajuste (y frente a x, expresado como `y ~ x`)

```{r}
datos <- 
  starwars |>
  drop_na(mass, height) 
ajuste_lineal <- lm(data = datos, formula = mass ~ height)
```

---

## Regresi√≥n en R

Una vez realizado el ajuste podemos [**resumirlo**]{.hl-yellow} con `summary()`

```{r}
ajuste_lineal |> summary()
```

. . .

¬øQu√© representa cada bloque de la salida?

---

## Regresi√≥n en R

```{r}
ajuste_lineal |> summary()
```

* `Call: ...`: la orden que hemos ejecutado

* `Residuals: ...`: un resumen en forma de [**cuartiles de los residuales**]{.hl-yellow}, es decir, de los [**errores**]{.hl-yellow} (f√≠jate que aunque la media de los errores siempre es 0, no tiene porque serlo la mediana, de hecho que no lo sea ya nos indica que normales no van a ser seguramente).


---

## Regresi√≥n en R

```{r}
ajuste_lineal |> summary()
```


* `Coefficients: ...`: de momento solo nos interesa la columna [**Estimate**]{.hl-yellow} que nos da las estimaciones de los par√°metros. En la fila `Intercept` siempre ir√° $\widehat{\beta}_0$, y el resto de filas tendr√° el nombre de la variable predictora a la que multiplica el par√°metro (en este caso la fila `height` corresponde a la estimaci√≥n $\widehat{\beta}_1$).

---


## Regresi√≥n en R

```{r}
ajuste_lineal |> summary()
```

En nuestro caso particular, el [**ajuste de regresi√≥n**]{.hl-yellow} viene dado por $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1 X = -11.4868 + 0.6240*X$, donde $X$ es la estatura: por cada cm extra que mida un personaje, el modelo estima que el peso aumenta 0.6240 kg, y la estimaci√≥n de peso para un persona que mida 0 cm es de -11.4868 kg (obviamente esta estimaci√≥n mucho sentido no tiene)

---

## Regresi√≥n en R

```{r}
ajuste_lineal |> summary()
```

* `Multiple R-squared: ...` (de momento lo dem√°s lo ignoramos): es el conocido como $R^2$ o bondad de ajuste. Hablaremos de √©l en las pr√≥ximas clases, pero de momento, nos basta saber que es una [**m√©trica de calidad del ajuste**]{.hl-yellow} que va de 0 (peor) a 1 (mejor).

---

## Predicci√≥n en R

Una vez estimado el modelo (sus coeficientes) podemos usarlo, bien para  [**estimar valores que el modelo conoce**]{.hl-yellow} (de los que aprendi√≥) o para [**predecir nuevos valores**]{.hl-yellow} de los que el modelo no ha aprendido

. . .

Simplemente necesitamos la funci√≥n `predict(modelo, nuevos_datos)`, usando como argumentos el **modelo entrenado** y un [**nuevo dataset que contenga una/s columna/s llamadas igual que nuestras variables predictores**]{.hl-yellow} con los valores de la predictora para los que queremos predecir

```{r}
predicciones <- 
  tibble("height" = c(15, 81.3, 153.1, 189.3, 210.3),
         "mass_predict" = predict(ajuste_lineal, tibble("height" = height)))
predicciones
```

---

## Rango de fiabilidad

Aunque hablaremos en profundidad de la bondad de ajuste, supongamos que tenemos un **modelo extremadamente bueno**, con una bondad de ajuste $R^2 = 0.99$. ¬øSer√≠a fiable nuestro modelo anterior para predecir el peso de un personaje con $X = 15$ cm?

. . .

```{r}
predicciones
```

Si te fijas la [**¬°predicci√≥n del peso es negativa!**]{.hl-red}: por muy beb√© que sea, algo pesar√°. ¬øPor qu√© sucede esto?

---

## Rango de fiabilidad


![](img/rango_1.png)
![](img/rango_2.png)

Nuestro [**modelo solo puede aprender de los datos que conoce**]{.hl-yellow}: solo podremos predecir para valores nuevos de las predictoras [**dentro del rango de las predictoras con las que entrenamos**]{.hl-yellow} el modelo

Da igual lo bueno que sea el modelo: las [**predicciones fuera del rango de las predictoras de entrenamiento**]{.hl-red} no ser√°n fiables ya que el modelo no sabe lo que sucede fuera.

# Clase 6: diagnosis  {#clase-6}

[**Hip√≥tesis e inferencia del modelo**]{style="color:#444442;"}

---

## Repaso


* [**Modelo lineal**]{.hl-yellow}: descrito como $Y = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p X_p + \varepsilon$, buscamos estimar el **valor esperado**  $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p x_p$, donde $\boldsymbol{X}$ es la poblaci√≥n y $x$ la muestra.

. . .

* [**Caso univariante**]{.hl-yellow}: $p = 1$ tal que $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 x$.

. . .

* [**Estimaci√≥n**]{.hl-yellow}: $\widehat{Y}$ en funci√≥n de $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$. De todas las rectas posibles, queremos la [**recta de regresi√≥n m√≠nimos-cuadrados**]{.hl-yellow}: **minimiza la varianza residual** (calculamos sus derivadas parciales, igualamos a cero y resolvemos el sistema). Esos estimadores ser√°n $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$  y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$.

. . .

* [**Predicci√≥n**]{.hl-yellow}: podemos estimar o predecir valores con la recta $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_0 X$, siempre y cuando $X = x$ est√© dentro del rango de valores de entrenamiento del modelo.

---

## Diagnosis

Hasta ahora no hemos pedido nada a la muestra $\left\lbrace \left( X_i, Y_i \right) \right\rbrace_{i=1}^{n}$, ¬øpara qu√© necesitar√≠amos [**hip√≥tesis**]{.hl-yellow} entonces?

. . .

La raz√≥n es que, hasta ahora, lo √∫nico que hemos podido realizar es una [**estimaci√≥n puntual**]{.hl-yellow} de los par√°metros, pero dado que dichos estimadores ser√°n variables aleatorias, necesitaremos realizar [**inferencia estad√≠stica**]{.hl-yellow} sobre ellos (recuerda: los par√°metros son simpleme estimaciones para esa muestra de la poblaci√≥n, de forma que dada otra muestra, la recta ser√° distinta).

. . .

Para poder cuantificar la [**variabilidad y precisi√≥n de nuestras estimaciones**]{.hl-yellow} necesitaremos que los datos cumplan ciertas [**hip√≥tesis probabil√≠sticas**]{.hl-purple}: lo interesante no es la estimaci√≥n puntual de los par√°metros a partir de la muestra sino lo que [**podamos inferir de ellos a la poblaci√≥n**]{.hl-green}

---

## Diagnosis

En el caso de la regresi√≥n lineal univariante pediremos [**4 hip√≥tesis**]{.hl-yellow}

. . .

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1 x$

. . .

2. [**Homocedasticidad**]{.hl-green}: el modelo no podr√° explicar toda la informaci√≥n (a veces se equivocar√° por arriba y otras por abajo), pero necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow}, tal que $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \boldsymbol{X} = x \right] = cte < \infty$, que no var√≠e seg√∫n aumenta o disminuye la $X$.

. . .

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$

. . .

4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser independientes entre s√≠ (el error en una observaci√≥n no depende de otras). En particular, ser√°n **incorrelados**

$${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] - E \left[\varepsilon_i \right] E \left[\varepsilon_j \right] = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$$

---


## Diagnosis


Las 4 hip√≥tesis se pueden [**resumir de manera te√≥rica**]{.hl-yellow} en

$$Y | \boldsymbol{X} = x \sim N \left(\beta_0 + \beta_1x, \sigma_{\varepsilon}^2 \right)$$

. . .

Su [**versi√≥n muestral**]{.hl-purple} ser√≠a simplemente

$$y_i | \boldsymbol{X} = x_i \sim N \left(\beta_0 + \beta_1x_i, \sigma_{\varepsilon}^2 \right), \quad i=1,\ldots,n$$

---

## Inferencia de los par√°metros

Las hip√≥tesis nos permiten decir (lo demostraremos m√°s adelante) que los [**par√°metros estimados siguen una distribuci√≥n (condicionada) normal**]{.hl-yellow} de [**media el par√°metro**]{.hl-purple}  a estimar y de [**varianza el conocido como standard error**]{.hl-purple}

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

. . .

Por las propiedades de la normal y el teorema central del l√≠mite 

$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)}| \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$

---

## Inferencia de los par√°metros


$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

Veremos m√°s adelante porqu√© pero de momento...estos son los $SE$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

¬øQu√© **propiedades** tienen estos estimadores?


---

## Inferencia de los par√°metros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

* [**Insesgados**]{.hl-yellow}: ambos son [**estimadores insesgados**]{.hl-green} ya que la esperanza de la estimaci√≥n es el valor a estimar. $E \left[ \widehat{\beta}_j \right] = \beta_j$

. . .
 
* [**Precisi√≥n vs tama√±o muestral**]{.hl-yellow}: cuando $n \to \infty$, sus varianzas tienden a cero, es decir, $SE \left(\widehat{\beta}_j\right)^2 \to^{n \to \infty} 0$ ya que $n$ est√° dividiendo. Traducci√≥n: [**a m√°s datos, mayor precision**]{.hl-green} (menos varianza tendr√°n los estimadores si repetimos la toma de muestras)


---

## Inferencia de los par√°metros


$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$

. . .

* [**Precisi√≥n vs var residual**]{.hl-yellow}: cuanto m√°s grande sea la varianza del error $\sigma_{\varepsilon}^{2}$, $SE \left(\widehat{\beta}_j\right)^2$ crecer√° (es decir, [**m√°s ruido implicar√° m√°s imprecisi√≥n**]{.hl-red})

. . .

* [**Precisi√≥n vs varianza de X**]{.hl-yellow}: cuanto m√°s grande sea la varianza de la predictora $s_{x}^2$, $SE \left(\widehat{\beta}_j\right)^2$ decrecer√°, tal que $SE \left(\widehat{\beta}_j\right)^2 \to^{s_{x}^2 \to \infty} 0$. Dicho de otra forma: cu√°nta [**m√°s informaci√≥n (varianza) contenga nuestra tabla, mayor precisi√≥n**]{.hl-green}.

. . .

* [**Precisi√≥n vs media X**]{.hl-yellow}: solo afecta a la estimaci√≥n de $\beta_0$, cuya [**precisi√≥n decrece cuando la media aumenta**]{.hl-red}. Esto tiene sentido ya que cu√°nto m√°s grande en media sean los datos, menos fiable ser√° la predicci√≥n para $X=0$.

---

## Inferencia de los par√°metros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


Pero tenemos un problema: la [**varianza residual poblacional**]{.hl-yellow} (varianza poblaci√≥n del error) $\sigma_{\varepsilon}^{2}$ es [**desconocida**]{.hl-yellow}. Como suele pasar, la [**sustituiremos por un estimador (insesgado)**]{.hl-yellow} de la misma (ya demostaremos...), donde $p$ es el n√∫mero de variables (lo que R llama `std error` en la salida)

. . .

$$\widehat{\sigma_{\varepsilon}^{2}} = \frac{n}{n-p-1} s_{r}^{2} = \frac{1}{n-p-1} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 =_{p=1} \frac{1}{n-2} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2$$
---

## Inferencia de los par√°metros



$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$


¬øQu√© suele suceder cuando en una normal **desconocemos la varianza real y la sustituimos por su estimador insesgado**?

. . .

Si [**sustituimos el error standard por su estimador**]{.hl-yellow} (sustituyendo la varianza residual por su estimador insesgado), obtenemos que sigue una **t-Student**

$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$
donde $\widehat{SE} \left(\widehat{\beta}_0\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right]$ y $\widehat{SE} \left(\widehat{\beta}_1\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n s_{x}^2}$

---

## Inferencia de los par√°metros


```{r}
ajuste_lineal |> summary()
```

* `std error`: es lo que hemos denotado como $\widehat{SE} \left(\widehat{\beta}_j\right)^2$

. . .

* `t value`: es el estad√≠stico $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ que hemos dicho que sigue una t-Student (f√≠jate que hemos puesto $\beta_j = 0$)

## Inferencia de los par√°metros


```{r}
ajuste_lineal |> summary()
```

* `Pr(>|t|)`: representa un p-valor. ¬øDe qu√© contraste se te ocurre?

. . .

* `Residual standard error`: es el estimador insesgado de la varianza residual, que hemos denotado como $\widehat{\sigma_{\varepsilon}^{2}}$

---

## Inferencia de los par√°metros

:::: columns
::: {.column width="55%"}

$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$

:::

::: {.column width="45%"}

![](img/tablas-intervalos.png)

:::
::::

Si te acuerdas de inferencia, esto implica que los [**intervalos de confianza de nuestros par√°metros**]{.hl-yellow} son

$$IC \left(\alpha, \widehat{\beta}_j \right) = \left[\widehat{\beta}_j - t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right), \widehat{\beta}_j + t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right) \right]$$
donde $t_{n-p-1; \alpha/2}$ es el cuantil $1-\alpha/2$ (deja a la izquierda una probabilidad de $1-\alpha/2$).

---


## Inferencia de los par√°metros

Podemos calcular los intervalos de confianza de los par√°metros en `R` con `confint()`

```{r}
confint(ajuste_lineal, level = 0.99)
confint(ajuste_lineal, level = 0.95)
confint(ajuste_lineal, level = 0.9)
```

---

## Inferencia de los par√°metros

[**¬øQu√© significa realmente un intervalo de confianza?**]{.hl-yellow}

. . .

La **probabilidad de que el par√°metro real** caiga dentro [**NO EXISTE**]{.hl-red}: el par√°metro es desconocido pero fijo, no aleatorio, as√≠ que no sentido calcular su probabilidad.

. . .

Si obtenemos para un par√°metro un intervalo $[-1, 1]$ al 95%, no significa que $P(parametro \in [-1, 1]) = 0.95$: significa que [**si tomamos 1000 muestras distintas de la poblaci√≥n**]{.hl-yellow} y calculamos para cada una el intervalo de confianza, [**aproximadamente 950 intervalos de confianza contendr√°n dentro el par√°metro real**]{.hl-yellow}

. . .

Un [**intervalo al 95% implica que habr√° una frecuencia esperada de 0.95**]{.hl-yellow} de que intervalos que no conocemos (porque se derivan de muestras que no hemos tomado) contengan al par√°metro real, pero [**no es la probabilidad de que tu intervalo calculado contenga a dicho par√°metro**]{.hl-red}: nos habla la precisi√≥n de nuestra metodolog√≠a de estimaci√≥n, no del par√°metro.


---

## Inferencia de los par√°metros

[**Deberes**]{.hl-yellow}. Dada una poblaci√≥n normal $\mu = 3$ y $\sigma = 1.2$, crea un c√≥digo que genere 500 muestras distintas (tama√±o $n = 100$ cada una), de manera que para cada una apliques un `t.test()` para calcular un IC para $\mu$. Tras ello, gr√°fica los intervalos como se muestra en la imagen (haz uso de `geom_segment()`)

```{r}
#| echo: false
mu <- 3
sigma <- 1.2
m <- 500
conf_int <- tibble("id" = 1:m, "low" = NA, "high" = NA)

for (i in 1:m) {
  
  sample <- rnorm(n = 100, mean = mu, sd = sigma)
  hyp_test <- t.test(x = sample)
  conf_int[i, 2] <- hyp_test$conf.int[1]
  conf_int[i, 3] <- hyp_test$conf.int[2]
}
conf_int <-
  conf_int |> 
  mutate(true_param = mu >= low & mu <= high)

ggplot(conf_int) +
  geom_segment(aes(y = id, yend = id, x = low, xend = high,
                   color = true_param, linewidth = true_param)) +
  geom_vline(xintercept = mu) +
  scale_linewidth_manual(values = c(1.1, 0.2)) +
  theme_minimal() +
  guides(linewidth = "none") +
  labs(x = "Valores del intervalo",
       y = "id intervalo",
       color = "¬øContiene mu real?")
```

---


## Inferencia de los par√°metros

Y si tenemos inferencia, tenemos contrastes: ¬øte acuerdas de los p-valores que devuelve la tabla para cada par√°metro?

. . .

Para cada par√°metro se realiza un [**contraste de significancia**]{.hl-yellow}: ¬øcu√°nta evidencia hay en mis datos para poder decir que el [**valor estimado de mi par√°metro es distinto de 0**]{.hl-yellow}?

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

. . .

El estad√≠stico usado es el [**t value**]{.hl-yellow} que devuelve `R` definido como $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ y que, [**BAJO EL SUPUESTO DE QUE LA HIP√ìTESIS NULA SEA CIERTA**]{.hl-purple}, sigue una t-Student $t_{n-p-1}$

---

## Inferencia de los par√°metros

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

Este contraste significancia en realidad **contesta a la siguiente pregunta**

¬øTiene mi variable $X_j$ un [**efecto lineal SIGNIFICATIVO**]{.hl-yellow} en mi variable $Y$?

. . .

&nbsp;

Esa pregunta puede ser respondida desde un punto de vista **frecuentista** haciendo uso de los [**p-valores**]{.hl-yellow}: si $p-value < \alpha$, se suele rechazar la nula (rechazamos que la variable no tenga efecto lineal significativo, es decir, aceptamos que s√≠ lo tiene); en caso contrario no se suele rechazar (que **no es lo mismo que aceptarla**). Pero...

---

## Par√©ntesis: p-valor

[**¬øQu√© representa un p-valor?**]{.hl-yellow}

. . .

Hablaremos m√°s adelante de ellos pero este es un [**peque√±o resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>

. . .

* [**Habla sobre los datos**]{.hl-yellow}. Los p-valores nos pueden servir como indicadores de **c√≥mo de incompatible son los datos respecto a un modelo/hipotesis/explicaci√≥n asumida**: habla sobre los datos, no sobre la veracidad de la hipotesis nula per se o la probabilidad de que los datos hayan salido tan extremos por azar. 

---

## Par√©ntesis: p-valor

[**¬øQu√© representa un p-valor?**]{.hl-yellow}

Hablaremos m√°s adelante de ellos pero este es un [**peque√±o resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>



* [**Realidades complejas, decisiones complejas**]{.hl-yellow}. Cuidado con reducir a decisiones binarias realidades complejas. La regla del p-valor es una herramienta m√°s, pero no debe ser la √∫nica en la que nos basemos para decidir. [**Otros aspectos a considerar**]{.hl-yellow}: calidad de las medidas, dise√±o del estudio, evidencia externa en la literatura respecto a la causalidad subyacente, validez de las hip√≥tesis planteadas, etc

---

## Par√©ntesis: p-valor

[**¬øQu√© representa un p-valor?**]{.hl-yellow}

Hablaremos m√°s adelante de ellos pero este es un [**peque√±o resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>


* [**No hagas cherry picking**]{.hl-red}: muestra de manera transparentes que has probado, que ha salido y que no, y no te quedes solo con lo que sale bien o los p-valores que te convenga (**p-hacking**).

---

## Par√©ntesis: p-valor

[**¬øQu√© representa un p-valor?**]{.hl-yellow}

Hablaremos m√°s adelante de ellos pero este es un [**peque√±o resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>


* [**Significancia estad√≠stica**]{.hl-yellow} no implica significacia real (cient√≠fica, humana, econ√≥mica, etc). Obtener [**p-valores muy peque√±os no implica un mayor efecto**]{.hl-red} que otros p-valores no tan peque√±os (y viceversa). [**Cualquier efecto, por peque√±o que sea, puede derivar en p-valores peque√±os si el tama√±o uestral o la precisi√≥n de las medidas es suficiente alto**]{.hl-yellow}, y al contrario (efectos evidentes pueden derivar en p-valores altos si $n$ es peque√±o)

---

## Par√©ntesis: p-valor

[**¬øQu√© representa un p-valor?**]{.hl-yellow}

Hablaremos m√°s adelante de ellos pero este es un [**peque√±o resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>


* [**Alternativas**]{.hl-purple}: intervalos de confianza, de credibilidad, m√©todos bayesianos, false discovery rates.

&nbsp;

üìö Ver m√°s en <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>, <https://anabelforte.com/2020/11/15/contraste/> y <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877414/>

---

## Inferencia de los par√°metros



```{r}
ajuste_lineal |> summary()
```

Si te fijas ambos p-valores est√°n por encima de $\alpha = 0.05$ (umbral adoptado habitualmente) los que nos dice que [**no hay evidencias de los datos sean compatibles con afirmar que haya efectos lineales significativos**]{.hl-yellow}

. . .

¬øY si probamos a quitar $\beta_0$ (es decir, la respuesta est√° centrada)?

---

## Inferencia de los par√°metros

Para ello basta a√±adir un `-1` antes de las variables predictoras.

```{r}
lm(data = datos, formula = mass ~ -1 + height) |> summary()
```

F√≠jate que ahora, am√©n que [**hay un efecto lineal significativo de la altura**]{.hl-yellow} y de tener $R^2$ mayor, $\beta_1$ tiene una precisi√≥n mayor. En este caso [**solo pod√≠a quitar uno**]{.hl-yellow} (perder√≠amos $X$), pero veremos m√°s adelante c√≥mo decidir cu√°l quitar si tuvi√©semos varias variables.

# Clases 7 y 8: caso pr√°ctico  {#clase-7-8}

[**¬øDe qu√© depende el precio del vino?**]{style="color:#444442;"}

---

## Caso pr√°ctico

Vamos a poner en pr√°ctica lo aprendido con el dataset `wine.csv` disponible en el campus.

```{r}
datos <- read_csv(file = "./datos/wine.csv")
datos
```

---

## Caso pr√°ctico


El conjunto de datos est√° formado por 27 observaciones (cosechas de vino rojo Burdeos) y 7 variables

* `Year`, `Age`: a√±o de la cosecha y n√∫mero de a√±os en barrica.
* `Price`: precio en 1990-1991 de cada cosecha (en **escala log**)
* `WinterRain`: lluvia (en mm) que cay√≥ ese a√±o en invierno.
* `AGST`: crecimiento medio de la temperatura (en grados Celsius) durante la temporada.
* `HarvestRain`: lluvia (en mm) que cay√≥ ese a√±o durante la cosecha.
* `FrancePop`: poblaci√≥n (miles de habitantes) de Francia.

Ver m√°s en detalles en <https://doi.org/10.1080/09332480.1995.10542468>

. . .

[**El objetivo: predecir el precio**]{.hl-yellow}


---

## Caso pr√°ctico

```{r}
datos
```

Para predecir el precio vamos a usar (de momento) una [**regresi√≥n lineal univariante**]{.hl-yellow}, donde $Y = precio$ y deberemos elegir la predictora $X$ m√°s apropiada.

---

## Pasos a seguir

1. [**An√°lisis exploratorio inicial**]{.hl-yellow}:
  - ¬øLas variables son [**num√©ricas (continuas)**]{.hl-purple}?
  - ¬øTienen [**problemas de rango**]{.hl-purple} (por ejemplo, pesos negativos)? ¬øTienen datos ausentes?
  - ¬øC√≥mo se [**distribuyen las variables**]{.hl-purple}? Ideas: res√∫men num√©rico, histogramas/densidades, boxplots, gr√°ficos de viol√≠n, etc
  - ¬øHay [**datos at√≠picos**]{.hl-purple}?

---

## Pasos a seguir

2. [**An√°lisis de dependencia**]{.hl-yellow}:
  - ¬øQu√© [**predictora est√° m√°s correlacionada (linealmente) con la variable objetivo**]{.hl-purple} a predecir? ¬øExiste otro tipo de dependencia (pendiente implementar en `R`)?
  - ¬øC√≥mo se [**relacionan las predictoras**]{.hl-purple} entre s√≠? ¬øEst√°n correlacionadas? Ideas: matriz de correlaciones, diagramas de dispersi√≥n vs Y, corrplots, etc

. . .

3. [**Formulaci√≥n**]{.hl-yellow} del modelo

. . .

4. [**Fase de estimaci√≥n**]{.hl-yellow}:
  - ¬øCu√°nto valen los [**par√°metros estimados**]{.hl-purple}? ¬øC√≥mo queda el ajuste?
  - ¬øQu√© [**interpretaci√≥n**]{.hl-purple} tienen?


---

## Pasos a seguir

5. [**Fase de diagnosis**]{.hl-yellow} (paquetes `{performance}` y `{olsrr}`):
  - ¬øCumplen los datos las [**hip√≥tesis par√°metricas**]{.hl-purple} requeridas para poder hacer inferencia? 
  - ¬øC√≥mo modificar los datos para que se cumplan?
  - An√°lisis de residuales
  
. . .

6. [**Fase de inferencia**]{.hl-yellow}:
  - ¬øQu√© [**variabilidad**]{.hl-purple} tienen las estimaciones de nuestro par√°metros?
  - ¬øLas predictoras/intercepto tienen un [**efecto lineal significativo**]{.hl-purple}?
  - ¬øDebemos [**re-entrenar el modelo**]{.hl-purple} sin alguno de ellos?

---

## Pasos a seguir

7. [**Fase de evaluaci√≥n**]{.hl-yellow}:
  - ¬øEs significativo el modelo? [**ANOVA: an√°lisis de la varianza**]{.hl-purple}
  - ¬øQu√© informaci√≥n de la predictora explica el modelo? [**Par√°metros de bondad de ajuste**]{.hl-purple} ($R^2$ por ejemplo)

. . .

8. [**Fase de predicci√≥n**]{.hl-yellow}


---

## An√°lisis exploratorio inicial


1. [**An√°lisis exploratorio inicial**]{.hl-yellow}:
  - ¬øLas variables son [**num√©ricas (continuas)**]{.hl-purple}?
  - ¬øTienen [**problemas de rango**]{.hl-purple} (por ejemplo, pesos negativos)? ¬øTienen datos ausentes?
  
&nbsp;

Para responder a dichas preguntas lo m√°s sencillos es hacer uso de la funci√≥n `skim()` del paquete `{skimr}`

```{r}
#| eval: false
library(skimr)
datos |> skim()
```

. . .

En este caso [**no tenemos ausentes ni problemas de codificaci√≥n**]{.hl-green}


---

## An√°lisis exploratorio inicial

  - ¬øC√≥mo se [**distribuyen las variables**]{.hl-purple}?
  - ¬øHay [**datos at√≠picos**]{.hl-purple}?


&nbsp;

Para ello podemos [**visualizar la distribuci√≥n de cada variable**]{.hl-yellow} haciendo uso de densidades, histogramas y/o boxplots (por ejemplo), pero si queremos ahorrar tiempo en visualizar convertimos nuestro dataset en [**tidydata**]{.hl-yellow}

```{r}
datos_tidy <-
  datos |> 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "values")
datos_tidy
```

---

## An√°lisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(x = values)) +
  geom_density(aes(color = variable, fill = variable),
               alpha = 0.75) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

---

## An√°lisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(x = values)) +
  geom_histogram(aes(color = variable, fill = variable),
               alpha = 0.75, bins = 10) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

---

## An√°lisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(x = values)) +
  geom_histogram(aes(color = variable, fill = variable),
               alpha = 0.75, bins = 10) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

---

## An√°lisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(y = values, x = variable)) +
  geom_boxplot(aes(color = variable, fill = variable),
               alpha = 0.75) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

[**No hay valores at√≠picos (respecto a los percentiles)**]{.hl-green}

---

## An√°lisis exploratorio inicial

Podemos incluso gr√°ficar todos los scatter plot sin transformar los datos con `facet_matrix()` del paquete `{ggforce}`

```{r}
#| code-fold: true
library(ggforce)
ggplot(datos) +
  geom_point(aes(x = .panel_x, y = .panel_y)) +
  facet_matrix(vars(everything())) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  theme_minimal()
  
```

---

## An√°lisis exploratorio inicial

Podemos tambi√©n visualizar con un [**scatter plot**]{.hl-yellow} todas las variables y adem√°s sus correlaciones, con el paquete `{GGally}` y la funci√≥n `ggpairs(), sin necesidad de convertir a tidydata.

```{r}
#| code-fold: true
library(GGally)
ggpairs(datos) +
  theme_minimal()
```
   
---


## An√°lisis de dependencia

2. [**An√°lisis de dependencia**]{.hl-yellow}:
  - ¬øQu√© [**predictora est√° m√°s correlacionada (linealmente) con la variable objetivo**]{.hl-purple} a predecir? ¬øExiste otro tipo de dependencia (pendiente implementar en `R`)?
  - ¬øC√≥mo se [**relacionan las predictoras**]{.hl-purple} entre s√≠? ¬øEst√°n correlacionadas? Ideas: matriz de correlaciones, diagramas de dispersi√≥n vs Y, corrplots, etc

. . .

Este paso ser√° crucial en el [**contexto multivariante**]{.hl-yellow} pero en este caso simplemente vamos a ver como se relacionan linealmente las predictoras entre s√≠, y cu√°l de ellas es la **m√°s adecuada para predecir linealmente** `precio`

---

## An√°lisis de dependencia

El primer paso es la [**matriz de correlaciones**]{.hl-yellow} con la funci√≥n `cor()` o con la funci√≥n `correlate()` del paquete `{corrr}` (importa en tibble m√°s visual)

```{r}
library(corrr)
datos |> correlate()
```

. . .

* [**Respecto a Y**]{.hl-yellow}: predictoras con mayor cor lineal son `AGST` (m√°s calor, menos cosechas, sube el precio) y `HarvestRain` (m√°s lluvias, m√°s cosechas, baja el precio, ¬°el signo importa!)

* [**Dependencia entre predictoras**]{.hl-yellow}: las variables `Age`, `Year` y `FrancePop` presentan la misma informaci√≥n.


---

## An√°lisis de dependencia

Tambi√©n podemos usar `corrplot()` del paquete `{corrplot}`, al que le pasamos una matriz de correlaciones cl√°sica y nos la [**visualiza**]{.hl-yellow}.

```{r}
library(corrplot)
datos |> cor() |> corrplot(method = "ellipse")
```

Puedes ver distintas opciones de visualizaci√≥n en <https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html>

---

## An√°lisis de dependencia

Otra opci√≥n es [**visualizar con un scatter plots todas las predictoras vs Y**]{.hl-yellow}, pivotando antes nuestro dataset (solo pivotamos las predictoras)

:::: columns
::: {.column width="65%"}

```{r}
#| code-fold: true
datos_tidy <-
  datos |> pivot_longer(cols = -Price, names_to = "variable",
                        values_to = "values")
ggplot(datos_tidy, aes(x = values, y = Price)) + 
  geom_point(aes(color = variable), alpha = 0.7) +
  geom_smooth(method = "lm") +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal()
```

:::

::: {.column width="35%"}

No solo comprobamos que las rectas con m√°s pendiente son `AGST` y `HarvestRain`, adem√°s los puntos parecen poder ajustarse a una recta sin otro patr√≥n identificable.

:::
::::

Esto es importante hacerlo ya que debemos [**descartar posibles correlaciones esp√∫reas**]{.hl-red} (ver ejemplo datasaurus)

---

## Formulaci√≥n del modelo

Una vez que hemos decidido que dos predictoras usaemos, vamos por tanto a plantear [**dos modelos univariantes**]{.hl-yellow}

$$Price = \beta_0 + \beta_1*AGST + \varepsilon$$
$$Price = \beta_0 + \beta_1*HarvestRain + \varepsilon$$

---

## Fase de estimaci√≥n


4. [**Fase de estimaci√≥n**]{.hl-yellow}:
  - ¬øCu√°nto valen los [**par√°metros estimados**]{.hl-purple}? ¬øC√≥mo queda el ajuste?
  - ¬øQu√© [**interpretaci√≥n**]{.hl-purple} tienen?


&nbsp;

Para ello ejecutaremos ambos modelos con `lm()`

```{r}
ajuste_AGST <- lm(data = datos, formula = Price ~ AGST)
ajuste_harvest <- lm(data = datos, formula = Price ~ HarvestRain)
```


---

## Fase de estimaci√≥n

[**Ajuste con AGST**]{.hl-yellow}

```{r}
ajuste_AGST |> summary()
```

* $\beta_0=$ `r round(ajuste_AGST$coefficients[1], 3)`: predicci√≥n del precio cuando $AGST = 0$ es de -3 (recuerda que est√° en escala logart√≠mica)

* $\beta_1=$ `r round(ajuste_AGST$coefficients[2], 3)`: por cada grado de aumento, el precio sube `r round(ajuste_AGST$coefficients[2], 3)` (escala log).

---

## Fase de estimaci√≥n

[**Ajuste con AGST**]{.hl-yellow}

```{r}
#| echo: false
ajuste_AGST |> summary()
```

* [**Residuales**]{.hl-yellow}: adem√°s de media cero, parecen presentar una distribuci√≥n sim√©trica con la mediana en torno al cero. Adem√°s se tiene que $\widehat{\sigma}_{\varepsilon}^{2}=\frac{1}{n-2}\sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = 0.4819$ (estimador insesgado de la varianza residual) y $R^2 = 0.4456$ (bondad de ajuste)


---

## Fase de estimaci√≥n

[**Ajuste con harvestRain**]{.hl-yellow}


```{r}
ajuste_harvest |> summary()
```

* $\beta_0=$ `r round(ajuste_harvest$coefficients[1], 3)`: predicci√≥n del precio cuando la lluvia fue nula es de 7.679 (recuerda que est√° en escala logart√≠mica)

* $\beta_1=$ `r round(ajuste_harvest$coefficients[2], 3)`: por cada litro de lluvia, precio baja `r round(1000*ajuste_AGST$coefficients[2], 3)` (escala log).

---

## Fase de estimaci√≥n

[**Ajuste con harvestRain**]{.hl-yellow}

```{r}
#| echo: false
ajuste_harvest |> summary()
```

* [**Residuales**]{.hl-yellow}: adem√°s de media cero, parecen presentar una distribuci√≥n sim√©trica con la mediana en torno al cero. Adem√°s se tiene que $\widehat{\sigma}_{\varepsilon}^{2}=\frac{1}{n-2}\sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = 0.5577$ (algo m√°s grande que el otro ajuste) y $R^2 = 0.2572$ (algo m√°s peque√±o que el otro ajuste) -> de momento es mejor el primer modelo.

---

## Fase de diagnosis

5. [**Fase de diagnosis**]{.hl-yellow} (paquetes `{performance}` y `{olsrr}`):
  - ¬øCumplen los datos las [**hip√≥tesis par√°metricas**]{.hl-purple} requeridas para poder hacer inferencia? 
  - ¬øC√≥mo modificar los datos para que se cumplan?
  - An√°lisis de residuales
  
&nbsp;

Recuerda que [**necesitamos verificar antes las hip√≥tesis**]{.hl-yellow} para poder hacer inferencia con los par√°metros, as√≠ que vamos a ello con el paquete `{performance}` y el paquete `{olsrr}`

---

## Fase de diagnosis

```{r}
library(performance)
check_model(ajuste_AGST)
```

---

## Diagnosis: linealidad

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1 x$


```{r}
#| code-fold: true
check_model(ajuste_AGST)
```

Si te fijas el gr√°fico que se refiere a ello est√° [**visualizando residuales vs valores estimados**]{.hl-yellow}: est√° volviendo a plantear un segundo modelo de regresi√≥n donde ahora $\widehat{\varepsilon}_i = \gamma_0 + \gamma_1 \widehat{y}_i$

---

## Diagnosis: linealidad

```{r}
linealidad <- lm(data = tibble("fitted" = ajuste_AGST$fitted.values,
                               "residuals" = ajuste_AGST$residuals),
                 formula = residuals ~ fitted)
linealidad |> summary()
```

Si te fijas [**ambos par√°metros no son significativamente distintos de 0**]{.hl-green}: no presentan una tendencia (lineal al menos)

---

## Diagnosis: linealidad

M√°s adelante probaremos alguna otra cosa pero de momento nos basta con eso. Tambi√©n podemos [**visualizar nosotros ese scatter plot residuales vs estimaciones**]{.hl-yellow}

```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "residuals" = ajuste_AGST$residuals),
       aes(x = fitted, y = residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm") +
  theme_minimal()
```

---

## Diagnosis: homocedasticidad

2. [**Homocedasticidad**]{.hl-green}: necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow}, tal que $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \boldsymbol{X} = x \right] = cte < \infty$.

```{r}
check_heteroscedasticity(ajuste_AGST)
```

. . .

El gr√°fico titulado `Homogeneity of variance` nos visualiza la ra√≠z cuadrada del valor absoluto de los residuos estandarizados frente a las predicciones (se conoce como [**gr√°fico de escala-localizaci√≥n**]{.hl-yellow})

---

## Diagnosis: homocedasticidad

Si visualizamos los [**residuales**]{.hl-yellow} deber√≠an estar en torno a 0, dentro de una banda constante (varianza constante)


```{r}
#| code-fold: true
ggplot(tibble("id" = 1:length(ajuste_AGST$residuals),
              "residuals" = ajuste_AGST$residuals),
       aes(x = id, y = residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

---


## Diagnosis: homocedasticidad

Si visualizamos el gr√°fico de [**escala-localizaci√≥n**]{.hl-yellow} deber√≠amos obtener un diagrama de dispersi√≥n cuya recta de regresi√≥n saliese casi plana en torno al 1.


```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "sqrt_std_residuals" = sqrt(abs((ajuste_AGST$residuals - mean(ajuste_AGST$residuals)) / sd(ajuste_AGST$residuals)))),
       aes(x = fitted, y = sqrt_std_residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

Seg√∫n el gr√°fico no deber√≠amos asumir homocedasticidad. **¬øPor qu√© el contraste no la rechaza?**

---

## Diagnosis: homocedasticidad


```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "sqrt_std_residuals" = sqrt(abs((ajuste_AGST$residuals - mean(ajuste_AGST$residuals)) / sd(ajuste_AGST$residuals)))),
       aes(x = fitted, y = sqrt_std_residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

Con el poco tama√±o muestral que tenemos, es complicado tener evidencias que refuten la hip√≥tesis nula (y el gr√°fico puede estar parcialmente dise√±ado). Por eso es la [**hip√≥tesis m√°s dif√≠cil de cumplir**]{.hl-yellow}. Lo importante es que en [**la recta de regresi√≥n al dibujar los residuos no se aprecia una banda cuya anchura se modifique groseramente**]{.hl-green}, m√°s o menos constante


---

## Diagnosis: normalidad

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$

Con la funci√≥n `ols_test_normality()` del paquete `{olsrr}` podemos obtener diferentes contrastes de normalidad

```{r}
library(olsrr)
ols_test_normality(ajuste_AGST)
```

Nos centraremos en los contrastes de `Shapiro-Wilk`, `Kolmogorov-Smirnov` y `Anderson-Darling`: [**no se rechaza normalidad**]{.hl-yellow}

---

## Diagnosis: normalidad

Adem√°s del contraste podemos visualizar con `stat_qq()` y `stat_qq_line()` el conocido como [**Q-Q plot**]{.hl-yellow}: enfrenta los cuantiles de una muestra con los cuantiles de una normal te√≥rica, teniendo que **obtener los puntos en torno a una recta** (especilamente en el centro).


```{r}
#| code-fold: true
ggplot(tibble("residuals" = ajuste_AGST$residuals)) +
  stat_qq(aes(sample = residuals)) +
  stat_qq_line(aes(sample = residuals))
```


---

## Diagnosis: independencia

4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser independientes entre s√≠ (el error en una observaci√≥n no depende de otras). En particular, ser√°n **incorrelados**

$${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] - E \left[\varepsilon_i \right] E \left[\varepsilon_j \right] = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$$

```{r}
check_autocorrelation(ajuste_AGST)
```

Por √∫ltimo, `check_autocorrelation()` comprueba como efectivamente los [**residuales/errores son independientes**]{.hl-yellow}, haciendo un test de autocorrelaci√≥n (nos tiene que salir lo contrario a una serie temporal, que el error i no depende del i-1).

---

## Diagnosis: independencia

Otra forma de verlo es [**visualizando los residuos respecto a su versi√≥n con retardo**]{.hl-yellow} (por ejemplo, $\left(\widehat{\varepsilon}_1, \widehat{\varepsilon}_2, \widehat{\varepsilon}_3, \ldots, \widehat{\varepsilon}_{n-1} \right)$ vs $\left(\widehat{\varepsilon}_2, \widehat{\varepsilon}_3, \ldots, \widehat{\varepsilon}_n \right)$ 

```{r}
#| code-fold: true
ggplot(tibble("lag1" = ajuste_AGST$residuals[-length(ajuste_AGST$residuals)],
              "residuals" = ajuste_AGST$residuals[-1]),
       aes(x = residuals, y = lag1)) +
  geom_point(size = 3, alpha = 0.7) + 
  geom_smooth(method = "lm") + 
  theme_minimal()
```

---

## Fase de diagnosis

```{r}
check_model(ajuste_AGST)
```

En nuestro caso se cumplen todas las hip√≥tesis (algunas m√°s fuertemente que otras).

[**Repite el proceso con el otro modelo**]{.hl-yellow}

---

## Fase de diagnosis

```{r}
check_predictions(ajuste_AGST)
```

Nos faltan dos gr√°ficas por comentar:

* `Posterior Predictive Checks`: [**simula distintas variables respuesta**]{.hl-yellow} suponiendo que el modelo fuese cierto (a√±adiendo ruido aleatorio) y lo compara con la muestra. [**Si lo observado se distancia mucho de las simulaciones**]{.hl-red} es que el modelo planteado no ajusta bien a la muestra.

---

## Fase de diagnosis

```{r}
check_outliers(ajuste_AGST)
```

* `Influential Observations`: nos permite identificar [**observaciones influyentes**]{.hl-yellow}, marcando aquellas (con su id de fila) que se salgan fuera de la banda definida por la conocida como **distancia de Cook** denotada como $D_i$ (realiza, para cada observaci√≥n, la suma de todos los cambios de la regresi√≥n cuando la observaci√≥n $i$ es retirada: si hay muchos cambios al cambiar una observaci√≥n, es que era muy influyente)

. . .

Diferencia dos tipos: 

* [**outliers**]{.hl-yellow}: valor at√≠pico de la **respuesta** pudiendo perturbar la varianza residual
* [**high-leverage points**]{.hl-yellow}: valor at√≠pico en alguna de las **predictoras**

---

## Fase de inferencia

6. [**Fase de inferencia**]{.hl-yellow}:
  - ¬øQu√© [**variabilidad**]{.hl-purple} tienen las estimaciones de nuestro par√°metros?
  - ¬øLas predictoras/intercepto tienen un [**efecto lineal significativo**]{.hl-purple}?
  - ¬øDebemos [**re-entrenar el modelo**]{.hl-purple} sin alguno de ellos?

&nbsp;

Una vez verificadas las hip√≥tesis lo que haremos ser√° [**inferir conclusiones de la poblaci√≥n en funci√≥n de la muestra**]{.hl-yellow}

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Variabilidad**]{.hl-yellow} de las estimaciones de nuestro par√°metros

  - $\widehat{SE} \left( \widehat{\beta}_0 \right)$ igual a 2.344 por lo que (aprox) $\widehat{\beta_0} \sim N(-3.547, 2.344)$
  - $\widehat{SE} \left( \widehat{\beta}_1 \right)$ igual a 0.143  por lo que (aprox) que $\widehat{\beta_1} \sim N(0.643, 0.143)$
  

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Estad√≠stico**]{.hl-yellow} del contraste

  - $\frac{\widehat{\beta}_0 - 0}{\widehat{SE} \left( \widehat{\beta}_0 \right)}$ igual a -1.5 (valor que tendr√≠as que buscar en las tablas a mano)
  - $\frac{\widehat{\beta}_1 - 0}{\widehat{SE} \left( \widehat{\beta}_1 \right)}$ igual a 4.483 (valor que tendr√≠as que buscar en las tablas a mano)
  
  
  
---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Efecto (lineal)**]{.hl-yellow}: si nos fijamos en la tabla, el p-valor de $\beta_0$ es 0.146052. Si adoptamos $\alpha = 0.05$ como suele ser habitual, el contraste $H_0:~\beta_0 = 0$ vs $H_1:~\beta_0 \neq 0$ nos dice que [**no podemos rechazar de forma significativa la hip√≥tesis nula**]{.hl-red} (no sucede con $\beta_1$, si sucediese no habr√≠a modelo)

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

[**¬øY si quitamos dicho par√°metro?**]{.hl-yellow}

. . .

Para quitarlo a√±adimos un -1 al modelo

```{r}
ajuste_AGST_sin_beta0 <- lm(data = datos, formula = Price ~ -1 + AGST)
```
 
---

## Re-aprendiendo

```{r}
ajuste_AGST_sin_beta0 |> summary()
```

* La [**bondad de ajuste**]{.hl-yellow} ha pasado de $R^2 = 0.446$ a $R^2 = 0.9953$

* La [**variabilidad de la estimaci√≥n**]{.hl-yellow} $\widehat{SE} \left( \widehat{\beta}_1 \right)$  ha pasado de 0.143 a 0.005757.

---

## Comparar modelos

Aunque no hemos hablado en profundidad de las **m√©tricas de evaluaci√≥n** podemos [**comparar los modelos**]{.hl-yellow} con `compare_performance()` del paquete `{performance}`

```{r}
#| echo: false
ajuste_harvest_sin_beta0 <- lm(data = datos, formula = Price ~ -1 + HarvestRain)
```
```{r}
compare_performance(ajuste_AGST, ajuste_AGST_sin_beta0,
                    ajuste_harvest, ajuste_harvest_sin_beta0)
```

# Clases 9: evaluaci√≥n y predicci√≥n {#clase-9}

[**¬øC√≥mo se puede evaluar un modelo? ¬øQu√© m√©tricas existen? ¬øC√≥mo predecir?**]{style="color:#444442;"}

---

## Repitamos el proceso


Para interiorizar lo aprendido, vamos a repetir todo el proceso con el conjunto `datos_linearreg.csv` (las variables predictoras representan diferentes variables meteorol√≥gicas y la variable objetivo `y` la temperatura media en primavera, para distintas ciudades).

```{r}
datos <- read_csv(file = "./datos/datos_linearreg.csv")
datos
```

---

## Regresi√≥n lineal

Debes seguir los siguientes pasos de la manera m√°s detallada posible

1. [**An√°lisis exploratorio inicial**]{.hl-yellow} tanto num√©rico como visualizando. ¬øSon num√©ricas sin problemas de codificaci√≥n? ¬øC√≥mo se distribuyen? ¬øHay datos at√≠picos?

2. [**An√°lisis de dependencia**]{.hl-yellow}. ¬øQu√© predictora correlaciona m√°s con la objetivo? ¬øC√≥mo se relacionan las predictoras entre s√≠?

3. [**Formulaci√≥n**]{.hl-yellow} del modelo

4. [**Fase de estimaci√≥n**]{.hl-yellow}. ¬øCu√°les son los par√°metros? ¬øC√≥mo se interpretan?

5. [**Fase de diagnosis**]{.hl-yellow}

6. [**Fase de inferencia**]{.hl-yellow}. ¬øQu√© variabilidad tiene la estimaci√≥n? ¬øHay efecto significativo? 


---

## Fase de evaluaci√≥n

```{r}
ajuste_lineal <- lm(data = datos, formula = y ~ x1)
ajuste_lineal |> summary()
```

Ten√≠amos pendiente la fase final: [**fase de evaluaci√≥n**]{.hl-yellow}

--- 

## Fase de evaluaci√≥n

7. [**Fase de evaluaci√≥n**]{.hl-yellow}:
  - ¬øEs significativo el modelo? [**ANOVA: an√°lisis de la varianza**]{.hl-purple}
  - ¬øQu√© informaci√≥n de la predictora explica el modelo? [**Par√°metros de bondad de ajuste**]{.hl-purple} ($R^2$ por ejemplo)
  - ¬øQu√© otras m√©tricas o herramientas podemos usar para [**cuantificar la calidad predictora de nuestro ajuste**]{.hl-purple}
  
. . . 

Una de las herramientas m√°s √∫tiles para evaluar nuestro modelo es [**enfrentar los valores ajustados con los valores reales**]{.hl-yellow} (dado que los conocemos al ser aprendizaje supervisado)

---

## Fase de evaluaci√≥n


```{r}
#| code-fold: true
ggplot(tibble("y" = datos$y, "y_est" = ajuste_lineal$fitted.values),
       aes(x = y, y = y_est)) +
  geom_point(size = 1.2, alpha = 0.75) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  labs(x = "Valores reales", y = "Valores estimados")
```

En el gr√°fica podemos ver como los [**valores reales vs estimados**]{.hl-green} est√°n muy cercanos a la diagonal: el error cometido es muy peque√±o.

---

## Fase de evaluaci√≥n

Podemos considerar algunas [**m√©tricas para cuantificar el acierto del modelo**]{.hl-yellow}

. . .

* [**SSE**]{.hl-red} (sum of squared errors): definido como la suma de errores al cuadrado

$$SSE = \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 = \sum_{i=1}^{n} \left( Y_i - \widehat{Y}_i \right)^2$$
F√≠jate que dado que $\widehat{\sigma}_{\varepsilon} = \widehat{\sigma}_{r} = \frac{n}{n-p-1} s_{r}^{2} =  \frac{1}{n-p-1} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2$ (el que `R` llama `Residual standard error`), tenemos que $SSE = (n-p-1)\widehat{\sigma}_{\varepsilon}$

. . .

* [**MSE**]{.hl-red}: media de lo anterior $MSE = s_{r}^{2} = \frac{1}{n} SSE  = \frac{1}{n} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 = \frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}$

. . .

Ambas m√©tricas nos hablan de la [**varianza del error**]{.hl-red}, es decir, de lo que el [**modelo no es capaz de explicar**]{.hl-red}

---


## Fase de evaluaci√≥n



* [**SSR**]{.hl-green} (regressions sum of squares): definido como la suma de las desviaciones de cada predicci√≥n a su media (al tener estimadores insesgados, la media de las estimaciones $\overline{\widehat{Y}}$ es la misma que la de la variable a estimar $\overline{Y}$)

$$SSR = \sum_{i=1}^{n} \left( \overline{Y} - \widehat{Y}_i \right)^2 = \sum_{i=1}^{n} \left( \overline{\widehat{Y}}_i - \widehat{Y}_i \right)^2$$

. . .


* [**MSR**]{.hl-green}: media de lo anterior $MSR = s_{\widehat{y}}^2 = \frac{1}{n} SSR  = \frac{1}{n} \sum_{i=1}^{n} \left( \overline{Y} - \widehat{Y}_i \right)^2$

. . .

Ambas m√©tricas nos hablan de la [**variaci√≥n de Y en torno a la regresi√≥n**]{.hl-green}, es decir, la variaci√≥n de $\overline{Y}$ que es explicada por la media condicional estimada $(Y_i|X=x_i) \sim \widehat{\beta}_0 + \widehat{\beta}_1 X_i$, cuantifica la [**informaci√≥n de Y explicada por el modelo**]{.hl-green}

---

## Fase de evaluaci√≥n


* [**SST**]{.hl-yellow} (total sum of squares): definido como la suma de las desviaciones de la variable objetivo $Y$ a su media.

$$SST =  \sum_{i=1}^{n} \left( Y_i - \overline{Y} \right)^2 $$

. . .

* [**Varianza muestral de Y**]{.hl-yellow}: la media de lo anterior (la varianza de $Y$)

$$s_{y}^2= \frac{1}{n} SST  = \frac{1}{n}\sum_{i=1}^{n} \left( Y_i - \overline{Y} \right)^2$$



. . .


Ambas m√©tricas nos hablan de la [**variaci√≥n total de Y**]{.hl-yellow}, es decir, la [**informaci√≥n total de nuestra variable objetivo**]{.hl-yellow}

---

## ANOVA

As√≠ tenemos 3 tipos de m√©tricas:

* `SST` y $s_{y}^2$: el [**total de info a explicar**]{.hl-yellow}

* `SSR` y $MSR$: el [**total de info explicada por el modelo**]{.hl-green}

* `SSE` y $MSE$: el [**total de info NO explicada por el modelo**]{.hl-red} (a veces se usa la ra√≠z cuadrada del MSE, conocido como $RMSE$, o el $MAE$, tomando valor absoluto en los errores).

. . .

Se pueden demostrar que, [**SOLO EN EL CASO LINEAL**]{.hl-purple}, desarrollando el sumatorio $SST =  \sum_{i=1}^{n} \left( Y_i - \overline{Y} \right)^2 = \sum_{i=1}^{n} \left( \left(Y_i - \widehat{Y}_i \right) +  \left(  \widehat{Y}_i - \overline{Y} \right) \right)^2$ se llega a que

[**SST**]{.hl-yellow} =  [**SSR**]{.hl-green} + [**SSE**]{.hl-red}

---


## ANOVA

[**SST**]{.hl-yellow} =  [**SSR**]{.hl-green} + [**SSE**]{.hl-red}


. . .

[$s_{y^2}$]{.hl-yellow} = [$s_{\widehat{y}}^2$]{.hl-green} + [$s_{r}^2$]{.hl-red} (equivalente, [$s_{y^2}$]{.hl-yellow} = [$s_{\widehat{y}}^2$]{.hl-green} + [$\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}$]{.hl-red})

. . .

Esta decomposici√≥n (solo se cumple en el caso lineal) se conoce como [**ANOVA o an√°lisis de la varianza**]{.hl-yellow} y podemos hacerlo en `R` con `aov()` o `anova()`

:::: columns
::: {.column width="45%"}

```{r}
ajuste_lineal |> aov()
```

:::

::: {.column width="55%"}

```{r}
ajuste_lineal |> anova()
```

:::
::::

---

## ANOVA

```{r}
ajuste_lineal |> aov()
```

| Terms | x1 (predictora) | Residuals |
|:---------:|:-----:|:------:|
| Sum of Squares  | SSR   |    SSE | 
| Deg. of Freedom  (grados libertad)  | p  |  n - p - 1 |

&nbsp;

`Residual standard error`: $\widehat{\sigma}_{\varepsilon}^{2}= \frac{n}{n-p-1} s_{r}^{2} = \frac{1}{n-p-1} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2}$

---

## ANOVA

```{r}
ajuste_lineal |> anova()
```

|  | Df (grados) | Sum Sq | Mean Sq | F value | Pr(>F) |
|:---------:|:-----:|:------:|:------:|:------:|:------:|
| x1  | p  | SSR | $\frac{SSR}{p}$ | $F-value = \frac{\frac{SSR}{p}}{\frac{SSE}{n-p-1}}$ | p-valor F test
| Residuals  | n - p - 1 | SSE | $\widehat{\sigma}_{\varepsilon}^{2} = \frac{SSE}{n-p-1}$ | | |

El F-value es el estad√≠tico  $F = \frac{\frac{SSR}{p}}{\frac{SSE}{n-p-1}} \sim F_{p, n-p-1}$ asociado al [**contraste de significaci√≥n global**]{.hl-yellow}

$$H_0:~\beta_1 = \ldots = \beta_p = 0 \quad vs \quad H_1:~\text{existe al menos un} \quad \beta_j \neq 0$$

---

## ANOVA

```{r}
ajuste_lineal |> anova()
```


$$H_0:~\beta_1 = \ldots = \beta_p = 0 \quad vs \quad H_1:~\text{existe al menos un} \quad \beta_j \neq 0$$

El contraste pretende responder a: [**¬øexiste una dependencia lineal entre $Y$ y el CONJUNTO de predictoras?**]{.hl-yellow} (global, no par√°metro a par√°metro).

. . .

Si se [**rechaza**]{.hl-yellow} significa que [**existe al menos un predictor cuyo efecto LINEAL sobre Y es significativo**]{.hl-yellow}.

. . .

[**Importante**]{.hl-red}: en el caso de la reg. lineal univariante, $F-value$ y $p-value$ del ANOVA es equivalente al $t-value$ y $p-value$ del contraste de significaci√≥n para $\beta_1$ (ya que...no hay m√°s).

---

## Bondad de ajuste

El conocido como $R^2$ o [**bondad de ajuste**]{.hl-yellow} o el [**coeficiente de determinaci√≥n**]{.hl-yellow} es una forma sencilla, y relacionada con el ANOVA, para cuantificar la [**cantidad de informaci√≥n explicada por el modelo**]{.hl-yellow}

. . .

De hecho es, literal, un [**ratio de informaci√≥n explicada**]{.hl-yellow} (lo que `R` llama `Multiple R-squared`, ya hablaremos de su versi√≥n ajustada en regresi√≥n multivariante)

$$R^2 = \text{Ratio info explicada} = \frac{SSR}{SST}$$

. . .

En el caso lineal, por lo visto en el ANOVA

$$R^2= \frac{SSR}{SST} = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST} = 1 - \frac{s_{r}^{2}}{s_{y}^{2}} =  1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^{2}}{s_{y}^{2}}$$

. . .

Por definici√≥n, la [**bondad de ajuste est√° entre 0 y 1**]{.hl-yellow}.

---

## Bondad de ajuste

[**Deberes: demuestra que $R^2 = r_{y \widehat{y}}^2$ y que en el caso de $p=1$ coincide adem√°s con $R^2 = r_{x,y}^2$**]{.hl-yellow}




$$R^2 = r_{y \widehat{y}}^2 =_{p=1} r_{x,y}^2$$

. . .

La bondad de ajuste tiene un [**problema importante**]{.hl-red}: no solo depende del modelo sino que tambi√©n de los datos. ¬øDe qu√© depende? ¬øPor qu√© es [**peligroso usar ciegamente $R^2$ para decidir**]{.hl-red}?



---

## Bondad de ajuste

$$R^2= \frac{SSR}{SST} = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST} = 1 - \frac{s_{r}^{2}}{s_{y}^{2}} =  1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^{2}}{s_{y}^{2}}$$

¬øDe qu√© depende?

. . .

* [**M√°s predictoras**]{.hl-yellow} implica que [**$R^2$ crece**]{.hl-yellow}, ¬°incluso aunque dichas [**predictoras no sean significativas**]{.hl-red}! (esto lo arreglaremos con el $R^2$ ajustado en el futuro)

* [**M√°s ruido, menor $R^2$**]{.hl-yellow}

* [**Ignora si el modelo cumple las hip√≥tesis**]{.hl-yellow}: un modelo con un alto $R^2$ puede dar predicciones nefastas.
  
  
---

## Predictoras vs R2

Vamos a repetir pero [**a√±adiendo 5 y 22 variables m√°s**]{.hl-yellow}, sin efecto lineal con $Y$.

```{r}
datos_extras <-
  tibble("y" = datos$y, "x1" = datos$x1, "x2" = rnorm(1e3), "x3" = rnorm(1e3),
         "x4" = rnorm(1e3), "x5" = rnorm(1e3), "x6" = rnorm(1e3), "x7" = rnorm(1e3),
         "x8" = rnorm(1e3), "x9" = rnorm(1e3), "x10" = rnorm(1e3), "x11" = rnorm(1e3),
         "x12" = rnorm(1e3), "x13" = rnorm(1e3), "x14" = rnorm(1e3), "x15" = rnorm(1e3),
         "x16" = rnorm(1e3), "x17" = rnorm(1e3), "x18" = rnorm(1e3), "x19" = rnorm(1e3),
         "x20" = rnorm(1e3), "x21" = rnorm(1e3), "x22" = rnorm(1e3), "x23" = rnorm(1e3))
ajuste_6pred <- lm(data = datos_extras, formula = y ~ x1 + x2 + x3 + x4 + x5 + x6)
ajuste_23pred <- lm(data = datos_extras, formula = y ~ .)
compare_performance(ajuste_lineal, ajuste_6pred, ajuste_23pred)
```

Con `compare_performance()` podemos comparar m√©tricas de modelos.

---

## Ruido vs R2

Hemos dicho que el ruido afecta...¬øqu√© crees que **pasar√° si fijamos la parte determin√≠stica y solo modificamos el ruido**? Vamos a [**simular 6 modelos**]{.hl-yellow}, con exactamente los mismos $\beta_0$ y $\beta_1$ (es decir, **mismo ajuste**) pero con diferente varianza en el ruido (supongamos que $X \sim N(3, 1.5)$).

$$\text{Modelo 1: } Y = -1.2 + 3.2X + N(0, 0.25)$$
$$\text{Modelo 2: } Y = -1.2 + 3.2X + N(0, 1)$$

$$\text{Modelo 3: } Y = -1.2 + 3.2X + N(0, 1.5)$$

$$\text{Modelo 4: } Y = -1.2 + 3.2X + N(0, 2)$$

$$\text{Modelo 5: } Y = -1.2 + 3.2X + N(0, 4)$$


$$\text{Modelo 6: } Y = -1.2 + 3.2X + N(0, 8)$$

---

## Ruido vs R2


```{r}
#| code-fold: true
x <- rnorm(n = 1000, mean = 3, sd = 1.5)
eps1 <- rnorm(n = 1000, mean = 0, sd = 0.25)
eps2 <- rnorm(n = 1000, mean = 0, sd = 1)
eps3 <- rnorm(n = 1000, mean = 0, sd = 1.5)
eps4 <- rnorm(n = 1000, mean = 0, sd = 2)
eps5 <- rnorm(n = 1000, mean = 0, sd = 4)
eps6 <- rnorm(n = 1000, mean = 0, sd = 8)
datos <- tibble("x" = x, "y1" = -1.2 + 3.2*x + eps1,
                "y2" = -1.2 + 3.2*x + eps2, "y3" = -1.2 + 3.2*x + eps3,
                "y4" = -1.2 + 3.2*x + eps4, "y5" = -1.2 + 3.2*x + eps5,
                "y6" = -1.2 + 3.2*x + eps6)
ajuste_1 <- lm(data = datos, formula = y1 ~ x)
ajuste_2 <- lm(data = datos, formula = y2 ~ x)
ajuste_3 <- lm(data = datos, formula = y3 ~ x)
ajuste_4 <- lm(data = datos, formula = y4 ~ x)
ajuste_5 <- lm(data = datos, formula = y5 ~ x)
ajuste_6 <- lm(data = datos, formula = y6 ~ x)
```

```{r}
compare_performance(ajuste_1, ajuste_2, ajuste_3, ajuste_4, ajuste_5, ajuste_6)
```

---

## Ruido vs R2

En todos los casos el [**ajuste debe ser (aprox) el mismo**]{.hl-yellow} ya que el modelo de regresi√≥n busca [**modelizar los efectos lineales no aleatorios**]{.hl-yellow} entre la variable objetivo y los predictores.

. . .

[**Moraleja**]{.hl-green}: tener un $R^2$ no implica que el modelo sea malo, ya que la [**cantidad de informaci√≥n no modelizable**]{.hl-yellow} puede deberse a una [**cantidad alta de ruido (algo aleatorio no predecible)**]{.hl-red}. Por ello es importante usar m√°s herramientas que un mero coeficiente para valorar un ajuste (por ejemplo, en campos como la sociolog√≠a o la econom√≠a la bondad de ajuste ser√° generalmente bajo)

. . .

&nbsp;

[**Deberes**]{.hl-yellow}: ¬øc√≥mo ilustrar gr√°ficamente que a mayor varianza del ruido, menor es $R^2$? Dise√±a un estudio de simulaci√≥n para ello con distintos modelos y gr√°fica la ca√≠da de $R^2$.

---

## Diagnosis vs R2

Entonces, si tenemos un modelo con un alto $R^2$, [**¬øno hace falta que cumpla las hip√≥tesis?**]{.hl-yellow}

. . .

Vamos a simular un modelo que [**incumple**]{.hl-red}

* Linealidad

* Homocedasticidad

. . .

La variable predictora $x_i = 0.01 + 0.01*(i-1)$ ($i = 1,\ldots, n = 200$) ser√° la siguiente

```{r}
x <- seq(0.01, 2, l = 200)
```


[**¬øC√≥mo podr√≠amos crear una $Y$ cuya relaci√≥n con $X$ sea no lineal?**]{.hl-yellow}

---

## Diagnosis vs R2

[**¬øC√≥mo podr√≠amos crear una $Y$ cuya relaci√≥n con $X$ sea no lineal?**]{.hl-yellow}

Tenemos muchas maneras, por ejemplo:

$$Y = X + X^2 + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

$$Y = \log(X^2) - cos(X) + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

$$Y = \frac{1}{X + 1} + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

$$Y = 1 - 2X(1 + 0.25 \sin(4 \pi X)) + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

. . .

Simula este √∫ltimo modelo (con $\sigma_{\varepsilon} = 0.5$) y realiza el ajuste

---

## Diagnosis vs R2

```{r}
x <- seq(0.15, 2, l = 200)
eps <- rnorm(n = 200, sd = 0.5)
y <- 1 - 2 * x * (1 + 0.25*sin(4 * pi * x)) + eps
datos <- tibble("y" = y, "x" = x)
ajuste_lineal <- lm(data = datos, formula = y ~ x)
ajuste_lineal |> summary()
```

. . .

[**¬øC√≥mo podr√≠amos hacer que la hip√≥tesis de homocedasticidad no se cumpla**]{.hl-yellow}


---

## Diagnosis vs R2

Vamos a considerar que la [**varianza del error no es cte, crece seg√∫n aumenta x**]{.hl-yellow}

$$y_i = f(x_i) + \varepsilon_i, \quad \varepsilon_i \sim N(0, \sigma_{\varepsilon,i}), \quad \sigma_{\varepsilon,i} = 0.25 * x_{i}^2 \quad i=1, \ldots, 200$$

```{r}
#| code-fold: true
x <- seq(0.01, 2, l = 200)
eps <- rnorm(n = 200, sd = 0.25 * x^2)
y <- 1 - 2 * x * (1 + 0.25*sin(4 * pi * x)) + eps
datos <- tibble("y" = y, "x" = x, "sigma" =  0.25 * x^2)
ajuste_lineal <- lm(data = datos, formula = y ~ x)
ajuste_lineal |> summary()
```

---

## Diagnosis vs R2

```{r}
#| code-fold: true
ggplot(datos, aes(x = x, y = y)) +
  geom_point(aes(color = sigma), size = 3, alpha = 0.75) +
  geom_smooth(method = "lm") +
  theme_minimal()
```

Aunque el $R^2$ es bastante alto, el modelo no tiene sentido, dando [**estimaciones cada vez m√°s erradas**]{.hl-red} seg√∫n nos movemos hacia la derecha en el eje X.


---

## Cosicas a examen

Algunas cosas que podr√≠an caer en la primera entrega:

. . .

* Realizar un [**an√°lisis exploratorio inicial**]{.hl-yellow} lo m√°s completo posible. Elegir de manera justificada [**la mejor predictora**]{.hl-yellow}

. . .

* Saber [**ajustar e INTERPRETAR**]{.hl-yellow} un modelo de regresi√≥n.

. . .

* Realizar e interpretar la [**fase de diagnosis**]{.hl-yellow} lo m√°s completa posible para luego interpretar (si procede) la [**inferencia**]{.hl-yellow} del modelo.

. . .

* Saber hacer una [**correcta fase de evaluaci√≥n**]{.hl-yellow} dle modelo.

. . .

* Saber [**simular modelos de regresi√≥n**]{.hl-yellow} (tanto que cumplan las hip√≥tesis como que no las cumplan).


# Clase 10: predicci√≥n y resumen {#clase-10}

[**Fase de predicci√≥n. Resumen**]{style="color:#444442;"}


---

## Resumen

Un breve resumen de lo aprendido sobre reg. lineal univariante

* [**Modelo supervisado de predicci√≥n**]{.hl-yellow}: hay una variable objetivo $Y$ continua (num√©rica) cuyo valor real conocemos

. . .

* [**La visualizaci√≥n importa**]{.hl-yellow}: no fies tu an√°lisis solo a los par√°metros matem√°ticos, la visualizaci√≥n ayuda a entender los datos.

. . .

* [**Relaci√≥n entre variables**]{.hl-yellow}: buscamos predictoras muy correladas (linealmente) con $Y$ y lo m√°s incorreladas/independientes entre ellas.

. . .

* [**Modelo**]{.hl-yellow}: el modelo par√°metrico se resume en $Y = \beta_0 + \beta_1 X + \varepsilon$, donde $\varepsilon$ es una variable aleatoria (ruido).


. . .

* [**Estimaci√≥n**]{.hl-yellow}: la estimaci√≥n viene modelizada bajo la hip√≥tesis de linealidad

$$E[Y|X=x] = \beta_0 + \beta_1 x$$




# Clase 11: entrega I {#clase-11}


[**Entrega I**]{style="color:#444442;"}

# Clase 12: incumpliendo hip√≥tesis {#clase-12}

[**¬øC√≥mo simular datos que incumplan las hip√≥tesis?**]{style="color:#444442;"}

---

## Incumpliendo hip√≥tesis

Los contenidos vistos en esta clase se subir√°n resumidos en formato notebook.


# Clase 13: reg. multivariante {#clase-13}

[**Introducci√≥n a la regresi√≥n multivariante. Formulaci√≥n del modelo y estimaci√≥n**]{style="color:#444442;"}

## Formulaci√≥n multivariante

De aqu√≠ en adelante llamaremos [**modelo multivariante**]{.hl-yellow} a todo modelo en el que $p > 1$ (es decir, tenemos m√°s de una variable predictora).


$$Y = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right)$$
tal que $E \left[ \varepsilon | \left( X_1 = x_1, \ldots, X_p = x_p \right) \right] = 0$.

. . .

En el caso del [**modelo lineal multivariante**]{.hl-yellow} se traducir√° por tanto en


$$E \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p, \quad \widehat{Y} = \widehat{\beta_0} + \displaystyle \sum_{j=1}^{p} \widehat{\beta_j} X_j $$

El objetivo seguir√° siendo obtener la estimaci√≥n de los $\widehat{\beta}$ tal que [**minimicemos el error**]{.hl-yellow}

---

## Formulaci√≥n matricial


Su formulaci√≥n muestral la podemos expresar mediante la [**matriz de dise√±o**]{.hl-yellow}

$$\mathbf{X} =\begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}$$

tal que $Y_i = \beta_0 + \sum_{j=1}^{p} \beta_j X_j + \varepsilon_i$, para todo elemento de la muestra $i=1,\ldots, n$


$$ \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix}_{(p+1)\times1} + \begin{pmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{pmatrix}_{n\times1}$$


---

## Formulaci√≥n matricial

La estimaci√≥n ser√° por tanto


$$\widehat{\mathbf{Y}} = \begin{pmatrix} \widehat{Y}_1 \\ \widehat{Y}_2 \\ \vdots \\ \widehat{Y}_n \end{pmatrix}_{n\times1} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \widehat{\beta}_1 \\ \widehat{\beta}_2 \\ \vdots \\ \widehat{\beta}_p \end{pmatrix}_{(p+1)\times1}$$


---

## Varianza residual

Como suced√≠a antes, el objetivo ser√° minimizar la [**varianza residual o error cuadr√°tico medio**]{.hl-yellow} (varianza del error), o lo que hemos llamado $SSE$

$$SSE \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \displaystyle \sum_{i=1}^{n} \left(Y_i - \beta_0 - \beta_1 X_{i1} - \ldots - \beta_p X_{ip} \right)^2 $$
¬øC√≥mo quedar√≠a **matricialmente**?

. . .

$$SSE \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = \varepsilon^{T} \varepsilon$$

As√≠, aplicando el [**m√©todo de los m√≠nimos cuadrados**]{.hl-yellow}, los [**par√°metros √≥ptimos**]{.hl-yellow} ser√°n aquellos $\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p\right)$ que minimicen dicha suma

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p \right) = \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)$$


Y repetimos la idea: [**calcularemos la derivada respecto a $\widehat{\boldsymbol{\beta}}$ e igualamos a cero**]{.hl-yellow}

--- 


## M√©todo m√≠nimos cuadrados

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p \right) = \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)$$

Derivando matricialmente tenemos (recuerda: $\left(A B \right)^{T} = B^{T} A^{T}$)

$$\begin{eqnarray}\frac{\partial SSE \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=& \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} \nonumber \\  &=& \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} + \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}}{\partial \boldsymbol{\beta}} \nonumber \\  &=& -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X} - \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \mathbf{X}^{T} \nonumber \\  &=& -\mathbf{X}^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)  - \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \mathbf{X}^{T} \nonumber \\ &=& -2\mathbf{X}^{T} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = 0 \end{eqnarray}$$

---

## M√©todo m√≠nimos cuadrados


$$\begin{eqnarray}\frac{\partial SSE \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=&  -2\mathbf{X}^{T} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = 0 \end{eqnarray}$$

Si descartamos el $-2$ como constante, tenemos que

$$\mathbf{X}^{T} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = 0 \longrightarrow \mathbf{X}^{T}  \mathbf{Y} = \mathbf{X}^{T} \mathbf{X} \boldsymbol{\beta}$$

Si ahora despejamos $\beta$  multiplicando en ambos lados por $\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}$ tenemos finalmente

$$\widehat{\boldsymbol{\beta}}=\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$$

---

## Estimaci√≥n

F√≠jate que cuando $p = 1$ tenemos que

$$\widehat{\mathbf{Y}} = \begin{pmatrix} \widehat{Y}_1 \\ \vdots \\ \widehat{Y}_n \end{pmatrix}_{n\times1} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \begin{pmatrix} 1 & X_{11} \\\vdots & \vdots \\ 1 & X_{n1} \end{pmatrix}_{n \times 2}\begin{pmatrix} \widehat{\beta}_0 \\ \widehat{\beta}_1 \end{pmatrix}_{2\times1}$$

y que por tanto $\widehat{\boldsymbol{\beta}}= \left(\begin{pmatrix} 1 & \ldots & 1 \\ X_{11}  & \ldots & X_{n1} \end{pmatrix} \begin{pmatrix} 1 & X_{11} \\ 1 & X_{21}  \\ \vdots & \vdots \\ 1 & X_{n1} \end{pmatrix} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$

[**Deberes**]{.hl-red}: ser√≠a interesante que para el examen supieses demostrar que cuando $p=1$ esa expresi√≥n acaba en 

$$\widehat{\beta}_1  = \frac{s_{xy}}{s_{x}^{2}}, \quad \widehat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$$

---

## Estimaci√≥n

$$\widehat{\boldsymbol{\beta}}=\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$$

* La [**estimaci√≥n**]{.hl-yellow} ser√° por tanto

$$\widehat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\beta}}=\mathbf{X}\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y} = \mathbf{H}\mathbf{Y}$$

* La matriz $\mathbf{H} = \mathbf{X}\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}$ se conoce como [**hat matrix o matriz de proyecci√≥n**]{.hl-yellow} (ya que hace que las estimaciones $\hat{y}$ sean en realidad los valores $y$ proyectados verticalmente sobre el plano de regresi√≥n ajustado).



---

## Interpretaci√≥n

Al igual que pasaba antes ser√° importante [**distinguir lo poblaci√≥n de lo muestral**]{.hl-yellow}

* Los par√°metros $\left( \beta_0, \beta_1, \ldots, \beta_p \right)$ son desconocidos, los [**par√°metros poblacionales**]{.hl-yellow}

* Los par√°metros $\left( \widehat{\beta}_0, \widehat{\beta}_1 , \ldots, \widehat{\beta}_p\right)$ son estimados a partir de los datos, son [**variables aleatorias**]{.hl-yellow} ya que han sido calculados en funci√≥n de una muestra aleatoria.

&nbsp;

[**¬øC√≥mo se interpretan ahora los par√°metros?**]{.hl-yellow}

---

## Interpretaci√≥n

$$\widehat{\mathbf{Y}} = \mathbf{X}\widehat{\boldsymbol{\beta}}= \widehat{\beta}_0 + \sum_{j=1}^{p} \widehat{\beta}_j X_j$$

* [**Ordenada en el origen**]{.hl-yellow}: tambi√©n llamado **intercepto**, y denotado como $\beta_0$ su valor real, es el valor de $Y$ cuando $X_1 = \ldots = X_p = 0$. Es decir, $\widehat{\beta}_0$ se puede interpretar como la estimaci√≥n $\widehat{Y}$ cuando [**TODAS las predictoras son nulas**]{.hl-purple}


. . .

* [**Pendientes**]{.hl-yellow}: denotadas como $\beta_j$, para $j=1,\ldots, p$, su valor real, cuantifica el incremento de $Y$ cuando solo $X_j$ aumenta una unidad. Es decir, $\widehat{\beta}_j$ se puede interpretar como la variaci√≥n de la estimaci√≥n $\widehat{Y}$ cuando [**$X_j$ tiene un incremento unitario y el RESTO DE PREDICTORAS permanecen fijas**]{.hl-purple}.

---

## Diagnosis

En el caso multivariante las [**4 hip√≥tesis**]{.hl-yellow} se convierten en

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es 

$$E \left[Y | \left( \boldsymbol{X}_1 = x_1, \ldots, \boldsymbol{X}_p = x_p \right) \right] = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$$

2. [**Homocedasticidad**]{.hl-green}: necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow} $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \left( \boldsymbol{X}_1 = x_1, \ldots, \boldsymbol{X}_p = x_p \right) \right] = cte < \infty$

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$


4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser en independientes, y en particular **incorrelados** ${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$

$$Y | \left( \boldsymbol{X}_1 = x_1, \ldots, \boldsymbol{X}_p = x_p \right) \sim N \left(\beta_0 + \beta_1x_1 + \ldots + \beta_p x_p, \sigma_{\varepsilon}^2 \right)$$


---

## Inferencia 

Ahora las hip√≥tesis nos permiten decir  que los [**par√°metros estimados siguen una distribuci√≥n normal multivariante**]{.hl-yellow} de [**media el vector de par√°metros**]{.hl-purple}  a estimar y de [**matriz de covarianzas la varianza residual por $\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}$**]{.hl-purple}

$$\widehat{\boldsymbol{\beta}} \sim N_{p+1}\left(\boldsymbol{\beta}, \sigma_{\varepsilon}^2 \left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}\right)$$

. . .

Esa normal multivariante, componente a componente, deriva en

$$\widehat{\beta}_j\sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad \frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)} \sim N \left(0, 1 \right), \quad \frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} \sim t_{n-p-1}$$

donde $SE \left(\widehat{\beta}_j\right)^2 = \sigma_{\varepsilon}^2 v_j$ y $\widehat{SE} \left(\widehat{\beta}_j\right)^2 = \widehat{\sigma}_{\varepsilon}^2 v_j$, con $v_j$ el elemento $j$-√©simo de la diagonal de $\left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}$ (matriz que representa la **variabilidad de predictores**)


---

## Caso pr√°ctico: wine.csv

Vamos a volver a usar nuestros datos `wine.csv`

```{r}
datos <- read_csv(file = "./datos/wine.csv")
datos
```

Pero esta vez no vamos a seleccionar ninguna variable previamente. Para [**ajustar un modelo multivariante**]{.hl-yellow} basta con a√±adir variables `+`

```{r}
ajuste_uni <- lm(data = datos, formula = Price ~ AGST)
ajuste_2 <- lm(data = datos, formula = Price ~ AGST + HarvestRain)
ajuste_full <- lm(data = datos, formula = Price ~ .)
```


---

## Caso pr√°ctico: wine.csv

F√≠jate que ahora la tabla `coefficients` tiene una [**l√≠nea por covariable**]{.hl-yellow} (m√°s $\beta_0$) y adem√°s en este caso dice `(1 not defined because of singularities)`: la [**matriz de covarianzas no es invertible**]{.hl-red} ya que el [**determinante es 0**]{.hl-red} (en este caso `Year` es "igual" que `Age`)

```{r}
ajuste_full |> summary()
```

---

## Caso pr√°ctico: wine.csv

Una vez eliminada, en la √∫ltima l√≠nea `F-statistic: ... p-value: 2.232e-07`: se est√° [**rechazando la hip√≥tesis nula**]{.hl-green} del contraste de significaci√≥n global (existe [**alguna predictora cuyo efecto lineal es significativo**]{.hl-green}). Recuerda que la nula es $H_0:~\beta_1 = \ldots = \beta_5 = 0$


```{r}
ajuste_full <- lm(data = datos |> select(-Age), formula = Price ~ .)
ajuste_full |> summary()
```

---

## Caso pr√°ctico: wine.csv

Si nos fijamos en la tabla de coeficientes el [**modelo ajustado**]{.hl-yellow} es

$$\begin{eqnarray}\widehat{Precio} &=& 2.496 - 0.0137 * Year + 0.0012 * WinterRain + 0.614 * AGST \nonumber \\ &-& 0.0038 * HarvestRain - 0.00002 * FrancePop \nonumber \end{eqnarray}$$

```{r}
ajuste_full |> summary()
```

---

## Caso pr√°ctico: wine.csv

$$\begin{eqnarray}\widehat{Precio} &=& 2.496 - 0.0137 * Year + 0.0012 * WinterRain + 0.614 * AGST \nonumber \\ &-& 0.0038 * HarvestRain - 0.00002 * FrancePop \nonumber \end{eqnarray}$$

La [**interpretaci√≥n**]{.hl_yellow} es

* La estimaci√≥n del modelo cuando todas las covariables son 0 (sin poblaci√≥n, sin lluvia ni temperatura, a√±o 0) es de 2.496 (escala log)

* Por ejemplo, si el resto de variables permaneciesen fijas, por cada litro de lluvia que caiga de m√°s durante la cosecha (agosto-septiembre), el modelo estima que el precio baja 3.8 (escala log)

---

## Caso pr√°ctico: wine.csv

Si nos fijamos en la tabla de coeficientes tenemos **2 predictoras cuyo efecto lineal no se acepta que sea significativo**: `Year`, `FrancePop` (adem√°s de $\beta_0$) ya que el contraste de significaci√≥n $H_0:~\beta_j = 0$ vs $H_1:~\beta_j \neq 0$ nos devuelve un $p-valor > \alpha$

```{r}
ajuste_full |> summary()
```

---

## Caso pr√°ctico: wine.csv

La pregunta que intentaremos resolver en futuras clases es:

[**¬øC√≥mo decidir que predictoras seleccionamos?**]{.hl-yellow} El problema si quitamos todas las no significativas de manera simult√°nea es que no sabemos qu√© **efectos puede haber entre las propias predictoras**

```{r}
ajuste_full |> summary()
```

---

## Caso pr√°ctico: wine.csv

F√≠jate que ahora en la [**fase de diagnosis**]{.hl-yellow} tenemos una **sexta gr√°fica a chequear**: una gr√°fica que nos calcula la conocida como `VIF` (Variance Inflaction), que nos [**cuantifica la colinealidad (efectos lineales) entre las predictoras**]{.hl-yellow}

```{r}
check_model(ajuste_full)
```

---

## ANOVA

Ahora `anova()` en realidad nos hace una [**ANOVA secuencial**]{.hl-yellow} (variable a variable)


```{r}
ajuste_full |> anova()
```

---

## ANOVA

Ahora `anova()` en realidad nos hace una [**ANOVA secuencial**]{.hl-yellow} (variable a variable)


|  | Df (grados) | Sum Sq | Mean Sq | F value | Pr(>F) |
|:---------:|:-----:|:------:|:------:|:------:|:------:|
| $x_1$  | 1  | $SSR_1$ | $\frac{SSR_1}{1}$ | $F-value = \frac{\frac{SSR_1}{1}}{\frac{SSE}{n-p-1}}$ | $p_1$
| ...  | ...  | ... | ... | $F-value = \frac{\frac{SSR}{p}}{\frac{SSE}{n-p-1}}$ | p-valor F test
| $x_p$  | 1  | $SSR_p$ | $\frac{SSR_p}{1}$ | $F-value = \frac{\frac{SSR_p}{1}}{\frac{SSE}{n-p-1}}$ | $p_p$
| Residuals  | n - p - 1 | SSE | $\widehat{\sigma}_{\varepsilon}^{2} = \frac{SSE}{n-p-1}$ | | |


---

## ANOVA


```{r}
ajuste_full |> anova()
```


Ahora $SSR_j$, con $j=1,\ldots,p$, representa la **suma de residuos al cuadrado** $SSR$ cuando incluimos la predictora j-√©sima $X_j$, frente a cuando no lo hacemos, tal que

$$SSR_j = SSR \left(X_1, \ldots, X_j \right) - SSR \left(X_1, \ldots, X_{j-1} \right)$$

As√≠ los p-valores $p_j$ son individuales (los mismos que los de la tabla de coeficientes).



# Clase 14: selecci√≥n de modelos {#clase-14}

[**Selecci√≥n secuencial de modelos**]{style="color:#444442;"}

---

## Bondad de ajuste

Como ya hemos hablado, el conocido como $R^2$ o [**bondad de ajuste**]{.hl-yellow} o el [**coeficiente de determinaci√≥n**]{.hl-yellow} es una forma sencilla, y relacionada con el ANOVA, para cuantificar la [**cantidad de informaci√≥n explicada por el modelo**]{.hl-yellow} definida como

$$R^2 = \text{Ratio info explicada} = \frac{SSR}{SST} =_{lineal} 1 - \frac{SSE}{SST} = 1 - \frac{s_{r}^{2}}{s_{y}^{2}} =  1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^{2}}{s_{y}^{2}}$$


Por definici√≥n, la [**bondad de ajuste est√° entre 0 y 1**]{.hl-yellow} pero adem√°s [**presenta un problema en el caso multivariante**]{.hl_red}


* [**M√°s predictoras**]{.hl-yellow} implica que [**$R^2$ crece**]{.hl-yellow} (al aumentar $p$), ¬°incluso aunque dichas [**predictoras no sean significativas**]{.hl-red}!

---
  
## R2 ajustado

Para evitar dicho problema vamos a definir la conocida como [**bondad de ajuste ajustada**]{.hl-yellow} 

$$R_{adj}^2 = 1 - \frac{\frac{SSE}{n-p-1}}{\frac{SST}{n-1}} = 1 - \frac{SSE}{SST}\frac{n-1}{n-p-1}  = 1 - \frac{\widehat{\sigma}_{\varepsilon}^{2}}{SST}(n-1)$$


. . .

As√≠ el $R_{adj}^2$ [**no depende del n√∫mero de predictoras de manera directa**]{.hl-green} (si sigue dependiendo del ruido, de manera que descender√° solo cuando incrementar $p$ implica reducir el error, es decir, variables con un efecto significativo). Se cumple adem√°s que que si $p=1$

$$\lim_{n\to \infty} R_{adj}^2 = \lim_{n\to \infty} \left(1 - \frac{\widehat{\sigma}_{\varepsilon}^{2}}{SST}(n-1) \right) = \lim_{n\to \infty} \left(1 - \frac{\widehat{\sigma}_{\varepsilon}^{2}}{SST}(n-p-1) \right) = R^2$$


---

## R2 ajustado

Para ver mejor su diferencia debes realizar el siguiente [**estudio de simulaci√≥n**]{.hl-yellow} (si quieres fija semilla `set.seed(12345)`):

. . .

1. Considera dos predictoras $X_1 \sim N(0, 1)$ y $X_2 \sim N(0, 1)$ de tama√±o muestral $n = 200$.  Considera el ruido como $\varepsilon \sim N(0, \sigma = 12)$

. . .

2. Simula el modelo $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \varepsilon_i$ con $\beta_0 = 0.01$, $\beta_1 = 1.5$ y $\beta_2 = -1.5$

. . .

3. Realiza el ajuste con todas las predictoras y calcula el $R^2$ y el $R_{adj}^2$ (lo necesitamos guardar, as√≠ usa la f√≥rmula no la salida del `lm()`)

. . .

4. Repite este proceso $M = 300$ veces (vuelve a simular las variables, vuelve a construir el modelo, y obt√©n de nuevo las bondades de ajuste, de manera que tengamos $300$ de cada uno)

---

## R2 ajustado


Lo que deber√≠a salirte si fijas `set.seed(12345)`

```{r}
#| echo: false
M <- 300
n <- 200
R2 <- R2_adj <- rep(NA, M)
p <- 2

for (i in 1:M) {
  
  x_1 <- rnorm(n)
  x_2 <- rnorm(n)
  eps <- rnorm(n, mean = 0, sd = 12)
  
  y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
  datos <- tibble("y" = y, "x_1" = x_1, "x_2" = x_2)
  ajuste <- lm(data = datos, y ~ .)
  
  R2[i] <- 1 - var(ajuste$residuals) / var(datos$y)
  R2_adj[i] <-
    1 - (var(ajuste$residuals) / var(datos$y)) * ((n - 1) / (n - p - 1))
}

library(ggridges)
ggplot(tibble("id" = 1:M, "R2" = R2, "R2_adj" = R2_adj) |> 
         pivot_longer(cols = -id, names_to = "R2_type",
                      values_to = "values")) +
  geom_density_ridges(aes(x = values, y = R2_type,
                          color = R2_type, fill = R2_type),
                      alpha = 0.5) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  coord_flip() +
  theme_minimal()
```


---

## R2 ajustado

Repite el estudio de simulaci√≥n  a√±adiendo 

1. Una nueva predictora "basura" $X_3 \sim N(0, 2)$ tal que $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + 10^{-10} X_{i3} + \varepsilon_i$.

2. Dos nuevas predictora "basura" $X_3 \sim N(0, 2)$ y $X_4 \sim N(0, 2)$ tal que $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + 10^{-10} X_{i3} + 10^{-10} X_{i4} + \varepsilon_i$.

...

3. 195 nuevas predictora "basura" $X_j \sim N(0, 2)$  tal que $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \displaystyle \sum_{j=3}^{195} 10^{-10} X_{ij} + \varepsilon_i$.


Ahora debes obtener $300*195$ valores de $R^2$ y $R_{adj}^2$ (300 simulaciones por cada nueva predictora que a√±adimos)

---

## R2 ajustado

![](img/predictoras-R2.png)


```{r}
#| echo: false
#| eval: false
M <- 300
n <- 200
p <- 195
R2 <- R2_adj <- matrix(NA, M, p)

set.seed(12345)
for (i in 1:M) {
  
  x_1 <- rnorm(n)
  x_2 <- rnorm(n)
  eps <- rnorm(n, mean = 0, sd = 12)
  y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
  
  X <- matrix(NA, n, p)
  for (j in 1:p) {
    
    X[, j] <- rnorm(n, mean = 0, sd = 2)
    y <- y + 1e-10 * X[, j]
    datos <- tibble("y" = y, "x_1" = x_1, "x_2" = x_2, X[, 1:j])
    ajuste <- lm(data = datos, y ~ .)
    
    R2[i, j] <- 1 - var(ajuste$residuals) / var(datos$y)
    R2_adj[i, j] <-
      1 - (var(ajuste$residuals) / var(datos$y)) *
      ((n - 1) / (n - (j + 2) - 1))
  }
}

mean_R2 <- R2 |> colMeans()
mean_R2_adj <- R2_adj |> colMeans()

gg <- ggplot()
for (i in 1:M) {
  gg <-
    gg +
    geom_line(data =
                 tibble("p" = 1:p, "R2" = R2[i, ],
                        "R2_adj" = R2_adj[i, ]) |>
                 pivot_longer(cols = -p, names_to = "R2_type",
                              values_to = "values"),
               aes(x = p, y = values, color = R2_type),
              linewidth = 0.01, alpha = 0.1) +
    geom_line(data = tibble("p" = 1:p, "R2" = mean_R2, "R2_adj" = mean_R2_adj) |>
                 pivot_longer(cols = -p, names_to = "R2_type",
                              values_to = "values"),
              aes(x = p, y = values, color = R2_type), linewidth = 1.5)
}
gg1 <- 
  gg +
  theme_minimal() +
  scale_y_continuous(limits = c(-0.5, 1.1)) +
  labs(x = "N√∫mero de predictoras basura")
```

---

## Multicolinealidad

Uno de los mayores problemas de los [**modelos lineales multivariantes**]{.hl-yellow} es el conocido como [**problema de colinealidad**]{.hl-red}

. . .

Imagina un modelo $Y = \beta_0 + \beta_1 X_1 + \beta_2X_2 + \beta_3 X_3$, donde $X_3$ es definida como (literal) la suma de $X_1$ y $X_2$.

```{r}
n <- 200
x_1 <- rnorm(n, sd = 2)
x_2 <- rnorm(n, sd = 2)
x_3 <- x_1 + x_2
eps <- rnorm(n)

y <- 0.5 - 2*x_1 + 3*x_2 -2.5*x_3
datos <- tibble(y, x_1, x_2, x_3)
ajuste <- lm(data = datos, formula = y ~ .)
```

---

## Multicolinealidad

Como ya vimos en el ejemplo de `wine.csv`, cuando tenemos [**dos o m√°s predictoras que dependen linealmente entre s√≠**]{.hl-yellow} la matriz $\left(X^{T} X \right)^{-1}$ no se puede invertir ya que es singular (determinante igual a 0), as√≠ que debe eliminar una de las ecuaciones para que el problema sea de rango completo

Esto se conoce como [**colinealidad exacta**]{.hl-yellow}

```{r}
ajuste |> summary()
```

---

## Multicolinealidad

La colinealidad exacta es solo el ejemplo m√°s extremo de lo que se conoce como [**problema de colinealidad**]{.hl-red}: un problema que aparece cuando varias predictoras est√°n **altamente correladas**. 
Un [**problema de colinealidad**]{.hl-yellow} tiene principalmente dos consecuencias:

. . .

* [**Reduce la precisi√≥n de los estimadores**]{.hl-red} ya que la matriz $X^{T} X$, seg√∫n aumenta la dependencia, tiene un determinante cada vez m√°s cercano a 0, por lo que $\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}$, que siempre depende de $1/det\left( \mathbf{X}^{T}\mathbf{X} \right)$, tendr√° valores cada vez m√°s grandes. Dado que $\widehat{\beta}_j\sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right)$, con $SE \left(\widehat{\beta}_j\right)^2 = \sigma_{\varepsilon}^2 v_j$ siendo $v_j$ el elemento $j$-√©simo de la diagonal de $\left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}$, implica que [**a mayor colinealidad, mayor varianza de las estimaciones**]{.hl_yellow}

. . .


* [**Distorsiona los efectos**]{.hl-red} de los predictores sobre la respuesta

---

## Multicolinealidad

¬øPero qu√© sucede cuando esa [**colinealidad no es tan obvia**]{.hl-yellow}? Veamos un ejemplo sencillo: simula el siguiente modelo (para $n = 100$)

$$Y = 1 + 0.5 X_1 + 2 X_2 - 3 X_3 - X_4 + \varepsilon, \quad \varepsilon \sim N(0, 1)$$


* $X_1 \sim N(0, 1)$, $X_2 = 0.5*X_1 + N(0, 1)$ y $X_3 = 0.5*X_2 + N(0, 1)$

* $X_4 = -X_1 + X_2 + N(0, 0.5)$

```{r}
#| code-fold: true
set.seed(12345)
n <- 100

x1 <- rnorm(n)
x2 <- 0.5 * x1 + rnorm(n)
x3 <- 0.5 * x2 + rnorm(n)
x4 <- -x1 + x2 + rnorm(n, sd = 0.5)
eps <- rnorm(100)

y <- 1 + 0.5 * x1 + 2 * x2 - 3 * x3 - x4 + eps
datos <- tibble(y, x1, x2, x3, x4)
```

¬øC√≥mo ser√°n sus **correlaciones (lineales)**?

---

## Multicolinealidad

A priori [**no se observa un problema obvio de colinealidad**]{.hl-yellow} ya que no hay dos predictoras altamente correladas.

```{r}
datos |> 
  correlate()
```

---

## Multicolinealidad

Por la definici√≥n realizada es obvio que hay una [**dependencia lineal entre todas las predictoras**]{.hl-yellow}, pero al ser una relaci√≥n lineal m√°s compleja (con ruido de por medio y distintas predictoras interactuando a la vez), una simple [**revisi√≥n de las correlaciones es necesaria pero no suficiente**]{.hl-yellow} ya que puede ocultar problemas de colinealidad que s√≠ existen.

```{r}
datos |> 
  cor() |> 
  corrplot(method = "color", addCoef.col = "#121212")
```


---

## Multicolinealidad

Necesitamos por tanto una forma de cuantificar dicha multicolinealidad, y para ello usaremos el conocido como [**factor de inflaci√≥n de la varianza (VIF)**]{.hl-yellow}

. . .

Para cada coeficiente $\beta_j$, y su estimador $\widehat{\beta}_j$, se define el VIF como

$$VIF\left(\widehat {\beta_j} \right) = \frac{1}{1 - R^2_{X_j | X_{-j}}}, \quad R_{j}^2:= R^2_{X_j | X_{-j}}$$

donde $R_{j}^2$ representa la [**bondad de ajuste $R^2$ cuando predecimos con una regresi√≥n lineal multivariante la predictora $X_j$**]{.hl-yellow} en funci√≥n del resto de predictoras (sin la variable objetivo)

$$X_j = \gamma_0 + \gamma_1 X_1 + \ldots + \gamma_{j-1} X_{j-1} + \gamma_{j+1} X_{j+1} + \ldots + \gamma_j X_j$$

---

## Multicolinealidad


$$VIF\left(\widehat {\beta_j} \right) = \frac{1}{1 - R^2_{X_j | X_{-j}}}, \quad R_{j}^2:= R^2_{X_j | X_{-j}}$$

La idea es [**cuantificar como de relacionado est√° cada predictor**]{.hl-yellow} respecto al resto (de manera conjunta, no dos a dos). Nota: **siempre es mayor que 1**

. . .


* Si $VIF\left(\widehat {\beta_j} \right)$ [**cercano a 1**]{.hl-yellow}, entonces $R_{j}^{2}$ cercano a 0 --> variabilidad del predictor $X_j$ no se puede explicar con la combinaci√≥n lineal de otras

. . .


* Si $VIF\left(\widehat {\beta_j} \right)$ [**mayor que 5**]{.hl-yellow}, entonces $1 - R_{j}^{2} < 0.2$, ergo $R_{j}^{2} > 0.8$ --> problema moderado de colinealidad

. . .


* Si adem√°s $VIF\left(\widehat {\beta_j} \right)$ [**mayor que 10**]{.hl-yellow}, entonces $R_{j}^{2} > 0.9$ --> problema grave de colinealidad 

---

## Multicolinealidad

Para calcular el $VIF$ podemos o bien hacer regresi√≥n con cada par√°metro, de manera manual, o bien hacer uso de `check_collinearity()` del paquete `{performance}`

```{r}
ajuste <- lm(data = datos, y ~ .)
performance::check_collinearity(ajuste)
```

---

## Multicolinealidad

Si te fijas $VIF \left(\widehat{\beta}_2 \right) = 10.41$, que es justamente el $1/(1-R^2)$ si hacemos la regresi√≥n $X_2$ vs el resto de predictoras

```{r}
check_collinearity(ajuste)$VIF
ajuste_X2 <- lm(data = datos |> select(-y), x2 ~ .)
ajuste_X2 |> summary()
```

---

## Multicolinealidad

El VIF podemos adem√°s [**visualizarlo en una sexta gr√°fica**]{.hl-model} cuando hacemos `check_model()`

```{r}
check_model(ajuste)
```

---

## Multicolinealidad

El VIF podemos adem√°s [**visualizarlo de manera manual**]{.hl-model}

```{r}
#| code-fold: true
VIF <- tibble("variable" = c("x1", "x2", "x3", "x4"),
              "VIF"= check_collinearity(ajuste)$VIF)
ggplot(VIF) +
  geom_col(aes(x = variable, y = VIF, fill = VIF),
           alpha = 0.75) +
  scale_fill_gradient2(low = "#1B8A54", mid = "#F7E865",
                       high = "#CE2424", midpoint = 5) +
  theme_minimal()
```


---

## Selecci√≥n de modelos

Por lo tanto parece que obvio que en un **modelo de regresi√≥n multivariante** vamos a tener que que [**seleccionar distintas variables**]{.hl-yellow} para solventar esos problemas de colinealidad

. . .

Llamaremos [**modelo saturado**]{.hl-yellow} al modelo con todas las $p$ predictoras, sin seleccionar.

. . .

Y como hemos visto, [**no podremos simplemente comparar 2 a 2 correlaciones o p-valores**]{.hl-red} ya que el efecto de una variable nos puede afectar en otras. ¬øQu√© hacer?


---

## Selecci√≥n de modelos

Un protocolo de actuaci√≥n habitual podr√≠a ser el siguiente:

1. Hacemos el [**ajuste del modelo saturado**]{.hl-yellow}

2. Calculamos el [**VIF de cada estimador**]{.hl-yellow}

3. Adoptamos un umbral (por ejemplo, $VIF > 10$), de manera que [**toda predictora que lo supere se elimina ya de antemano**]{.hl-yellow}

4. Volvemos a chequear el VIF. Si hay que eliminar, volver al paso 3.

5. Del resto de predictoras se hace una [**selecci√≥n m√°s fina**]{.hl-yellow}. Algunas opciones: BIC/AIC, regresi√≥n penalizada (LASSO, ridge, elastic net), PCA.

---

## Selecci√≥n de modelos

Lo que nos dice el VIF y lo visto hasta ahora es que, incluso aunque tengan efecto, [**a√±adir predictoras complejizando el modelo no es gratis**]{.hl-yellow}, ya que lo hacemos a costa de [**sobreajustar el modelo**]{.hl-yellow} (la varianza de los estimadores se incrementa considerablemente)

¬øCu√°l es el [**n√∫mero m√≠nimo/m√°ximo de predictoras**]{.hl-yellow} que podremos incluir?

. . .

* El n√∫mero m√≠nimo de predictoras ser√° $p = 1$
* El n√∫mero m√°ximo ser√° $p = n-2$, o dicho de otra forma, necesito al menos $n \geq p+2$ observaciones. Piensa en $p=1$: si quiero una recta, necesito al menos 2 puntos para que la recta existe y al menos 3 para poder calcular la varianza residual estimada (acu√©rdate que se divide entre $n-p-1$)

. . .

De manera resumida: dado un modelo con $p$ predictores, necesitaremos estimar [**$p+2$ inc√≥gnitas**]{.hl-yellow} ($p+1$ coeficientes y la varianza residual).

---

## BIC y AIC

[**¬øC√≥mo seleccionar los predictores m√°s adecuados para el CONJUNTO del ajuste?**]{.hl-yellow}

. . .

Los m√©todos m√°s conocidos son los conocidos como [**selecci√≥n de modelos stepwise (paso-a-paso)**]{.hl-yellow}, que de manera **iterativa**, va incluyendo y descartando distintos predictores, y comparando su calidad, para decidir que **combinaci√≥n de par√°metros** es la m√°s √≥ptima.

. . .

La idea es [**seleccionar el modelo m√°s √≥ptimo**]{.hl-yellow} en funci√≥n de un [**criterio de informaci√≥n**]{.hl-yellow} que combina la calidad del modelo con el n√∫mero de predictoras empleadas: vamos a [**penalizar el uso de variables que no mejore suficiente el modelo**]{.hl-yellow}

---


## BIC y AIC

Los dos criterios de informaci√≥n m√°s famosos son el [**Bayesian Information Criterion (BIC)**]{.hl-yellow} y el [**Akaike Information Criterion (AIC)**]{.hl-yellow} definidos ambos como

$$AIC/BIC_{modelo} = -Calidad + \underbrace{\text{npar(modelo)} * \text{penalizaci√≥n}}_{\text{Complejidad}}$$

* $\text{npar(modelo)}$ es el [**n√∫mero de par√°metros a estimar del modelo**]{.hl-yellow}, que en el caso que nos ocupa es $\text{npar(modelo)} = p+2$ (coeficientes + var residual)

* $\text{Calidad}$ una medida que nos diga como de bueno es nuestro ajuste: al tener signo negativo, buscamos el [**menor valor de AIC/BIC**]{.hl-yellow}

* $\text{penalizaci√≥n}$: cuando sube $p$, entonces suba el $AIC/BIC$

El objetivo ser√° probar [**distintos modelos y quedarnos con el que tenga AIC/BIC m√°s peque√±o**]{.hl-yellow}

---

## BIC y AIC

Es habitual que la [**calidad del modelo**]{.hl-yellow} venga cuantificada por $2\ell(\text{modelo})$, donde $\ell(\text{modelo})$ es lo que se conoce como [**log-verosimilitud del modelo**]{.hl-yellow} 

$$\ell(\text{modelo}) = \log \left(P \left(\text{datos} |  \left(\text{par√°metros}, \text{modelo} \right) \right) \right)$$

es decir, suponiendo que el **modelo fuese correcto** y los **par√°metros valiesen la estimaci√≥n obtenida**, ¬øc√≥mo de [**probable es que mis datos hayan sido los que han sido**]{.hl-yellow}?

. . .

As√≠ nuestros criterios quedan como

$$AIC/BIC_{modelo} = -2\ell(\text{modelo}) + \underbrace{(p+2) * \text{penalizaci√≥n}}_{\text{Complejidad}}$$

---

## BIC y AIC

La diferencia entre ambos est√° en la **penalizaci√≥n usada**


$$BIC(modelo_p) = -2\ell(\text{modelo}) + \underbrace{(p+2) * \log(n)}_{\text{Complejidad}}$$

$$AIC (modelo_p) = -2\ell(\text{modelo}) + \underbrace{(p+2) * 2}_{\text{Complejidad}}$$

El [**criterio BIC es m√°s agresivo**]{.hl-yellow} seleccionando variables ya que, si $n$ crece, $\log(n) >> 2$ (penaliza m√°s el sobreajuste) y, adem√°s, [**depende del tama√±o muestral**]{.hl-yellow}

---

## BIC y AIC

Vamos a ver un **peque√±o ejemplo en R** con `wine.csv`, aplicando un modelo saturado (habiendo hecho un filtro inicial por VIF), un modelo solo con `AGST`,  otro con `AGST + HarvestRain` y otro con `AGST + HarvestRain + Year`, y calcular su BIC/AIC con las funciones `BIC()` y `AIC()`

```{r}
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, formula = Price ~ .)
ajuste_1 <- lm(data = datos, formula = Price ~ AGST)
ajuste_2 <- lm(data = datos, formula = Price ~ AGST + HarvestRain)
ajuste_3 <- lm(data = datos, formula = Price ~ AGST + HarvestRain + Age)
```

:::: columns
::: {.column width="50%"}
```{r}
BIC(ajuste_saturado)
BIC(ajuste_1)
```
:::

::: {.column width="50%"}
```{r}
BIC(ajuste_2)
BIC(ajuste_3)
```
:::
::::

---

## BIC y AIC

Vamos a ver un **peque√±o ejemplo en R** con `wine.csv`, aplicando un modelo saturado (habiendo hecho un filtro inicial por VIF), un modelo solo con `AGST`,  otro con `AGST + HarvestRain` y otro con `AGST + HarvestRain + Year`, y calcular su BIC/AIC con las funciones `BIC()` y `AIC()`

:::: columns
::: {.column width="50%"}
```{r}
AIC(ajuste_saturado)
AIC(ajuste_1)
```
:::

::: {.column width="50%"}
```{r}
AIC(ajuste_2)
AIC(ajuste_3)
```
:::
::::

---

## BIC y AIC

Podemos usar `compare_performance()`  ($AICc = AIC +\frac{2(p+2)^{2} + 2(p+2)}{n-(p+2)-1}$ es una versi√≥n corregida para tama√±os muestrales peque√±os)

```{r}
performance::compare_performance(ajuste_saturado, ajuste_1, ajuste_2, ajuste_3)
```

. . .

En ambos criterios la conclusi√≥n es que de los 4 modelos, el [**"mejor" es el 3¬∫ ya que AIC/BIC m√°s bajos**]{.hl-yellow} aunque tenga un $R_{adj}^2$ menor: la mejora que produce meter todas las variables no es suficiente para lo que se complica el modelo.

---

## BIC y AIC

En realidad lo correcto ser√≠a antes chequear si [**podemos eliminar alguna variable**]{.hl-yellow} ya de manera preliminar usando el VIF

```{r}
check_collinearity(ajuste_saturado)
```

---

## BIC y AIC

Seg√∫n el VIF debemos ya eliminar de antemano `Year` y `FrancePop` as√≠ que lo hacemos y volvemos calcularlo para las restantes

```{r}
datos_VIF <- datos |> select(-Year, -FrancePop)
ajuste_saturado <- lm(data = datos_VIF, formula = Price ~ .)
check_collinearity(ajuste_saturado)
```

---

## BIC y AIC

Tras este primer **filtro grosero** volvemos a implementar los 3 modelos

```{r}
ajuste_saturado <- lm(data = datos_VIF, formula = Price ~ .)
ajuste_1 <- lm(data = datos_VIF, formula = Price ~ AGST)
ajuste_2 <- lm(data = datos_VIF, formula = Price ~ AGST + HarvestRain)
ajuste_3 <- lm(data = datos_VIF, formula = Price ~ AGST + HarvestRain + Age)
compare_performance(ajuste_saturado, ajuste_1, ajuste_2, ajuste_3)
```

Ahora **nos indica que el mejor modelo es el saturado** (pero que ya no tiene 6 variables sino 4): la mejora de bondad de ajuste compensa por incrementar una sola variable (entre ajuste3 y saturado)

---

## BIC y AIC

F√≠jate que adem√°s al haber quitado dos variables con alta dependenica lineal del resto las [**hip√≥tesis se cumplen**]{.hl-yellow} (antes no)

```{r}
check_model(ajuste_saturado)
```

---

## BIC y AIC

Vamos a repetirlo con un ejemplo simulado $Y = 0.01 + 1.5*X_1 -1.5X_2 + \varepsilon$, donde $X_1,X_2 \sim N(0,1)$ y $\varepsilon \sim N(0, 2)$, y al que vamos a√±adiendo predictoras basura $X_{2+j} \sim N(0, 2)$ con $\beta_{2+j} = 10^{-8}$, probando cuando $j=1$, $j = 5$, $j = 10$, $j=25$, $j=50$ y $j = 100$. El mejor modelo es el que solo tiene las **dos predictoras con un efecto real** sobre y.

```{r}
#| code-fold: true
n <- 200
p <- c(1, 5, 10, 25, 50, 100)

set.seed(12345)
x_1 <- rnorm(n)
x_2 <- rnorm(n)
eps <- rnorm(n, mean = 0, sd = 2)
y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
ajuste_0 <- lm(data = tibble("y" = y, "x_1" = x_1, "x_2" = x_2), y ~ .)

X <- matrix(rnorm(n * max(p), mean = 0, sd = 2), nrow = n)
ajuste <- list()
for (j in 1:length(p)) {
  
  ynew <- y + 1e-8 * sum(X[, 1:p[j]])
  datos <- tibble("y" = ynew, "x_1" = x_1, "x_2" = x_2, X[, 1:p[j]])
  ajuste[[j]] <- lm(data = datos, y ~ .)
}
compare_performance(ajuste_0, ajuste[[1]], ajuste[[2]],
                    ajuste[[3]], ajuste[[4]], ajuste[[5]])
```



---


## Sobreajuste

Algo importante a tener en cuenta es que, aunque ambos criterios nos ayudan a seleccionar modelos, ambos [**funcionan de manera aceptable bajo la hip√≥tesis**]{.hl-yellow} de que $n >> p +2$: en caso contrario, si $n$ se acerca a $p +2$, el sobreajuste seguir√° produci√©ndose

. . .

Dado que la penalizaci√≥n es m√°s grande en el BIC, el [**criterio BIC nos garantiza una m√°s temprana detecci√≥n del sobreajuste**]{.hl-yellow}

---

## Sobreajuste

Lo anterior se puede ilustrar calculando BIC e AIC del anterior estudio de simulaci√≥n de $R^2$ vs $R_{adj}^2$: f√≠jate como el [**BIC tarda m√°s en bajar**]{.hl-yellow}, tal que $BIC(p = 195) > BIC(p=2)$ (nos har√≠a quedarnos con el modelo sin sobreajustar) mientras que $AIC(p = 156) < AIC(p=2)$ (y para todos los que van detr√°s), por lo que acabar√≠amos eligiendo el modelo m√°s sobreajustado.

![](img/BIC-AIC.png)

```{r}
#| echo: false
#| eval: false
M <- 300
n <- 200
p <- 195
BIC_values <- AIC_values <- matrix(NA, M, p)

set.seed(12345)
for (i in 1:M) {
  
  x_1 <- rnorm(n)
  x_2 <- rnorm(n)
  eps <- rnorm(n, mean = 0, sd = 12)
  y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
  
  X <- matrix(NA, n, p)
  for (j in 1:p) {
    
    X[, j] <- rnorm(n, mean = 0, sd = 2)
    y <- y + 1e-10 * X[, j]
    datos <- tibble("y" = y, "x_1" = x_1, "x_2" = x_2, X[, 1:j])
    ajuste <- lm(data = datos, y ~ .)
    
    BIC_values[i, j] <- BIC(ajuste)
    AIC_values[i, j] <- AIC(ajuste)
  }
}

mean_BIC <- BIC_values |> colMeans()
mean_AIC <- AIC_values |> colMeans()

gg <- ggplot()
for (i in 1:M) {
  gg <-
    gg +
    geom_line(data =
                 tibble("p" = 1:p, "BIC" = BIC_values[i, ],
                        "AIC" = AIC_values[i, ]) |>
                 pivot_longer(cols = -p, names_to = "criterio",
                              values_to = "values"),
               aes(x = p, y = values, color = criterio),
              linewidth = 0.01, alpha = 0.1) +
    geom_line(data = tibble("p" = 1:p, "BIC" = mean_BIC,
                            "AIC" = mean_AIC) |>
                 pivot_longer(cols = -p, names_to = "criterio",
                              values_to = "values"),
              aes(x = p, y = values, color = criterio), linewidth = 1.5)
}
gg1 <- 
  gg +
  theme_minimal() +
  labs(x = "N√∫mero de predictoras basura")
```

---

## Consistencia

Otra [**enorme ventaja del BIC**]{.hl-yellow} es que se ha demostrado matem√°ticamente que es [**consistente**]{.hl-yellow}: si el tama√±o muestral fuese lo suficientemente grande, el [**BIC garantiza elegir el modelo correcto que genera los datos**]{.hl-green}. Matem√°ticamente se cumple que, si tenemos una serie de modelos $M_1, \ldots, M_m$, y el modelo real que genera los datos $M_0$ (que pretendemos estimar), entonces

$$P\left[\arg\min_{k=0,\ldots,m}\text{BIC}(\widehat{M}_k)=0\right]\to 1, \quad  n \to \infty$$

Esto solo suponiendo que el modelo subyacente sea lineal claro... Esto [**no sucede con el AIC**]{.hl-red} (para vuestro yo del futuro: en <https://doi.org/10.2307/2290328.> se prueba que el AIC es equivalente a usar validaci√≥n cruzada leave-one-out, el cual es inconsistente)

---

## stepAIC

Y aqu√≠ nos puede nacer una duda: si tengo muchos predictores, [**¬øtengo que calcular el BIC/AIC de todas las combinaciones posibles?**]{.hl-yellow}

. . .

S√≠, pero lo har√° por nosotros `MASS::stepAIC()`, donde en el par√°metro `k = ...` le indicamos la penalizaci√≥n (si `k = 2` es AIC y si `k = log(n)` es el BIC)

```{r}
#| eval: false
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, Price ~ .)
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```

---

## stepAIC

```{r}
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, Price ~ .)
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```


---

## stepAIC

Lo que obtendremos ser√° una [**selecci√≥n secuencial de modelos**]{.hl-yellow}, de manera que el ir√° probando las combinaciones, nos **muestra el ajuste y el valor del BIC/AIC** y se parar√° cuando el AIC/BIC no mejore.

```{r}
#| eval: false
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, Price ~ .)
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```


---

## stepAIC

```{r}
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "forward")
```

El argumento `direction = ...` puede tomar los valores `"both"`, `"forward"` y `"backward"` (por defecto) que nos determina la [**direcci√≥n de b√∫squeda**]{.hl-yellow}:


* `direction = "forward"`: empieza con el modelo proporcionado y va [**a√±adiendo**]{.hl-yellow} predictoras haciendo modelos cada vez m√°s complejos.

---

## stepAIC

```{r}
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "forward")
```


F√≠jate que ahora  como le hemos dado el modelo saturado y `direction = "forward"`, entonces no hace nada. Para controlar esto podemos incluir la variable `scope = list(lower = mod_easy, upper = mod_complex)`, donde **podemos pasarle dos modelos**p: los modelos m√°s complejos y m√°s sencillo **pentre los que queremos que se mueva en la b√∫squeda**.

---

## stepAIC

```{r}
#| eval: false
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "backward")
```

* `direction = "backward"`: empieza con el modelo proporcionado y va [**eliminando**]{.hl-yellow} predictoras haciendo modelos cada vez m√°s sencillos.


```{r}
#| eval: false
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```

* `direction = "both"`: empieza con el modelo proporcionado y va [**a√±adiendo y eliminando**]{.hl-yellow} predictoras seg√∫n sea m√°s conveniente (pudiendo a√±adir/eliminar una variable que previamente e hab√≠a eliminado/a√±adido)




---

## stepAIC

```{r}
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "forward")
```

[**Aviso**]{.hl-red}: en realidad `MASS::stepAIC()` no calcula exactamente el mismo BIC/AIC que las funciones `BIC()` e `AIC()`, sino que les suma una constante $n(\log(2\pi) + 1) + \log(n)$ para BIC y $n(\log(2\pi) + 1) + 2$ para AIC

Dado que es una constante nos da igual para comparar modelos, pero hace que [**NO podamos comparar salidas**]{.hl-red} de `BIC()` y `AIC()` con salidas de `MASS::stepAIC()`.



# Clase 15: casos pr√°cticos {#clase-15}

[**Casos pr√°cticos: datos de viviendas de Boston y seatpos dataset**]{style="color:#444442;"}

---

## Casos pr√°cticos

Realiza todo el ajuste completo multivariante con los datasets:

* `seatpos` del paquete `{faraway}`: datos de 38 conductores donde el objetivo es predecir `hipcenter`, la posici√≥n del asiento del conductor, en funci√≥n de distintas variables (ver `? faraway::seatpos`)

* `Boston` del paquete `{MASS}`:  datos de 560 suburbios de Boston, en el que se han medido 14 variables en cada uno, con el objetivo de predecir `medv` el precio mediano de inmuebles (en millones de dolares), en funci√≥n de variables estructurales (`rm` y `age`), variables de vecindario (`crim`, `zn`, `indus`, `chas`, `tax`, `ptratio`, `black` y `lstat`), variables de accesibilidad (`dis` y `rad`) y variables de calidad del aire (`nox`)


# Clase 16: variables cualitativas {#clase-16}

[**¬øC√≥mo introducir predictoras cualitativas?**]{style="color:#444442;"}


---

## Cosas que faltan por a√±adir

* ejemplo seatpos

* ejemplo one-hot-encoding

* ejemplo boston



# Clase n:

---

## Regresi√≥n penalizada

Hagamos un resumen: [**¬øcu√°l era el objetivo de la regresi√≥n ordinaria?**]{.hl-yellow} 

. . .

Asumiendo que 

$$E \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \displaystyle \sum_{j=1}^{p} \beta_j X_j, \quad \widehat{Y} = \widehat{\beta_0} + \displaystyle \sum_{j=1}^{p} \widehat{\beta_j} X_j, \quad E \left[ \varepsilon | \left( X_1 = x_1, \ldots, X_p = x_p \right) \right] = 0 $$

El objetivo ha sido obtener la estimaci√≥n de los $\widehat{\beta}$ tal que [**minimicemos la varianza residual o suma de errores al cuadrado**]{.hl-yellow} $SSE$

$$SSE \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \displaystyle \sum_{i=1}^{n} \left(Y_i - \beta_0 - \beta_1 X_{i1} - \ldots - \beta_p X_{ip} \right)^2 = \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) $$

. . .

Tras derivar e igualar a cero obten√≠amos $\widehat{\boldsymbol{\beta}}=\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$

---

## Regresi√≥n penalizada


El estimador $\widehat{\boldsymbol{\beta}}$ es aquel que nos garantiza

$$\widehat{\boldsymbol{\beta}} = \arg \min_{\beta \in \mathbb{R}^{p+1}} SSE \left(\boldsymbol{\beta} \right)$$

. . .


Tambi√©n tenemos garantizado que si se [**cumple las hip√≥tesis**]{.hl-green} tenemos que

$$\widehat{\boldsymbol{\beta}} \sim N_{p+1}\left(\boldsymbol{\beta}, \sigma_{\varepsilon}^2 \left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}\right)$$

lo que implica que $E \left[ \widehat{\boldsymbol{\beta}}\right] = \boldsymbol{\beta}$, es decir, tenemos un [**estimador insesgado**]{.hl-yellow}.

. . .

El [**sesgo en una estimaci√≥n**]{.hl-yellow} es un [**error sistem√°tico**]{.hl-yellow} en la misma. Pero hay otra componente importante que hasta ahora solo hemos mencionado pero que no hemos tenido en cuenta: la [**varianza de la estimaci√≥n**]{.hl-yellow}

---

## Balance sesgo-varianza

Si descomponemos el error cuadr√°tico medio de una estimaci√≥n obtenemos que


$$\begin{eqnarray}MSE \left(\widehat{Y} \right) &=& E \left[ \left( \widehat{Y} - Y \right)^2 \right] = E \left[ \left( \left(\widehat{Y} - E \left[ \widehat{Y}\right] \right) - \left(E \left[ \widehat{Y}\right] - Y \right)\right)^2 \right] \nonumber \\ &=& E \left[  \left(\widehat{Y} - E \left[ \widehat{Y}\right] \right)^2 \right] +\left( E \left[  \widehat{Y}\right] - Y \right)^2 + 2*cosas \nonumber \\ &=& \underbrace{\left( E \left[  \widehat{Y}\right] - Y \right)^2}_{Sesgo^2}+\underbrace{Var \left[ \widehat{Y} \right]}_{Varianza} + ruido\end{eqnarray}$$

. . .

El [**eror cuadr√°tico medio podemos descomponerlo**]{.hl-yellow} en una suma de sesgo (al cuadrado), varianza y ruido. El √∫ltimo es intr√≠nseco a los datos, por lo que la metodolog√≠a de estimaci√≥n puede bajar el sesgo (subiendo la varianza) o bajar la varianza (subiendo el sesgo).

---

## Balance sesgo-varianza

:::: columns
::: {.column width="40%"}

* [**Bajoajuste (underfitting)**]{.hl-yellow}: modelos muy simples proporcionan un **sesgo muy grande** y **poca varianza** ya que la predicci√≥n siempre ser√° muy parecida (errores altos en train).

* [**Sobreajuste (overfitting)**]{.hl-yellow}: modelos muy complicados proporcionan un **sesgo bajo** pero al ser tan complejas proporcionar√°n una **mayor varianza** para cada intento (errores altos en test).

:::

::: {.column width="60%"}

![](img/bias_varianc_tradeoff.jpg)

Extra√≠da de <https://mlu-explain.github.io/bias-variance/>
:::
::::

---


## Balance sesgo-varianza

Si lo visto en las estimaciones lo trasladamos a los  [**coeficientes estimados**]{.hl-yellow} tenemos

$$\begin{eqnarray}MSE \left(\widehat{\beta}_j \right) =  \underbrace{\left( E \left[  \widehat{\beta}_j \right] - \beta_j \right)^2}_{Sesgo^2}+\underbrace{Var \left[ \widehat{\beta}_j \right]}_{Varianza} + ruido \end{eqnarray} $$

. . .

En este caso adem√°s sabemos que $Var \left[ \widehat{\beta}_j \right] = SE \left(\widehat{\beta}_j\right)^2 = \sigma_{\varepsilon}^2 v_j$, con $v_j$ el elemento $j$-√©simo de la diagonal de $\left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}$.

. . .

La forma m√°s sencilla de reducir dr√°sticamente la varianza residual es aumentando el n√∫mero de par√°metros, es decir sobreajustando. Dicho de otra forma: para conseguir [**simplificar modelos vamos a reducir la varianza de las estimaciones**]{.hl-yellow}, y para ello lo que haremos ser√° [**introducir un peque√±o sesgo**]{.hl-yellow}

---



## Regresi√≥n penalizada



$$\begin{eqnarray}MSE \left(\widehat{\beta}_j \right) &=&  \underbrace{\left( E \left[  \widehat{\beta}_j \right] - \beta_j \right)^2}_{Sesgo^2}+\underbrace{Var \left[ \widehat{\beta}_j \right]}_{Varianza} + ruido = cte\end{eqnarray} $$

La idea ser√° [**sesgar los coeficientes**]{.hl-yellow} de tal manera que se busque obtener [**coeficientes que, en caso de no ser tan importantes como otros, tiendan a cero**]{.hl-yellow}, un sesgo hacia lo que se conoce como [**sparsity**]{.hl-purple}.

. . .

Para ello haremos uso de lo que se conoce como [**shrinkage methods**]{.hl-yellow}, y  en particular hablaremos de [**regresi√≥n penalizada**]{.hl-yellow}

---

## Regresi√≥n penalizada

Si en el caso de la regresi√≥n lineal ordinaria ten√≠amos

$$\widehat{\boldsymbol{\beta}} = \arg \min_{\beta \in \mathbb{R}^{p+1}} SSE \left(\boldsymbol{\beta} \right)$$
 
 
 Ahora incluiremos una [**penalizaci√≥n $\lambda \geq 0$**]{.hl-red}
 
 $$\widehat{\boldsymbol{\beta}}_{\lambda} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left\| \beta_{-1} \right\| \right), \quad \lambda \geq 0$$
 
 donde $\left\| \beta \right\|$ ser√° [**alguna norma del vector de coeficientes (excluyendo el intercepto)**]{.hl-yellow} (distancia del vector a 0) y $\lambda$ deberemos determinar su valor √≥ptimo.
 
. . .

Por tanto ahora el objetivo no ser√° solo [**minimizar los errores al cuadrado**]{.hl-yellow} sino tambi√©n [**minimizar la penalizaci√≥n**]{.hl-yellow}, y la √∫nica forma de hacerlo (dado que una norma siempre devuelve algo positiva) es que los [**coeficientes sean lo m√°s peque√±o posibles**]{.hl-yellow}

---

## Penalizaci√≥n


$$\widehat{\boldsymbol{\beta}}_{\lambda} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left\| \beta_{-1} \right\| \right), \quad \lambda \geq 0$$
 
 ¬øQu√© sucede cuando $\lambda$ **crece o decrece**?
 
 . . .
 
 * Si $\lambda \to 0$ --> estamos en el caso de **m√≠nimos cuadrados ordinarios** (no hay penalizaci√≥n)
 
 * Si $\lambda \to \infty$ --> $\beta_j \to 0$ para todo $j \geq 0$ (el modelo tiende a desaparecer)
 
---

## Familia de normas
 
$$\widehat{\boldsymbol{\beta}}_{\lambda} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left\| \beta_{-1} \right\| \right), \quad \lambda \geq 0$$
 
Una de las [**familias de normas**]{.hl-yellow} m√°s habituales son las [**normas $\ell^p$**]{.hl-yellow}

$$\left\| x \right\|_{p} = \left\| \left(x_1, \ldots, x_k \right)\right\|_{p} = \left( \sum_{j=1}^{k} \left| x_{j} \right|^{p} \right)^{1/p}, \quad x \in \mathbb{R}^{k}$$

. . .

* Si $p=1$ --> $\left\| x \right\|_{1}  =  \sum_{j=1}^{k} \left| x_{j} \right|$ se conoce como [**norma taxicab o de Manhattan**]{.hl-yellow} (la m√©trica que usar√≠amos para recorrer calles en un mapa)

* Si $p=2$ --> $\left\| x \right\|_{2}  =  \left(\sum_{j=1}^{k} x_{j}^{2} \right)^{1/2}$ se conoce como [**norma Eucl√≠dea**]{.hl-yellow} (la habitual)

* Si $p=\infty$ --> $\left\| x \right\|_{\infty}  =  \lim_{p \to \infty} \left(\sum_{j=1}^{k} \left| x_{j} \right|^{p} \right)^{1/p} = \displaystyle \max_{j} \left| x_{j} \right|$ se conoce como [**norma infinita o de Chebyshev**]{.hl-yellow} (movimientos de rey en ajedrez)

---


## Regresi√≥n elastic-net

 
Dado que las m√°s usadas son la [**norma Manhattan y la Eucl√≠dea**]{.hl-yellow}, podemos generalizar la regresi√≥n penalizada anterior combinando ambas penalizaciones

 
$$\widehat{\boldsymbol{\beta}}_{\lambda, \alpha} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left(\alpha \left\| \beta_{-1} \right\|_1  + (1-\alpha) \left\| \beta_{-1} \right\|_{2}^{2} \right)\right), \quad \lambda \geq 0,~0 \leq \alpha \leq 1$$

La [**regresi√≥n elastic-net**]{.hl-yellow} es una regresi√≥n penalizada que combina una [**penalizaci√≥n $\lambda$**]{.hl-red} (que nos cuantifica lo agresivo que queremos hacer tender los coeficientes a cero) y una [**proporci√≥n $\alpha$**]{.hl-green} (que nos permite combinar ambas penalizaciones)

. . .

::: callout-important
## Importante

Dado que la penalizaci√≥n depende de la norma de los coeficientes, su magnitud ser√° importante, por lo que [**deberemos estandarizar antes los predictores**]{.hl-yellow}.

:::

---
 
## Ridge y lasso

 
$$\widehat{\boldsymbol{\beta}}_{\lambda, \alpha} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left(\alpha \left\| \beta_{-1} \right\|_1  + (1-\alpha) \left\| \beta_{-1} \right\|_{2}^{2} \right)\right), \quad \lambda \geq 0,~0 \leq \alpha \leq 1$$

Hay dos escenarios de penalizaci√≥n principalmente conocidos:

* [**Regresi√≥n ridge**]{.hl-yellow}: cuando $\alpha = 0$ (penalizaci√≥n cuadr√°tica)

$$\widehat{\boldsymbol{\beta}}_{\lambda, ridge} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda  \left\| \beta_{-1} \right\|_{2}^{2} \right), \quad \lambda \geq 0$$

* [**Regresi√≥n LASSO**]{.hl-yellow}: cuando $\alpha = 1$ (penalizaci√≥n absoluta)


$$\widehat{\boldsymbol{\beta}}_{\lambda, LASSO} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda  \left\| \beta_{-1} \right\|_1  \right), \quad \lambda \geq 0$$

---

## Ridge y lasso

* [**Regresi√≥n ridge**]{.hl-yellow}: cuando $\alpha = 0$ (penalizaci√≥n cuadr√°tica)

$$\widehat{\boldsymbol{\beta}}_{\lambda, ridge} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda  \left\| \beta_{-1} \right\|_{2}^{2} \right), \quad \lambda \geq 0$$

Una de las [**ventajas de la regresi√≥n ridge**]{.hl-green} es que podemos calcular su [**expresi√≥n expl√≠cita**]{.hl-green} (algo que no sucede en la regresi√≥n lasso, solo se puede obtener mediante simulaci√≥n num√©rica)

$$\widehat{\boldsymbol{\beta}}_{\lambda, ridge} = \left(\mathbf{X}^{T}\mathbf{X} + \lambda \mathbf{I}_{p \times p} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$$
donde $\mathbf{I}_{p \times p}$ es la matriz identidad.


---

## Ridge y lasso

* [**Regresi√≥n LASSO**]{.hl-yellow}: cuando $\alpha = 1$ (penalizaci√≥n absoluta)


$$\widehat{\boldsymbol{\beta}}_{\lambda, LASSO} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda  \left\| \beta_{-1} \right\|_1  \right), \quad \lambda \geq 0$$

Una de las [**ventajas de la regresi√≥n LASSO**]{.hl-green} es que nos realiza una [**selecci√≥n de variables**]{.hl-green} ya hace [**tender m√°s r√°pido los coeficientes a cero**]{.hl-yellow}.

&nbsp;

¬øPor qu√©?

--- 

## Regresi√≥n elastic-net

 
$$\widehat{\boldsymbol{\beta}}_{\lambda, \alpha} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left(\alpha \left\| \beta_{-1} \right\|_1  + (1-\alpha) \left\| \beta_{-1} \right\|_{2}^{2} \right)\right), \quad \lambda \geq 0,~0 \leq \alpha \leq 1$$

La [**regresi√≥n elastic-net**]{.hl-yellow} puede ser tambi√©n vista como un [**problema de minimizaci√≥n de la suma de los residuos al cuadrado**]{.hl-yellow} con una restricci√≥n en los argumentos


$$\widehat{\boldsymbol{\beta}}_{s_\lambda, \alpha} = \arg \min_{\beta \in \mathbb{R}^{p+1}:~\sum_{j=1}^{p} \left(\alpha \left| \beta_{j} \right| + (1-\alpha) \left| \beta_{j} \right|^{2} \right) \leq s_\lambda} SSE \left(\boldsymbol{\beta} \right) , \quad 0 \leq \alpha \leq 1$$

donde $s_\lambda$ es un valor num√©rico que no depende de $\beta$.

---

## Regresi√≥n elastic-net

Por ejemplo, si tenemos solo dos predictoras...

![](img/elastic-net.jpg)

Las elipses nos muestran los valores para los que $SSE \left( \boldsymbol{\beta} \right)$ es constante (lo que queremos minimizar), y las **regiones sombreadas representan el espacio de valores posibles** debido a la penalizaci√≥n (ver m√°s en <https://link.springer.com/book/10.1007/978-1-4614-7138-7>)

---

## LASSO


Si nos fijamos en ambas regiones, dado que el [**LASSO usa una norma absoluta**]{.hl-yellow}, el espacio de restricciones es m√°s "picudo", lo que har√° m√°s probable que cuando la **elipse corte con la zona sombreada** lo haga en los v√©rtices (donde uno de los coeficientes es cero) en lugar de en los laterales.

![](img/elastic-net.jpg)

Ver m√°s en <https://link.springer.com/book/10.1007/978-1-4614-7138-7>



---


## Selecci√≥n de la penalizaci√≥n

En todo este proceso de regresi√≥n penalizada hay un "elefante en la habitaci√≥n": [**¬øc√≥mo seleccionar el $\lambda$ m√°s √≥ptimo?**]{.hl-yellow}

. . .


Una de las m√°s formas m√°s habituales para ello es lo que se conoce como [**validaci√≥n cruzada**]{.hl-yellow}. La idea se basa en un ideal: poder **disponer de muchas muestras** tal que, probando en cada una distintos valores de $\lambda$, consigamos obtener aquel que nos proporcione (en media) el menor error.

. . .

[**¬øEl problema?**]{.hl-red} Casi nunca disponemos de un gran n√∫mero de muestras de la misma poblaci√≥n, as√≠ que una [**soluci√≥n es "simularlas" nosotros mismos a partir de la muestra original**]{.hl-green}

---

## Validaci√≥n cruzada

Supongamos que tenemos una muestra tal que contamos con una variable objetivo a predecir en funci√≥n de $p$ predictoras $\left\lbrace Y_i, \left(X_{i1}, \ldots, X_{ip} \right) \right\rbrace_{i=1,\ldots,n}$, y que pretendemos usar un [**estimador $\widehat{f}_{\lambda}$ que depende de un par√°metro $\lambda$**]{.hl-yellow}, tal que

$$\widehat{Y} =\widehat{f}_{\lambda} \left(X_{1}, \ldots, X_{p} \right)$$

La opci√≥n m√°s sencilla es lo que se conoce como [**validaci√≥n simple**]{.hl-yellow}:


---

## Validaci√≥n simple


$$\widehat{Y} =\widehat{f}_{\lambda} \left(X_{1}, \ldots, X_{p} \right)$$
La opci√≥n m√°s sencilla es lo que se conoce como [**validaci√≥n simple**]{.hl-yellow}:


1. Hacemos una partici√≥n de train y test. De los datos de train, realizamos una segunda partici√≥n que llamamos [**conjunto de validaci√≥n**]{.hl-yellow}.

. . .

2. [**Entrenamos solo  con train**]{.hl-yellow} el modelo para distintos valores de $\lambda$

. . .

3. Evaluamos el error que produce cada $\widehat{f}_{\lambda}$ en el [**conjunto de validaci√≥n**]{.hl-yellow} (un conjunto del que el modelo no ha aprendido) para [**elegir el mejor par√°metro**]{.hl-yellow} $\lambda$.

. . .

4. Proporcionamos una evaluaci√≥n final en la tercera partici√≥n (test), que no ha sido usada ni para la estimaci√≥n ni para la elecci√≥n del mejor par√°metro.

---

## Validaci√≥n cruzada

La [**validaci√≥n simple**]{.hl-yellow} tiene un problema: [**depende much√≠simo de la partici√≥n aleatoria**]{.hl-red} que se realice (si tenemos mala o buena suerta) y, adem√°s, puede provocar un [**problema de tama√±o muestral**]{.hl-red} si $n$ no es muy grande (ya que necesitamos 3 particiones de los datos).

. . .

Para solventarlo existe una alternativa conocida como [**validaci√≥n cruzada**]{.hl-yellow}: en lugar de extraer un subconjunto de train, lo que se hace es que **cada observaci√≥n pueda jugar ambos roles**: el rol de entrenamiento y el rol de validaci√≥n.


---

## Validaci√≥n cruzada


La m√°s famosa es la conocida como [**leave-one-out cross-validation (LOOCV)**]{.hl-yellow}:

1. Hacemos una partici√≥n de train (con $m$ elementos) y test ($n - m$).

2. De los datos de train $\left\lbrace X_{i} \right\rbrace_{i=1,\ldots,m}$, entrenamos el modelo sin una de las observaciones de entrenamiento $\boldsymbol{X}_{-1} = \left\lbrace X_{i} \right\rbrace_{i=2,\ldots,m}$, y usamos esa observaci√≥n $X_{1}$ para obtener una m√©trica del error para cada $\lambda$ (un conjunto de validaci√≥n de tama√±o muestral igual a 1).

3. Repetimos el proceso para cada una de ellas, considerando como entrenamiento $\boldsymbol{X}_{-j} = \left\lbrace X_{i} \right\rbrace_{i=1,\ldots, j-1, j+1, \ldots,m}$ y $X_j$ como conjunto de validaci√≥n.

4. En total se entrena el modelo con $n$ conjuntos train (de $n-1$ observaciones cada uno) y $n$ conjuntos de validaci√≥n (de una observaci√≥n cada uno), obteniendo al final una media del error en entrenamiento y otra en validaci√≥n, usando esta √∫ltima para [**elegir el mejor par√°metro**]{.hl-yellow} $\lambda$.

---

## Validaci√≥n cruzada


La anterior idea puede ser generalizada a lo que se conoce como [**leave-k-out cross-validation (LOOCV)**]{.hl-yellow}:

1. Hacemos una partici√≥n de train (con $m$ elementos) y test ($n - m$).

2. De los datos de train $\left\lbrace X_{i} \right\rbrace_{i=1,\ldots,m}$, entrenamos el modelo sin $k$ observaciones de entrenamiento $\boldsymbol{X}_{-i_1, \ldots, -i_k}$, y usamos esa $k$ observaci√≥nes $\left\lbrace X_{i_1}, \ldots, X_{i_k} \right\rbrace$ para obtener una m√©trica del error (un conjunto de validaci√≥n de tama√±o muestral igual a $k$).

3. Repetimos el proceso para cada posible forma de combinar $m$ elementos de $k$ en $k$ 

4. En total se entrena el modelo con $C_{k}^{m}$ conjuntos train (de $m-k$ observaciones cada uno) y $C_{k}^{m}$ conjuntos de validaci√≥n (de $k$ observaciones cada uno), obteniendo estos √∫ltimos para [**elegir el mejor par√°metro**]{.hl-yellow} $\lambda$.


&nbsp;

Aunque esta opci√≥n nos da una m√©trica m√°s robusta del error, si hacemos unas sencillas cuentas vemos que puede no ser factible si $m$ y/o $k$ toman valores elevados. Por ejemplo, si $m = 100$ y $k = 30$, tendr√≠amos que entrenar $C_{k}^{m} \simeq 10^{25}$ modelos


---

## Validaci√≥n cruzada k-folds

Dado que las dos opciones anteriores pueden no servirnos existe una tercera v√≠a, la conocida como [**validaci√≥n cruzada k-folds**]{.hl-yellow}:

![](img/k-fold.png)

---

## Validaci√≥n cruzada k-folds

Dado que las dos opciones anteriores pueden no servirnos existe una tercera v√≠a, la conocida como [**validaci√≥n cruzada k-folds**]{.hl-yellow}:


1. Hacemos una partici√≥n de train (con $m$ elementos) y test ($n - m$).

2. De los datos de train $\left\lbrace X_{i} \right\rbrace_{i=1,\ldots,m}$, dividimos dicho conjunto en $k$ particiones ($k$ folds) que denotaremos como $\boldsymbol{X}_{fold_1}, \ldots, \boldsymbol{X}_{fold_k}$ (con $m/k$ observaciones cada una aproximadamente).

3. Entrenamos $k$ modelos, de manera que en la iteraci√≥n $j$, usaremos las $m-m/k$ observaciones de las particiones $\boldsymbol{X}_{fold_1}, \ldots, \boldsymbol{X}_{fold_{j-1}}, \boldsymbol{X}_{fold_{j+1}}, \ldots, \boldsymbol{X}_{fold_{k}}$ para entrenar y las $m/k$ observaciones de $\boldsymbol{X}_{fold_{j}}$ para validar.

4. De esta manera **cada observaci√≥n jugar√° $k-1$ veces el rol de entrenamiento** y **una sola vez el rol de validaci√≥n**, teniendo $k$ validaciones cuyos resultados promediaremos para [**elegir el mejor par√°metro**]{.hl-yellow} $\lambda$.

---

## Selecci√≥n de la penalizaci√≥n

As√≠ llamaremos $\widehat{\lambda}_{k-CV}$ a la [**penalizaci√≥n √≥ptima elegida a trav√©s de validaci√≥n k-fold**]{.hl-yellow}

$$\widehat{\lambda}_{k-CV} = \arg \min_{\lambda \geq 0} CV_k(\lambda),\quad CV_k(\lambda)=\sum_{j=1}^k  SSE \left(\widehat{\boldsymbol{\beta}_{(j)} } \right)$$
donde $SSE \left(\widehat{\boldsymbol{\beta}}_{(k)} \right)$ es la suma de residuos al cuadrado del $j$-√©simo slot de validaci√≥n (con los par√°metros obtenidos en el entrenamiento sin ese slot).

---

## Selecci√≥n de la penalizaci√≥n


$$\widehat{\lambda}_{k-CV} = \arg \min_{\lambda \geq 0} CV_k(\lambda),\quad CV_k(\lambda)=\sum_{j=1}^k  SSE \left(\widehat{\boldsymbol{\beta}_{(j)} } \right)$$

Con el objetivo de [**simplificar a√∫n m√°s el modelo**]{.hl-yellow} existe una variante de penalizaci√≥n $\widehat{\lambda}_{k-1SE}$ conocida como [**¬´one standard error rule¬ª**]{.hl-yellow}: la idea es elegir el [**modelo m√°s sencillo de los modelos aceptanles**]{.hl-yellow}, la penalizaci√≥n m√°s grande posible (es decir, el modelo m√°s sencillo posible) acerc√°ndose lo m√°ximo que pueda a $\widehat{\lambda}_{k-CV}$. La idea consiste en estimar $\widehat{\lambda}_{k-CV}$ y calcular su correspondiente $CV_k(\widehat{\lambda}_{k-CV})$  para cada partici√≥n de validaci√≥n(que es en s√≠ una variable aleatoria), para posteriormente [**estimar su variabilidad**]{.hl-yellow} ($SE$ igual a desviaci√≥n t√≠pica dividida entre $\sqrt{n}$) entre todas las particiones de validaci√≥n, tal que

$$\widehat{\lambda}_{k-1SE}= \max \left\lbrace \lambda \geq 0: CV_k(\lambda) \in \left(CV_k(\widehat{\lambda}_{k-CV})\pm\widehat{\mathrm{SE}}\left(CV_k(\widehat{\lambda}_{k-CV})\right)\right)\right\rbrace$$

---


## Regresi√≥n penalizada en R

Vamos a realizar un peque√±o ejemplo con el famoso dataset `iris`, al que primero lo preprocesaremos convenientemente para tener solo variables num√©ricas (la variable objetivo ser√° `Sepal.Length`)

```{r}
#| code-fold: true
datos <- as_tibble(iris)
datos_preproc <-
  datos |> 
  fastDummies::dummy_cols(select_columns = "Species", remove_first_dummy = TRUE) |> 
  select(-Species)

# Ajuste Saturado
ajuste_saturado <- lm(data = datos_preproc, Sepal.Length ~ .)
ajuste_saturado |> summary()
```

---

## Regresi√≥n penalizada en R

Tras realizar el ajuste saturado (vamos a hacer una prueba sencilla, no hacemos colinealidad ni diagnosis, solo ajuste), [**seleccionamos modelos con AIC y BIC**]{.hl-yellow}

```{r}
#| eval: false
ajuste_AIC <- MASS::stepAIC(ajuste_saturado, direction = "both", k = 2)
ajuste_BIC <- MASS::stepAIC(ajuste_saturado, direction = "both", k = log(nrow(datos_preproc)))
```

```{r}
#| eval: false
ajuste_AIC |> summary()
ajuste_BIC |> summary()
```

---

## Regresi√≥n ridge en R

$$\widehat{\boldsymbol{\beta}}_{\lambda, \alpha} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left(\alpha \left\| \beta_{-1} \right\|_1  + (1-\alpha) \left\| \beta_{-1} \right\|_{2}^{2} \right)\right), \quad \lambda \geq 0,~0 \leq \alpha \leq 1$$

[**¬øC√≥mo realizar una regresi√≥n penalizada en R?**]{.hl-yellow}. Para ello vamos a usar el complet√≠simo paquete `{glmnet}`, cuya documentaci√≥n puedes consultar en <https://glmnet.stanford.edu/articles/glmnet.html#linear-regression-family-gaussian-default>

. . .

Para realizar una ridge regresi√≥n, es decir $\alpha = 0$, basta con hacer uso de la funci√≥n `glmnet()`, d√°ndole en `x` matriz de variables (solo num√©ricas en formato matriz), en `y` la variable objetivo y `alpha = 0`

```{r}
library(glmnet)

y <- datos_preproc |> select(Sepal.Length) |> as.matrix()
x <- datos_preproc |> select(-Sepal.Length) |> as.matrix()

ajuste_ridge <- glmnet(x = x, y = y, alpha = 0)
```

---


## Regresi√≥n ridge en R

[**¬øQu√© tenemos guardado?**]{.hl-yellow}

* `$lambda`: tenemos los valores de $\lambda$ probados (por defecto, hay un argumento `nlambda` en `glmnet()` que vale 100, y la secuencia de valores los decide en funci√≥n de otro argumento `lambda.min.ratio` que por defecto vale 0.01)

```{r}
ajuste_ridge$lambda
```

---

## Regresi√≥n ridge en R

[**¬øQu√© tenemos guardado?**]{.hl-yellow}

* `$dev.ratio`: tenemos lo que se conoce como ¬´deviance¬ª para las penalizaciones probadas, que es una generalizaci√≥n de $R^2$ para modelos m√°s complejos (en nuestro caso es equivalente)

```{r}
ajuste_ridge$dev.ratio
```

---

## Regresi√≥n ridge en R

F√≠jate que a [**mayor penalizaci√≥n (modelos m√°s simples, m√°s sparsity)**]{.hl-yellow}, el $R^2$ disminuye (o lo que es lo mismo, aumenta el SSE)

```{r}
ggplot(tibble("lambda" = log(ajuste_ridge$lambda),
              "R2" = ajuste_ridge$dev.ratio)) +
  geom_line(aes(x = lambda, y = R2)) +
  theme_minimal() + labs(x = "log-lambda", y = "R2")
```


---

## Regresi√≥n ridge en R

* `$a0`: tenemos guardados los [**interceptos $\widehat{\beta}_0$**]{.hl-yellow} de las regresiones ajustadas para cada penalizaci√≥n (en este caso 100 valores distintos)

* `$beta`: tenemos guardados en formato matricial los [**coeficientes $\widehat{\beta}_j$**]{.hl-yellow} (para $j \geq 1$) de las regresiones ajustadas para cada penalizaci√≥n (en este caso una matriz de 5 filas (5 predictoras) y 100 columnas (100 penalizaciones)

```{r}
dim(ajuste_ridge$beta)
length(ajuste_ridge$a0)
```

---

## Regresi√≥n ridge en R

F√≠jate que a [**mayor penalizaci√≥n (modelos m√°s simples, m√°s sparsity)**]{.hl-yellow}, la [**norma Eucl√≠dea $\left\| \cdot \right\|_2$ de los coeficientes**]{.hl-yellow} disminuye (ya que penaliza m√°s)


```{r}
ggplot(tibble("lambda" = log(ajuste_ridge$lambda),
              "norma" = sqrt(colSums(ajuste_ridge$beta^2)))) +
  geom_line(aes(x = lambda, y = norma)) +
  theme_minimal() + labs(x = "log-lambda", y = "norma")
```


---

## Selecci√≥n de lambda en R

Para no tener que elegir manualmente el mejor $\lambda$, la funci√≥n `cv.glmnet()` nos permite elegir el mejor  $\widehat{\lambda}_{k-CV}$ ([**penalizaci√≥n √≥ptima elegida a trav√©s de validaci√≥n k-fold**]{.hl-yellow})

* `nfolds`: indica el n√∫mero $k$ de particiones en la validaci√≥n cruzada.

* `type.measure`: m√©trica de error usada (por defecto lo que hemos llamado $SSE$ o $MSE$)

```{r}
set.seed(12345)
kfolds_lambda_ridge <- cv.glmnet(x = x, y = y, alpha = 0, nfolds = 20)
```


---

## Selecci√≥n de lambda en R

```{r}
set.seed(12345)
kfolds_lambda_ridge <- cv.glmnet(x = x, y = y, alpha = 0, nfolds = 20)
```

* `$lambda`: los valores $\lambda$ probados.
* `$lambda.min`: el valor que nos proporciona el promedio de errores en validaci√≥n m√°s peque√±o
* `$cvm`: valores $CV_k(\lambda)$


```{r}
# Lambda m√≠nimo
kfolds_lambda_ridge$lambda.min

# Lambda m√≠nimo manualmente
cv_min <- which.min(kfolds_lambda_ridge$cvm)
kfolds_lambda_ridge$lambda[cv_min]
```

---

## Selecci√≥n de lambda en R


Es [**importante chequear que el m√≠nimo no sea un extremo**]{.hl-yellow} de la secuencia de $\lambda$ (ya que sino no sabemos si el m√≠nimo est√° fuera del rango).

```{r}
min(kfolds_lambda_ridge$lambda)
kfolds_lambda_ridge$lambda.min
```

. . .

Para solventarlo podemos [**proporcionar manualmente un grid (normalmente logar√≠tmico)**]{.hl-yellow} de penalizaciones con las que debe probar la funci√≥n `glmnet(..., lambda = ...)`

```{r}
# grid logar√≠tmico
lambda_seq <- 10^seq(log10(max(kfolds_lambda_ridge$lambda)), log10(0.0001), l = 200)

set.seed(12345)
kfolds_lambda_ridge <-
  cv.glmnet(x = x, y = y, alpha = 0, nfolds = 20, lambda = lambda_seq)
kfolds_lambda_ridge$lambda.min
```

---

## Selecci√≥n de lambda en R


* `$lambda.1se`: mejor penalizaci√≥n $\widehat{\lambda}_{k-1SE}$ seg√∫n [**¬´one standard error rule¬ª**]{.hl-yellow}

```{r}
kfolds_lambda_ridge$lambda.1se
kfolds_lambda_ridge
```

---

## Selecci√≥n de lambda en R

Con la funci√≥n `plot()` podemos dibujar de manera autom√°tica los distintos valores $CV_k(\lambda)$, resultado de promediar en los $k$-folds de la validaci√≥n (la incertidumbre se representa con las barras de error). Con `abline()` podemos pintar las lineas horizontales para $\widehat{\lambda}_{k-CV}$ y $\widehat{\lambda}_{k-1SE}$.

```{r}
plot(kfolds_lambda_ridge)
abline(h = min(kfolds_lambda_ridge$cvm) + c(0, min(kfolds_lambda_ridge$cvsd)))
```

---

## Selecci√≥n de lambda en R

Podemos replicarlo en ggplot ya que en `$cvlo` y `$cvup` tenemos guardado los valores superiores e inferiores en las validaciones probadas (los extremos de las barras de error)

```{r}
#| code-fold: true
ggplot() +
  geom_errorbar(data =
                  tibble("log_lambda" = log(kfolds_lambda_ridge$lambda),
                         "cvlo" = kfolds_lambda_ridge$cvlo,
                         "cvup" = kfolds_lambda_ridge$cvup),
                aes(x = log_lambda,
                    ymin = cvlo, ymax = cvup), color = "gray60") + 
		geom_point(data =
		             tibble("log_lambda" = log(kfolds_lambda_ridge$lambda),
		                    "MSE" = kfolds_lambda_ridge$cvm),
		           aes(x = log_lambda, y = MSE), color = "red") +
		geom_vline(xintercept = log(kfolds_lambda_ridge$lambda.min),
		           linetype = "dashed") +
		geom_vline(xintercept = log(kfolds_lambda_ridge$lambda.1se),
		           linetype = "dashed") +
  theme_minimal()
  
```


---

## Regresi√≥n ridge en R


Para [**aplicar el modelo de regresi√≥n ridge con la penalizaci√≥n √≥ptima considerada**]{.hl-yellow} se puede hacer uso de `predict(kfolds_lambda_ridge, ...)`, indic√°ndole que quieres ¬´predecir¬ª los coeficientes (`type = "coefficients"`) o con `coef()`


```{r}
predict(kfolds_lambda_ridge, type = "coefficients",
        s = kfolds_lambda_ridge$lambda.1se)
coef(kfolds_lambda_ridge)
```   

---

## Regresi√≥n ridge en R


::: callout-important
## Importante

Los coeficientes calculados son los promedios en validaci√≥n. Si se quiere un ajuste "exacto" en el dataset original...

```{r}
ajuste_ridge_CV <- glmnet(x = x, y = y, alpha = 0,
                          lambda = kfolds_lambda_ridge$lambda.1se)
ajuste_ridge_CV$dev.ratio
ajuste_ridge_CV$a0
ajuste_ridge_CV$beta

predict(kfolds_lambda_ridge, type = "coefficients",
        s = kfolds_lambda_ridge$lambda.1se, newx = x)
```  

:::

---

## Regresi√≥n lasso en R


$$\widehat{\boldsymbol{\beta}}_{\lambda, \alpha} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left(\alpha \left\| \beta_{-1} \right\|_1  + (1-\alpha) \left\| \beta_{-1} \right\|_{2}^{2} \right)\right), \quad \lambda \geq 0,~0 \leq \alpha \leq 1$$

La √∫nica diferencia para implementar la [**regresi√≥n lasso en R**]{.hl-yellow} es $\alpha = 1$ (con la misma funci√≥n `glmnet()`)

```{r}
ajuste_lasso <- glmnet(x = x, y = y, alpha = 1)
dim(ajuste_lasso$beta)
length(ajuste_lasso$a0)
ajuste_lasso$lambda
```

---

## Regresi√≥n lasso en R

F√≠jate que, de nuevo, a [**mayor penalizaci√≥n (modelos m√°s simples, m√°s sparsity)**]{.hl-yellow}, el $R^2$ disminuye (o lo que es lo mismo, aumenta el SSE)

```{r}
ggplot(tibble("lambda" = log(ajuste_lasso$lambda),
              "R2" = ajuste_lasso$dev.ratio)) +
  geom_line(aes(x = lambda, y = R2)) +
  theme_minimal() + labs(x = "log-lambda", y = "R2")
```

---

## Regresi√≥n lasso en R

De nuevo a [**mayor penalizaci√≥n (modelos m√°s simples, m√°s sparsity)**]{.hl-yellow}, la [**norma absoluta $\left\| \cdot \right\|_1$ de los coeficientes**]{.hl-yellow} disminuye (ya que penaliza m√°s)


```{r}
ggplot(tibble("lambda" = log(ajuste_lasso$lambda),
              "norma" = colSums(abs(ajuste_lasso$beta)))) +
  geom_line(aes(x = lambda, y = norma)) +
  theme_minimal() + labs(x = "log-lambda", y = "norma")
```


---

## Selecci√≥n de lambda en R

De nuevo podemos hacer uso de `cv.glmnet()` para elegir la ([**penalizaci√≥n √≥ptima elegida a trav√©s de validaci√≥n k-fold**]{.hl-yellow})

```{r}
set.seed(12345)
kfolds_lambda_lasso <- cv.glmnet(x = x, y = y, alpha = 1, nfolds = 20)
min(kfolds_lambda_lasso$lambda)
kfolds_lambda_lasso$lambda.min
kfolds_lambda_lasso$lambda.1se

lambda_seq <- 10^seq(log10(max(kfolds_lambda_ridge$lambda)), log10(0.0001), l = 200)

set.seed(12345)
kfolds_lambda_lasso <- cv.glmnet(x = x, y = y, alpha = 1, nfolds = 20, lambda = lambda_seq)
min(kfolds_lambda_lasso$lambda)
kfolds_lambda_lasso$lambda.min
kfolds_lambda_lasso$lambda.1se
```

---


## Selecci√≥n de lambda en R

Con la funci√≥n `plot()` podemos dibujar de nuevo de manera autom√°tica los distintos valores $CV_k(\lambda)$: los n√∫meros que aparecen arriba indican el [**n√∫mero de predictores distintos de 0**]{.hl-yellow}

```{r}
plot(kfolds_lambda_lasso)
abline(h = min(kfolds_lambda_lasso$cvm) + c(0, min(kfolds_lambda_lasso$cvsd)))
```


---

## Regresi√≥n lasso en R

La predicci√≥n se realiza de la misma manera (depende de si queremos el promedio en validaci√≥n o del conjunto de train original): ahora **algunos coeficientes est√°n a 0** lo cual nos puede servir no solo como modelo sino como [**m√©todo de selecci√≥n de variables**]{.hl-yellow} (ejecutando ahora una regresi√≥n sin penalizar pero sin las variables cuyos coeficientes son 0).

```{r}
predict(kfolds_lambda_lasso, type = "coefficients",
        s = kfolds_lambda_lasso$lambda.1se, newx = x)
```  

---

## M√≠nimo global

Uno de los [**problemas de la validaci√≥n cruzada**]{.hl-red} es que podemos encontrarnos con lo que se conoce como [**L-shaped problem**]{.hl-yellow}: la curva de la cual pretendemos encontrar el m√≠nimo no tiene un m√≠nimo global. Veamos un ejemplo.

```{r}
#| code-fold: true
set.seed(12345)
p <- 100
n <- 300
x <- matrix(rnorm(n * p), n, p)
y <- 1 + rnorm(n)
lambdaGrid <- exp(seq(-10, 3, l = 200))

# Validaci√≥n leave-1-out
plot(cv.glmnet(x = x, y = y, alpha = 1, nfolds = n, lambda = lambdaGrid))
```

Esto puede ser un indicio de que el **intercepto es significativo y no as√≠ el resto de predictores**

---

## Consistencia reg. penalizada



:::: columns
::: {.column width="55%"}

![](img/consistency-lasso.jpg)

:::

::: {.column width="45%"}

Como suced√≠a entre el BIC/AIC, en [Zhao and Yu (2006)](https://www.jmlr.org/papers/v7/zhao06a.html) se prueba como la [**selecci√≥n lasso es consistente bajo la one standard rule**]{.hl-yellow} (bajo ciertas condiciones), pero no as√≠ cuando $\lambda$ es seleccionado por validaci√≥n cruzada (incluye demasiados m√°s predictores de los necesarios, como le suced√≠a al AIC)

Imagen obtenida de <https://bookdown.org/egarpor/PM-UC3M/lm-iii-shrink.html#lm-iii-shrink-varsel>

:::
::::

---

## Deberes

> Dise√±a un estudio que, con el dataset iris y considerando el modelo general de elastic-net, nos determina el mejor par $\left(\alpha, \lambda \right)$ (de manera conjunta).

# Clase n+1: regresi√≥n log√≠stica


[**Introducci√≥n a los modelos lineales generalizados (glm)**]{style="color:#444442;"}

---

## Regresi√≥n log√≠stica

Hasta ahora nuestra [**variable objetivo era una variable continua**]{.hl-yellow}. De hecho si echamos la vista atr√°s, siempre y cuando se cumplan las hip√≥tesis, el modelo lineal cumple que


$$Y | \left( \boldsymbol{X} = \left( x_1, \ldots, x_p \right) \right)  \sim N \left(\beta_0 + \beta_1x_1 + \ldots + \beta_p x_p, \sigma_{\varepsilon}^2 \right)$$

. . .

¬øPero qu√© sucede cuando nuestra [**variable objetivo es cualitativa**]{.hl-yellow} (o cuantitativa discreta)? 

---

## Regresi√≥n log√≠stica

Vamos a empezar por el caso m√°s sencillo: una [**variable objetivo binaria**]{.hl-yellow}. Para ello vamos a cargar el dataset `manif.rda` (basta con usar `load()` ya que la extensi√≥n `.rda` es un fichero nativo de `R`)

```{r}
load(file = "./datos/manif.rda")
data <- as_tibble(manif)
```

&nbsp;

En este caso la **variable objetivo** ser√° `man` (binaria, asisti√≥ una persona o no a un manifestaci√≥n) y contamos **solo con una predictora, la edad**, que es una cuantitativa continua.

```{r}
data |> count(man)
```

---

## Regresi√≥n log√≠stica

```{r}
#| code-fold: true
ggplot(manif |> mutate(man = factor(man, labels = c("S√≠", "No"))),
       aes(x = edad, y = man)) +
  geom_point(aes(color = man), size = 3, alpha = 0.7) +
  theme_minimal() +
  labs(x = "Edad (a√±os)", y = "¬øAsisti√≥ a manifestaci√≥n?",
       color = "Asistencia",
       title = "¬øC√≥mo usar la regresi√≥n lineal para CLASIFICAR?",
       subtitle = "Reminder: la regresi√≥n lineal debe cumplir unas hip√≥tesis")
```


Dado que `man` es una **variable binaria** podemos pensarla de la misma manera que pensamos en el [**lanzamiento de una moneda**]{.hl-yellow}: tiene dos estados posibles (1 y 0), y cada uno con una [**probabilidad**]{.hl-yellow} asignada

---

## Regresi√≥n log√≠stica


Si lo equiparamos al lanzamiento de una moneda, podemos asumir que la variable objetivo `man` sigue una [**distribuci√≥n binomial (con n = 1) o de Bernoulli**]{.hl-yellow} con [**probabilidad de √©xito (asistir) $p$**]{.hl-yellow} (y $1-p$ de no asistir) tal que


$$Y | \left( \boldsymbol{X} = \left( x_1, \ldots, x_p \right) \right)  \sim Ber \left(p \right)$$

. . .

Como suced√≠a en el caso de la regresi√≥n lineal, nuestro objetivo por tanto ser√° [**estimar la esperanza de la variable objetivo**]{.hl-yellow} condicionada a la informaci√≥n de las predictoras

$$E \left[ Y | \left( \boldsymbol{X} = \left( x_1, \ldots, x_p \right) \right) \right] = p, \quad \widehat{E} \left[ Y | \left( \boldsymbol{X} = \left( x_1, \ldots, x_p \right) \right) \right] = \widehat{p}$$
Lo [**importante**]{.hl-yellow}: ¬øc√≥mo aprovechar lo que sabemos de modelizaci√≥n lineal para estimar $\widehat{p}$?



---

## Regresi√≥n log√≠stica


La idea m√°s inmediata es, precisamente, [**asignarle a esa esperanza estimada la misma estimaci√≥n de la regresi√≥n**]{.hl-yellow}, es decir, 

$$\widehat{E} \left[ Y | \left( \boldsymbol{X} = \left( x_1, \ldots, x_p \right) \right) \right] = \widehat{p}  = \widehat{\beta}_0 + \widehat{\beta}_1 X_1 + \ldots + \widehat{\beta}_p X_p$$

de manera que, si $G$ son las modalidades que puede adoptar la variable objetivo (en nuestro caso $G = \left\lbrace 0,1 \right\rbrace$)

$$\widehat{y_i} = j \quad \text{si} \quad  \widehat{P}(Y = j | X = \left(x_{1}, \ldots, x_{p} \right)) =  \arg \max_{g \in G} \widehat{P}(Y = g | X = \left(x_{1}, \ldots, x_{p} \right))$$

---

## Regresi√≥n log√≠stica

¬øQu√© pasar√≠a si aplic√°semos un modelo lineal?

```{r}
set.seed(12345)
library(rsample)
split <- initial_split(data, prop = 0.8, strata = man)
train <- training(split)
test <- testing(split)

ajuste_lineal <- lm(data, formula = man ~ .)
predict(ajuste_lineal, test)
```

. . .

Tenemos un [**problema**]{.hl-red}: si aplicamos una regresi√≥n lineal, no hay nada que nos garantice que la probabilidad predicha est√© entre 0 y 1. ¬°Puede salir [**incluso negativa**]{.hl-red}!

---

## Regresi√≥n log√≠stica

La idea ser√° [**garantizar que la salida de la regresi√≥n lineal**]{.hl-yellow} acabe [**dentro del rango [0,1]**]{.hl-green}, encapsulando esa salida  con una [**funci√≥n de enlace $g^{-1}:\mathbb{R} \to [0,1]$**]{.hl-yellow} tal que


$$P(Y = 1 | X = \left(x_{1}, \ldots, x_{p} \right)) =  p = g^{-1} \left(\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p\right) $$

$$g(p) = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$$

&nbsp;

¬øQu√© **condiciones** debe cumplir dicha funci√≥n? ¬øNos vale cualquiera?

---

## Regresi√≥n log√≠stica

Necesitamos unas [**m√≠nimas condiciones de regularidad**]{.hl-green}

* [**Invertible**]{.hl-yellow}: la funci√≥n  $g:~[0,1] \to \mathbb{R}$ debe ser invertible, tal que $g^{-1}:~\mathbb{R} \to [0,1]$

* [**Soporte en [0, 1]**]{.hl-yellow}: la funci√≥n $g:~[0,1] \to \mathbb{R}$ debe  estar definida para cualquier valor en $[0, 1]$

* [**Codiminio real**]{.hl-yellow}: la funci√≥n $g^{-1}:~\mathbb{R} \to [0,1]$ debe estar definida para todo valor real (para cualquier salida de una regresi√≥n)

* [**Mon√≥tona creciente**]{.hl-yellow}:  dado que $\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$ cuantifica el [**efecto de los predictores en la probabilidad de √©xito**]{.hl-yellow} de la variable objetivo, la probabilidad debe **aumentar seg√∫n crezca dicha cantidad**, as√≠ que $g^{-1}$ nunca debe decrecer

 
---

## Regresi√≥n log√≠stica

En funci√≥n de las distintas funciones de enlace podemos [**obtener distintos modelos**]{.hl-yellow} (siempre y cuando cumplan las 4 propiedades anteriores):

* [**Enlace uniforme (unit)**]{.hl-yellow}: funci√≥n $g^{‚àí1}(x) = x I_{0< x < 1} + I_{x \geq 1}$ (nos devuelve la salida de la regresi√≥n si est√° entre 0 y 1, y lo trunca a 0 en cualquier otro caso).

. . .

* [**Enlace probit**]{.hl-yellow}: funci√≥n $g^{‚àí1}(x) = \Phi (x)$ donde $\Phi(x)=\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{u^2}{2}} du$ es la **funci√≥n de distribuci√≥n acumulada de una normal **

. . .

* [**Enlace logit**]{.hl-yellow}: funci√≥n $g^{‚àí1}(x) = logistic(x):= \frac{e^{x}}{1 + e^{x}} = \frac{1}{1 + e^{-x}}$ basada en la distribuci√≥n acumulada de una distribuci√≥n log√≠stica.

&nbsp;

Esta [**√∫ltima es la abordaremos en este curso**]{.hl-yellow} por ser la m√°s com√∫n y f√°cil de interpretar

---

## Regresi√≥n log√≠stica

![](img/link-functions.jpg)

---

## Regresi√≥n log√≠stica

La funci√≥n de enlace m√°s usada es la [**funci√≥n logit**]{.hl-yellow}, que no es m√°s que la [**inversa de la funci√≥n $logistic$**]{.hl-yellow} que hemos definido arriba. 

&nbsp;

Para **ahorar notaci√≥n** llamaremos $\eta = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$ a la salida de la regresi√≥n y $\widehat{\eta} = \widehat{\beta}_0 + \widehat{\beta}_1 x_1 + \ldots + \widehat{\beta}_p x_p$  a su estimaci√≥n

$$g(p) = logit(p) = \ln \left( \frac{p}{1-p} \right) = \eta$$

$$p = g^{‚àí1}(\eta) = logistic(\eta):= \frac{1}{1 + e^{-\eta}}$$

---

## Regresi√≥n log√≠stica

$$g(p) = logit(p) = \ln \left( \frac{p}{1-p} \right) = \eta, \quad p = g^{‚àí1}(\eta) = logistic(\eta):= \frac{1}{1 + e^{-\eta}}$$

As√≠ la **estimaci√≥n** buscada ser√°

$$\widehat{p} = g^{‚àí1}(\widehat{\eta} ) = \frac{1}{1 + e^{-\widehat{\eta} }} = \frac{1}{1 + e^{-\left(  \widehat{\beta}_0 + \widehat{\beta}_1 x_1 + \ldots + \widehat{\beta}_p x_p\right) }}$$

¬øC√≥mo interpretar la salida de la regresi√≥n? ¬øQu√© suceder√° con $\widehat{p}$ cuando $\widehat{\eta} > 0$? ¬øY  $\widehat{\eta}< 0$?

---

## Regresi√≥n log√≠stica

$$\widehat{p} = g^{‚àí1}(\widehat{\eta} ) = \frac{1}{1 + e^{-\widehat{\eta} }} = \frac{1}{1 + e^{-\left(  \widehat{\beta}_0 + \widehat{\beta}_1 x_1 + \ldots + \widehat{\beta}_p x_p\right) }}$$

* Si $\widehat{\eta} = \widehat{\beta}_0 + \widehat{\beta}_1x_1 +\ldots + \widehat{\beta}_p x_p = 0$, $\widehat{p} = g^{‚àí1}(\widehat{\eta} ) = \frac{1}{1 + e^{-\widehat{\eta} }} = 0.5$ --> la [**probabilidad de √©xito es la misma que la de fracaso**]{.hl-yellow}

* Si $\widehat{\eta} = \widehat{\beta}_0 + \widehat{\beta}_1x_1 +\ldots + \widehat{\beta}_p x_p > 0$, $\widehat{p} = g^{‚àí1}(\widehat{\eta} ) = \frac{1}{1 + e^{-\widehat{\eta} }} > \frac{1}{1 + e^{0}} > 0.5$ --> la [**probabilidad de √©xito es mayor que la de fracaso**]{.hl-green} --> $\widehat{y} = 1$

* Si $\widehat{\eta} = \widehat{\beta}_0 + \widehat{\beta}_1x_1 +\ldots + \widehat{\beta}_p x_p < 0$, $\widehat{p} = g^{‚àí1}(\widehat{\eta} ) = \frac{1}{1 + e^{-\widehat{\eta} }} > \frac{1}{1 + e^{0}} < 0.5$ --> la [**probabilidad de √©xito es menor que la de fracaso**]{.hl-red} --> $\widehat{y} = 0$

---

## Cuotas u odds

$$g(p) = logit(p) = \ln \left( \frac{p}{1-p} \right) = \eta, \quad p = g^{‚àí1}(\eta) = logistic(\eta):= \frac{1}{1 + e^{-\eta}}$$

Como hemos visto antes, la inversa es lo que se conoce como [**funci√≥n log√≠stica**]{.hl-yellow} tal que $g(p) = logit(p) = \ln \left( \frac{p}{1-p} \right) = \eta$ es precisamente la salida de la regresi√≥n (lo que nos facilita su interpretaci√≥n)

. . .

De hecho al cociente entre $p$ y $1-p$ es lo que se conoce como [**odds o cuotas**]{.hl-yellow}: probabilidad de √©xito vs probabilidad de fracaso (como las cuotas de las casas de apuestas), cuantificando [**cu√°ntas veces es m√°s o menos probable el √©xito que el fracaso**]{.hl-yellow}



---

## Cuotas u odds

As√≠ podemos definir los [**log-odds**]{.hl-yellow} como el logaritmo de las cuotas

$$log-odds(Y)= \ln \left(\frac{p}{1-p} \right) =  \eta = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$$
$$odds (Y) =  \frac{p}{1-p}  = e^{\eta} = e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p} = e^{\beta_0} e^{\beta_1x_1} \ldots e^{\beta_px_p} $$

. . .

* $e^{\beta_0}$: estimaci√≥n (media) de odds cuando todas las predictoras son cero --> el [**ratio esperado p:1‚àíp (√©xito vs fracaso)**]{.hl-yellow} cuando las predictoras se anulan

* $e^{\beta_j}$ para $j\geq 1$: [**incremento MULTIPLICATVO**]{.hl-yellow} medio de odds para un [**incremento ADITIVO unitario de la predictora**]{.hl-yellow} $X_j$ (siempre y cuando el resto permanezcan fijas) --> por cada unidad que se incremente $X_j$, el ratio √©xito/fracaso se ver√° MULTIPLICADO por $e^{\widehat{\beta}_j}$ (si $\beta_j$ es positivo, aumenta; en caso contrario, disminuye).

---

## Ajuste en R

Vamos a realizar el ajuste en `R` a nuestro dataset haciendo uso de la funci√≥n `glm()`, indic√°ndole adem√°s de los datos y la variable objetivo, la **distribuci√≥n de la variable objetivo (binomial)** y la **funci√≥n de enlace** (logit)

```{r}
ajuste_logit <- glm(data = train, formula = man ~ ., family = binomial(link = "logit"))
ajuste_logit
```

El ajuste obtenido es $\widehat{\eta} = 2.592 - 0.089 X$, es decir,

$$\widehat{p} = \widehat{P} \left(Y = 1 | X = x\right) = \frac{1}{1 + e^{-\left(2.592 - 0.089 X \right)}}, \quad \widehat{odds} (Y) =  \frac{\widehat{p} }{1-\widehat{p} }  = e^{\widehat{\eta}} = e^{2.592} e^{- 0.089x}$$

---

## Ajuste en R

$$\widehat{p} = \widehat{P} \left(Y = 1 | X = x\right) = \frac{1}{1 + e^{-\left(2.592 - 0.089 X \right)}}, \quad \widehat{odds} (Y) =  \frac{\widehat{p} }{1-\widehat{p} }  = e^{\widehat{\eta}} = e^{2.592} e^{- 0.089x}$$

Si calculamos la exponencial de los coeficientes

```{r}
exp(coef(ajuste_logit))
```

* Es **13 veces m√°s probable que una persona de 0 a√±os asista a la manifestaci√≥n a que no lo haga** (estimaci√≥n poco √∫til ya que $X=0$ no est√° dentro del rango)

* Por **cada a√±o que cumpla la persona**, la [**probabilidad de asistir frente a no asistir**]{.hl-yellow} se MULTIPLICA por 0.9146, es decir, se reduce un 8.54%.

---

## Evaluaci√≥n: deviance


```{r}
ajuste_logit
```

En los modelos de clasificaci√≥n [**no podemos calcular m√©tricas como el error cuadr√°tico medio, var residual o $R^2$**]{.hl-red} ya que no tenemos una objetivo continua.


Como vemos en la salida se nos proporciona dos medidas: **Null Deviance** y **Residual Deviance**

---

## Evaluaci√≥n: deviance


```{r}
ajuste_logit
```

* [**Null Deviance**]{.hl-yellow}: al igual que en la bondad de ajuste de un modelo lineal se usa como referencia la SST, la [**anomal√≠a nula o null deviance**]{.hl-yellow} es la ¬´diferencia¬ª al comparar la log-verosimilitud del [**modelo perfectamente sobreajustado vs un modelo sin par√°metros (solo intercepto)**]{.hl-yellow}

$$D_0 = -2(\mathcal{L}_{\hat{\beta}_0} - \mathcal{L}_{saturado}) \geq 0, \quad \mathcal{L}_{modelo} = ln(P(observado | modelo))$$

---

## Evaluaci√≥n: deviance

```{r}
ajuste_logit
```

* [**Residual Deviance**]{.hl-yellow}: la [**anomal√≠a residual o anomal√≠a del modelo o residual deviance**]{.hl-yellow} es la ¬´diferencia¬ª al comparar la log-verosimilitud del [**modelo perfectamente sobreajustado vs nuestro modelo**]{.hl-yellow}

$$D = -2(\mathcal{L}_{\widehat{\beta}} - \mathcal{L}_{saturado}) \geq 0$$

---

## Evaluaci√≥n: deviance


Dado que $D_0$ mide la m√°xima distancia posible al modelo ¬´perfecto¬ª (sobreajustado) y $D$ mide la distancia de nuestro modelo, una forma de [**evaluar una regresi√≥n log√≠stica**]{.hl-yellow} es con el conocido como [**pseudo-$R^2$ o coeficiente de McFadden**]{.hl-yellow} (de todo lo mal que lo pod√≠a hacer, ¬øcu√°nto lo he hecho?)

$$pseudo-R^2 = \frac{D_0 - D}{D_0} = 1 - \frac{D}{D_0}$$

. . .


El coeficiente de McFadden podemos extraerlo con la funci√≥n `pR2()` del paquete `{pscl}`.

```{r}
library(pscl)
pR2(ajuste_logit)
1 - ajuste_logit$deviance/ajuste_logit$null.deviance
```


---


## Predicci√≥n log√≠stica

Una vez realizado el ajuste podemos usar como es habitual `predict(..., type = "response")` para obtener las predicciones de un conjunto de datos (en este caso lo vamos a hacer en train), teniendo claro que nos devuelve la [**probabilidad de ser 1**]{.hl-yellow}

```{r}
train |>
  mutate(prob_1 = predict(ajuste_logit, train, type = "response"),
         prob_0 = 1 - prob_1)
```

---

## Predicci√≥n log√≠stica

Con dichas probabilidades podemos construir un **dataset donde proporcionemos las cuotas, los log-odds y la clasificaci√≥n de la clase propiamente dicha** (fijando un umbral a partir del cual una observaci√≥n es clasificada como 1)

```{r}
umbral <- 0.5
pred_train <-
  train |> 
  mutate(prob_1 = predict(ajuste_logit, train, type = "response"), prob_0 = 1 - prob_1,
         odds = prob_1 / prob_0, log.odds = log(odds),
         pred_class = if_else(prob_1 > umbral, 1, 0))
pred_train
```

---

## Matriz de confusi√≥n

Una de las formas m√°s habituales de [**evaluar un modelo de clasificaci√≥n**]{.hl-yellow} es con la [**matriz de confusi√≥n**]{.hl-yellow}: ¬øcu√°ntos 1's reales son clasificados como 1? ¬øcu√°ntos 0's? ¬øcu√°ntos 1's reales son erroneamente clasificados como 0?

Para ello usamos `conf_mat()` del paquete `{yardstick}` (paquete de `{tidymodels}`), indic√°ndole la columna con la clase real (`truth = ...`) y la columna con la clase predicha (`estimate = ...`), ambas como factor.

```{r}
library(yardstick)
conf_mat_train <-
  pred_train |>
  mutate(man = as_factor(man), pred_class = as_factor(pred_class)) |> 
  conf_mat(truth = man, estimate = pred_class)
conf_mat_train
```


El **modelo ha acertado 59 de las 60 observaciones etiquetadas con 0** y ha **acertado solo 9 de las 19 observaciones etiquetadas con 1**


---

## M√©tricas de evaluaci√≥n

```{r}
conf_mat_train
```

Esa matriz de confusi√≥n puede ser analizada de diversas formas en funci√≥n de la **pregunta que queramos contestar**. 

&nbsp;

De ahora en adelante llamaremos [**verdadero positivo/negativo (VP/VN)**]{.hl-yellow} a las observaciones 1's/0's que han sido clasificadas correctamente como 1's/0's, respectivamente. En nuestro ejemplo tenemos 59 VN y 9 VP.

Llamaremos  [**falsos positivo/negativo (FP/FN)**]{.hl-yellow} a las observaciones 0's/1's que han sido erroneamente clasificadas como 1's/0's, respectivamente. En nuestro caso tenemos 10 FN y 1 FP.


---

## M√©tricas de evaluaci√≥n

```{r}
conf_mat_train
```

* ¬øQu√© [**proporci√≥n de observaciones han sido bien clasificadas**]{.hl-yellow}

&nbsp;

Dicha m√©trica se conoce como [**accuracy**]{.hl-yellow} (la suma de la diagonal de la matriz de confusi√≥n entre el total de observaciones, en nuestro caso un $0.86076$)

$$accuracy = \frac{VP + VN}{total}$$

---

## M√©tricas de evaluaci√≥n

```{r}
conf_mat_train
```

* ¬øQu√© [**proporci√≥n de positivos reales han sido bien clasificados**]{.hl-yellow}?

&nbsp;

Dicha m√©trica se conoce como [**sensibilidad (sensitivity)**]{.hl-yellow} y aproxima la probabilidad de que una observaci√≥n positiva sea clasificada como tal por el modelo (la probabilidad de que una PCR d√© positiva en una persona que tiene realmente el COVID), en nuestro caso un $0.47368$

$$sensitivity = \frac{VP}{VP + FN}$$

---

## M√©tricas de evaluaci√≥n

```{r}
conf_mat_train
```

* ¬øQu√© [**proporci√≥n de negativos reales han sido bien clasificados**]{.hl-yellow}?

&nbsp;

Dicha m√©trica se conoce como [**especificidad (specificity)**]{.hl-yellow} y aproxima la probabilidad de que una observaci√≥n negativa sea clasificada como tal por el modelo (la probabilidad de que una PCR d√© negativa en una persona que no tiene COVID), en nuestro caso un $0.98333$

$$specificity = \frac{VN}{VN + FP}$$

---

## M√©tricas de evaluaci√≥n

```{r}
conf_mat_train
```

* ¬øQu√© [**concordancia (no aleatoria) ah√≠ entre la realidad y la predicci√≥n**]{.hl-yellow}?

Dicha m√©trica se conoce como [**coeficiente kappa (de Cohen)**]{.hl-yellow} y aproxima la probabilidad de que la clase real y la clase predicha coincidan m√°s all√° de por motivos de azar (asumiendo independencia)

$$\kappa = \frac{P(a) - P(e)}{1-P(e)} = \frac{acc - P(e)}{1-P(e)}$$

donde $P(a)$ es la probabilidad emp√≠rica de acuerdo (es decir, el accuracy) y $P(e)$ la probabilidad emp√≠rica de acuerdo esp√∫reo (acuerdo por azar)

---


## M√©tricas de evaluaci√≥n

```{r}
conf_mat_train
```


$$\kappa = \frac{P(a) - P(e)}{1-P(e)} = \frac{acc - P(e)}{1-P(e)}$$

$$\begin{eqnarray}P(e) &=& P(real = 0, pred = 0) + P(real = 1, pred = 1) \nonumber \\ &=&  \frac{VN + FN}{total}* \frac{VN + FP}{total} + \frac{FP + VP}{total}* \frac{FN + VP}{total}\end{eqnarray}$$

En este caso $P(e) = 0.6937991$ y $\kappa = 0.54605$.

---

## M√©tricas de evaluaci√≥n

```{r}
conf_mat_train
```

* ¬øQu√© [**proporci√≥n de los clasificados como positivos son realmente verdaderos positivos**]{.hl-yellow}

&nbsp;

Dicha m√©trica se conoce como [**precision**]{.hl-yellow} y aproxima la probabilidad de que una observaci√≥n clasificada como positiva fuese realmente positiva (la probabilidad de que est√©s con covid si una PCR te da positiva), en nuestro caso un $0.9$

$$precision = \frac{VP}{VP + FP}$$

---

## M√©tricas de evaluaci√≥n

```{r}
conf_mat_train
```

* ¬øQu√© [**proporci√≥n de los clasificados como positivos son mal clasificados como negativos**]{.hl-yellow}

&nbsp;

Dicha m√©trica se conoce como [**tasa de descubrimiento falso o false discovery rate (FDR)**]{.hl-yellow} (tasa de error de tipo I) y aproxima la probabilidad de que una observaci√≥n clasificada como positiva no fuese realmente positiva (la probabilidad de que est√©s sano a pesar de que una PCR te da positiva), en nuestro caso un $0.1$

$$FDR = \frac{FP}{VP + FP} = 1 - precision$$

---


## M√©tricas de evaluaci√≥n

Las m√©tricas mencionadas, am√©n de otras muchas, pueden ser obtenidas haciendo `summary()` de la matriz de confusi√≥n (indicando que el evento de inter√©s es el segundo nivel del factor, los 1's)

```{r}
conf_mat_train |> summary(event_level = "second")
```

---


## M√©tricas de evaluaci√≥n


```{r}
conf_mat_train |>
  summary(event_level = "second") |> 
  slice(1:4)
```

Si nos fijamos en las primeras

* a priori parece que el [**modelo funciona bastante bien**]{.hl-yellow} ya que acierta el 86% de las veces

. . .

* de hecho la [**probabilidad de acierto en el caso de personas que no asisten**]{.hl-green} a la manifestaci√≥n (0's) es del 98.3% pero ...

. . .

* la [**probabilidad de acierto en el caso de los asistentes**]{.hl-red} (1's) es solo del 47.4%

---

## M√©tricas de evaluaci√≥n


```{r}
#| code-fold: true
conf_mat_train |>
  summary(event_level = "second") |> 
  slice(1:4)
```

Si pensamos nuestro modelo como una PCR

* la prueba [**acierta el 86% de las veces**]{.hl-yellow} 

* de hecho la [**probabilidad de que la PCR d√© negativa estando sano**]{.hl-green} es del 98.3% (no hay apenas falsos positivos) pero ...

* la [**probabilidad de que la PCR d√© positiva estando enfermo**]{.hl-red} (1's) es solo del 47.4% (hay much√≠simos falsos negativos)


. . .

Si el objetivo fuese, por ejemplo [**evitar contagios...no parece muy buen m√©todo**]{.hl-red}

---

## M√©tricas de evaluaci√≥n



Esto puede suceder por muchos motivos pero el principal es que, en nuestro caso, la [**muestra est√° desbalanceada**]{.hl-yellow}: el modelo ha podido aprender muy bien de los 0's pero mucho menos de los 1's (ya que solo hay un 25%).

```{r}
train |> count(man)
```

Por eso es **importante no quedarse solo con el accuracy** en la evaluaci√≥n del modelo. De hecho, seg√∫n $\kappa$, la probabilidad de concordancia entre realidad y predicci√≥n, excluyendo el azar, es solo de $0.545$.


---

## Curva ROC

Todas las [**m√©tricas anteriores se basan en una decisi√≥n**]{.hl-yellow}: todo depende del [**umbral (threshold)**]{.hl-yellow} que hayamos usado para decidir que una observaci√≥n es clasificada como 1 o como 0 (ya que, recuerda, el ajuste nos devuelve solo la probabilidad estimada de serlo $\widehat{p}$)

. . .

¬øQu√© pasar√≠a si ese [**umbral fuese variando**]{.hl-yellow}, desde 0 hasta 1? ¬øC√≥mo de bueno ser√≠a nuestro m√©todo si movemos ese umbral?

. . .


* Si el [**umbral tiende a cero**]{.hl-yellow} --> casi todas las observaciones ser√°n predichas como 1's --> no habr√° falsos negativos (todo positivo ser√° clasificado como positivo) pero ning√∫n negativo real ser√° clasificado como tal --> [**sensibilidad tiende a 1, especificidad tiende a 0**]{.hl-yellow}

* Si el [**umbral tiende a 1**]{.hl-yellow} --> casi todas las observaciones ser√°n predichas como 0's --> no habr√° falsos positivos pero ning√∫n positivo real ser√° clasificado como tal --> [**sensibilidad tiende a 0, especificidad tiende a 1**]{.hl-yellow}


---

## Curva ROC

Podemos **calcular cada valor de sensibilidad y especificidad** con `roc_curve()` indicando la clase real a predecir (como factor), indicando el evento de inter√©s (los 1's) y, en este caso, **necesitamos proporcionar la probabilidad estimada de ser 1** (en lugar de la clase predicha en s√≠ como antes)

Lo que nos devuelve es la [**sensibilidad y especificidad para cada umbral**]{.hl-yellow}

```{r}
roc_data <- 
  pred_train |>
  mutate(man = as_factor(man)) |> 
  roc_curve(man, prob_1, event_level = "second")
roc_data
```

---

## Curva ROC

Podemos visualizar dicho dataset a√±adiendo `autoplot()` al dataset, lo que nos proporciona la conocida como [**curva ROC**]{.hl-yellow}: una curva que nos permite visualizar de manera global la [**calidad de nuestro clasificador para cada posible decisi√≥n**]{.hl-yellow}

```{r}
roc_data |> autoplot()
```

---

## Curva ROC


```{r}
#| echo: false
roc_data |> autoplot()
```


F√≠jate que en el **eje X se visualiza 1 - especificidad**, es decir, la probabilidad emp√≠rica de que el m√©todo clasifique mal los 0's: la [**curva ROC visualiza la probabilidad de falso positivo vs verdadero positivo**]{.hl-yellow}

Cuando el **umbral es muy bajo**, dado que todo ser√° clasificado como positivo, clasifica perfecto los verdaderos positivos (sensibilidad  = 1) pero todos los negativos son falsos positivos (1-especificidad = 1). Y al contrario.

---

## Curva ROC

```{r}
#| echo: false
roc_data |> autoplot()
```


Lo importante de esta curva es lo que se conoce como [**AUC o √°rea debajo de la curva**]{.hl-yellow}, que podemos obtener con `roc_auc()`: buscamos un clasificador cuya √°rea sea lo m√°s pr√≥xima a 1.

```{r}
pred_train |>
  mutate(man = as_factor(man)) |> 
  roc_auc(man, prob_1, event_level = "second")
```

---


## Curva ROC

![](img/roc-auc.jpg)

---

## Sobre/bajomuestreo

Como hemos visto, tener la [**muestra desbalanceada**]{.hl-yellow} puede ser un problema. Para resolverlo, podemos (antes de realizar el ajuste) lo que se conoce como [**sobre/bajo muestreo**]{.hl-yellow}

. . .

La √∫nica informaci√≥n que vamos a usar para ello (para no hacer ¬´trampas¬ª) es el **conjunto de entrenamiento**

```{r}
train |>
  count(man) |> 
  mutate(porc = 100*n/sum(n))
```

En √©l observamos que contamos con un **75.9% de 0's y 19% de 1's**. ¬øC√≥mo [**equilibrar los porcentajes**]{.hl-yellow}? Parece obvio que hay dos formas: subir el n√∫mero de 1's o bajar el n√∫mero de 0's.

---

## Sobre/bajomuestreo

Las dos ideas buscan [**equilibrar dichos porcentajes**]{.hl-yellow}

* [**Bajomuestreo**]{.hl-yellow}: dejamos [**fija la cantidad de la clase minoritaria**]{.hl-yellow} (los 1's en este caso) y [**reducimos la cantidad de la clase mayoritaria**]{.hl-yellow} (los 0's) para que haya algo parecido a un equilibrio.

. . .

Para ello **filtramos los individuos de la clase mayoritaria**, **seleccionamos un % aleatorio** de ellas (en este caso un 33% ya que basta con tener 20 de las 60 para ese equilibrio) y **unimos la clase minoritaria**.

```{r}
set.seed(12345)
downsampling_data <-
  train |> filter(man == 0) |> 
  slice_sample(prop = 0.33, replace = FALSE) |> 
  bind_rows(train |> filter(man == 1))

downsampling_data |>
  count(man) |> mutate(porc = 100*n/sum(n))
```


---

## Sobre/bajomuestreo

* [**Sobremuestreo**]{.hl-yellow}: dejamos [**fija la cantidad de la clase mayoritaria**]{.hl-yellow} (los 0's en este caso) y [**aumentamos la cantidad de la clase minoritaria**]{.hl-yellow} (los 1's) para que haya algo parecido a un equilibrio.

. . .

Para ello **filtramos los individuos de la clase minoritaria**, **seleccionamos un % aleatorio** de ellas (en este caso un 300% ya pasar de 20 a 60, x3, para ese equilibrio) y **unimos la clase mayoritaria**.

```{r}
set.seed(12345)
oversampling_data <-
  train |> filter(man == 1) |> 
  slice_sample(prop = 3, replace = TRUE) |> 
  bind_rows(train |> filter(man == 0))

oversampling_data|>
  count(man) |>  mutate(porc = 100*n/sum(n))
```

---

## Sobre/bajomuestreo

:::: columns
::: {.column width="50%"}

```{r}
set.seed(12345)
downsampling_data <-
  train |> filter(man == 0) |> 
  slice_sample(prop = 0.33, replace = FALSE) |> 
  bind_rows(train |> filter(man == 1))
```

:::

::: {.column width="50%"}

```{r}
set.seed(12345)
oversampling_data <-
  train |> filter(man == 1) |> 
  slice_sample(prop = 3, replace = TRUE) |> 
  bind_rows(train |> filter(man == 0))
```

:::
::::

* [**Bajomuestreo**]{.hl-yellow}: la principal [**ventaja**]{.hl-green} es que los nuevos datos son una submuestra real de los datos originales; la [**desventaja**]{.hl-red} es que se reduce considerablemente el tama√±o muestral.

* [**Sobremuestreo**]{.hl-yellow}: la principal [**ventaja**]{.hl-green} es que no reducimos el tama√±o muestral; la [**desventaja**]{.hl-red} es que los datos nuevos son generados artificialmente (en este caso, son observaciones repetidas, de ah√≠ el `replace = TRUE`)

Existen otras t√©cnicas (SMOTE, ROSE, etc) que lo que hacen es [**simular datos sint√©ticos**]{.hl-yellow}, bien usando la distribuci√≥n de cada variable o bien tomando los datos originales y simulando nuevos perturbando con ruido.

---

## Ajuste con bajomuestreo

```{r}
#| code-fold: true
ajuste_logit_down <-
  glm(data = downsampling_data, formula = man ~ ., family = binomial(link = "logit"))
pred_train_down <-
  downsampling_data |> 
  mutate(prob_1 = predict(ajuste_logit_down, downsampling_data, type = "response"),
         prob_0 = 1 - prob_1, odds = prob_1 / prob_0, log.odds = log(odds),
         pred_class = if_else(prob_1 > 0.5, 1, 0))

conf_mat_train_down <-
  pred_train_down |>
  mutate(man = as_factor(man), pred_class = as_factor(pred_class)) |> 
  conf_mat(truth = man, estimate = pred_class)

pred_train_down |>
  mutate(man = as_factor(man)) |> roc_auc(man, prob_1, event_level = "second")
conf_mat_train_down |> summary(event_level = "second")
```

[**Empeora el accuracy y un poco la especificidad**]{.hl-red} pero [**mejora la sensibilidad**]{.hl-green}

---

## Ajuste con sobremuestreo

```{r}
#| code-fold: true
ajuste_logit_over <- 
  glm(data = oversampling_data, formula = man ~ ., family = binomial(link = "logit"))
pred_train_over <-
  oversampling_data |> 
  mutate(prob_1 = predict(ajuste_logit_over, oversampling_data, type = "response"),
         prob_0 = 1 - prob_1, odds = prob_1 / prob_0, log.odds = log(odds),
         pred_class = if_else(prob_1 > 0.5, 1, 0))

conf_mat_train_over <-
  pred_train_over |>
  mutate(man = as_factor(man), pred_class = as_factor(pred_class)) |> 
  conf_mat(truth = man, estimate = pred_class)

pred_train_over |>
  mutate(man = as_factor(man)) |> roc_auc(man, prob_1, event_level = "second")
conf_mat_train_over |> summary(event_level = "second")
```

[**Empeora el accuracy y mucho la especificidad**]{.hl-red} pero [**mejora la sensibilidad**]{.hl-green}


---

## Caso real: challenger

Pr√°ctica con el dataset `challenger.txt` subido al campus. El objetivo ser√° obtener la probabilidad de accidente en distintos lanzamientos espaciales, donde `fail.field` es **nuestra variable objetivo (si hubo o no accidente debido a un fallo en las juntas)** en funci√≥n de si hubo o no fallo en los inyectores, la temperatura exterior, la presi√≥n de las juntas y la presi√≥n de los inyectores

```{r}
datos <-
  read_delim(file = "./datos/challenger.txt")
datos <- 
  datos |> select(-contains("nfails"))
datos
```

---

## Pues...ha sido un placer

![](img/me.jpeg)

[**Espero que hay√°is aprendido algo <3**]{style="color:#444442;"}


