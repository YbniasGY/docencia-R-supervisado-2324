---
title: "Aprendizaje Supervisado I"
subtitle: "Métodos de predicción lineal"
title-slide-attributes:
  data-background-image: img/data-science-2.jpeg
  data-background-size: cover
  data-background-opacity: "0.2"
author: "Grado en Ciencia de Datos Aplicada • curso 2023-2024"
affiliation: Facultad de Estudios Estadísticos (UCM)
lang: es
language: custom_lang.yml
format: 
  revealjs:
    theme: [default, style.scss]
    chalkboard: true
    multiplex: true
    menu:
      side: left
      width: normal
    footer: "[<strong>Javier Álvarez Liébana</strong>](...) • Grado en Ciencia de Datos Aplicada (UCM) • curso 2023-2024"
    slide-number: c/t
execute:
  echo: true
---

# Empieza lo bueno...empieza la modelización

[**Vamos a juntar las piezas del puzzle para hacer «magia»**]{style="color:#444442;"}

```{r}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
```

---

## ¡Buenas!

[**Correo**]{.hl-green}: **<javalv09@ucm.es>**. [**Despacho**]{.hl-green}: 722 (3ª planta). [**Tutorías**]{.hl-green}: lunes (14:30-16:00), martes (13:00-15:00) y viernes (13:00-14:00).

::: columns
::: {.column width="30%"}
![](img/me.jpeg)
:::

::: {.column width="70%"}
::: incremental
-   [**Javier Álvarez Liébana**]{.hl-yellow}, de Carabanchel (Bajo).

-   Licenciado en Matemáticas (UCM). [**Doctorado en estadística**]{.hl-yellow} (UGR).

-   Encargado de la [**visualización y análisis de datos covid**]{.hl-yellow} del Principado de Asturias (2021-2022).

-   Miembro de la [**Sociedad Española de Estadística e IO**]{.hl-yellow} y la [**Real Sociedad Matemática Española**]{.hl-yellow}.
:::
:::
:::

. . .

Actualmente, [**investigador y docente en la Facultad de Estadística de la UCM**]{.hl-yellow}. Divulgando por [**Twitter**](https://twitter.com/dadosdelaplace) e [**Instagram**](https://instagram.com/javieralvarezliebana)


---

## Objetivos

::: columns
::: {.column width="37%"}
![](https://assets-global.website-files.com/6092cb6b4ac959f39728dd26/6188a97fa499b5fbfe410417_target%20(1).png)
:::

::: {.column width="63%"}
::: incremental
- Empezar a [**relacionar asignaturas**]{.hl-yellow} como matemáticas, inferencia y R.

- Aprender los fundamentos del [**aprendizaje estadístico**]{.hl-yellow} (ahora llamado Machine Learning o data science)

- Pasar de los descriptivo a lo predictivo: [**construir nuestros primeros modelos**]{.hl-yellow}

- Entender en profundidad el contexto de la [**predicción lineal**]{.hl-yellow}.


:::
:::
:::

---

## Evaluación

-   [**Asistencia**]{.hl-yellow}. Se [**valorará muy positivamente**]{.hl-purple} la participación. Si se [**restarán puntos si eres expulsado de clase**]{.hl-purple}: -0.3 la 1ª vez, -0.6 la 2ª, -1.2 la 3ª...

. . .

- [**Evaluación continua**]{.hl-yellow}: 1 **examen a papel** (25%), **2 entregas R en clase** (20%-30%) y **entrega grupal último día** (3-6p obligatoriamente, 25%)
  
. . .

- [**Examen final**]{.hl-yellow}:
  - [**Menos de un 6 (sin la grupal)**]{.hl-purple} -> examen de R en lugar de la grupal.
  - [**Más de un 6.5 (con la grupal)**]{.hl-purple} -> podrás decidir **peso del final entre un 20% y un 100%**.
  - [**Entre 5 y 6.5 (con la grupal)**]{.hl-purple} -> podrás decidir **peso del final entre un 40% y un 100%**.
  - [**Por debajo de 5 (con la grupal)**]{.hl-purple} -> podrás decidir **peso del final entre un 70% y un 100%**.
  

---

## Planificación


* [**Entrega I (20%)**]{.hl-yellow}: semana 27-29 de febrero de 2024

* [**Entrega II (30%)**]{.hl-yellow}: semana 2-4 de abril de 2024

* [**Parcial con papel y boli (25%)**]{.hl-yellow}: 16-18 de abril de 2024 

* [**Entrega grupal (25%)**]{.hl-yellow}: 9 de mayo de 2024 (incluyendo lo visto hasta el 30 de abril)

&nbsp;

(se admiten propuestas)

. . . 

[**Clase a recuperar otro día**]{.hl-yellow}: clase del 21 de marzo.

---

## Materiales

* [**Diapositivas**]{.hl-yellow}: las diapositivas que usaremos en el aula a lo largo del curso, estructuradas por clases, estarán disponibles y actualizadas en **<https://javieralvarezliebana.es/docencia-R-supervisado-2324/diapos>** 

En el menú de las diapositivas (abajo a la izquierda) tienes una [**opción para descargarlas en pdf**]{.hl-yellow} en `Tools` (consejo: no lo hagas hasta el final del curso ya que irán modificándose)
  
&nbsp;

* [**Material**]{.hl-yellow}: [**scripts de cada tema**](https://github.com/dadosdelaplace/docencia-R-supervisado-2324/tree/main/material) y materiales extras

* [**Resúmenes de paquetes**]{.hl-yellow}: [**chuletas de los paquetes**](https://github.com/dadosdelaplace/docencia-R-supervisado-2324/tree/main/fichas%20paquetes) en formato .pdf

---


## Requisitos

Para el curso los únicos requisitos serán:

1.  [**Conexión a internet**]{.hl-yellow} (para la descarga de algunos datos y paquetes).

2.  [**Instalar R y RStudio**]{.hl-yellow}: la descarga la haremos (gratuitamente) desde <https://cran.r-project.org/> y  <https://posit.co/download/rstudio-desktop/>

3. Se darán por asumido conocimientos aprendidos de [**R base, tidyverse y ggplot**]{.hl-yellow}

4. Se darán por asumido conocimientos aprendidos de [**Quarto, diapositivas en Quarto y Github**]{.hl-yellow}. Para las entregas [**SOLO SE VALORARÁ**]{.hl-purple} la salida html correspodiente.

5. [**Recomendable**]{.hl-yellow}: saber usar la calculadora en modo estadístico.


---

## Listado de clases

* [**Clase 1**](#clase-1): repaso de descriptiva
* [**Clase 2**](#clase-2): importancia de la visualización
* [**Clase 3**](#clase-3): intro al aprendizaje estadístico. Sesgo vs varianza. Supervisado vs no supervisado. Correlación vs dependencia vs causalidad.


# Clase 1: repaso {#clase-1}

[**Objetivo de la predicción lineal. Concepto de linealidad. Repaso de estadística descriptiva**]{style="color:#444442;"}

---

## ¿Qué es predecir?

Como veremos más adelante, en esta asignatura vamos a tratar principalmente lo que se conoce el aprendizaje estadístico como [**predicción (continua)**]{.hl-yellow}

. . .

Dada una [**variable objetivo (variable dependiente)**]{.hl-yellow}, y con la información aportada por un conjunto de [**variables predictoras (covariables)**]{.hl-yellow}, el objetivo será construir un modelo que consiga dar una estimación/predicción lo «mejor posible»

. . .

Es importante que - de momento - distingamos dos conceptos:

* [**Estimación**]{.hl-yellow}: el modelo aprende de unos datos e intenta estimar dichos valores que ha usado.
* [**Predicción**]{.hl-yellow}: el modelo aprende de unos datos e intenta estimar valores que el **modelo no conoce**.

Más adelante los llamaremos «predicción en train» y «predicción en test»

---

## ¿Qué es la linealidad?

En matemáticas decimos que una función $f(x)$ es [**lineal**]{.hl-yellow} cuando se cumple:

* [**Propiedad aditiva**]{.hl-purple}: $f(x + y) = f(x) + f(y)$

* [**Propiedad homogénea**]{.hl-purple}: $f(k*x) = k*f(x)$ (donde $k$ es una constante en $\mathbb{R}$).

Ambas se pueden resumir en $f(a*x + b*y) = a*f(x) + b*f(y)$

. . .

En estadística llamamos [**modelo de predicción lineal**]{.hl-yellow} a un modelo que usa la información de covariables $X_1, X_2, \ldots, X_p$, de manera que su información siempre [**se relacionen entre sí con sumas y restas**]{.hl-yellow}.

- [**Ejemplos lineales**]{.hl-green}: $y = 2*x_1 - 3$ o  $y = 4 - \frac{x_1}{2} + 3*x_2$

- [**Ejemplos no lineales**]{.hl-red}: $y = 2*\frac{1}{x_1}$ o  $y = 4 - x_{1}^{2} - x_2$ o $y = ln(x_1) + cos(x_2)$

En general son modelos que se pueden representar con rectas (una sola predictora), planos (dos predictoras), etc.

---

## Repaso: continua vs discreta

* **Caracteres**: cada una de las características o cualidades que se podrían medir o analizar para cada individuo de la población (y de los que disponemos el valor para cada individuo de la muestra).

* **Modalidades**: diferentes valores que puede adoptar una característica o variable.

&nbsp;

Como veremos más adelante, en el ámbito del aprendizaje estadístico va a ser fundamental tener clara la diferencia entre una [**variable cualitativa y cuantitativa**]{.hl-purple} y entre [**variable continua y otra discreta**]{.hl-yellow}. ¿Cuál es la diferencia?


---

## Repaso: continua vs discreta

Imagina las siguientes variables:

- ¿Tienes hermanos? (si/no)
- Color de zapatillas
- Nivel de estudios
- Número de hermanos
- Número de pelos en la cabeza
- Resultado de un dado
- Temperatura ºC
- Estatura o peso

&nbsp;

[**¿Cuál es la diferencia entre ellas**]{.hl-yellow}


---

## Repaso: continua vs discreta



* [**Cualitativas**]{.hl-yellow}: **cualidades o categorías**. Ejemplos: sexo, estado civil, estudios, etc.
  * [**Ordinales**]{.hl-purple}: admiten relación jerarquica (suspenso-aprobado-bien, sano-leve-grave).
  * [**Nominales**]{.hl-purple}: no tienen asociada una jerarquía (sexo, religión, color de zapatillas, etc).

. . .

* [**Cuantitativas**]{.hl-yellow}: cuantificación numérica.
  * [**Discretas**]{.hl-purple} (barras): se pueden contar y enumerar (aunque sean infinitos) (granos de arena, nº hermanos, etc) → se **suman**
  * [**Continuas**]{.hl-purple}: toman infinitos valores y entre dos valores cualesquiera hay a su vez infinitas opciones (estatura, peso, etc) → se **integran**

---

## Repaso: continua vs discreta

![](img/vitro-fuego-discretas.jpg)


---

## Repaso: población vs muestra

En estadística llamaremos [**población**]{.hl-yellow} al universo teórico, al colectivo de individuos a estudiar, o de posibles elementos/eventos de los podríamos tener observaciones (ejemplo: 47 millones de españoles).

. . .

Cada uno de esos elementos o eventos se llaman **individuos**.

. . .

Aunque nuestro objetivo será conocer algunas de las propiedades de la población, la [**población suele ser inaccesible**]{.hl-red} en su totalidad → [**SELECCIÓN**]{.hl-green} de un conjunto de individuos

---

## Repaso: población vs muestra

Para ello en estadística usamos lo que se conoce como [**muestra**]{.hl-yellow}: un subconjunto finito, de tamaño $n$, «representativo» de la población (en estudio estadístico realizado sobre la totalidad de una población se denomina **censo**).

. . .

Para ello existe una rama conocida como [**muestreo**]{.hl-yellow}

* [**Probabilístico**]{.hl-purple}: todos los individuos tienen oportunidad de ser seleccionados, y la probabilidad de que suceda puede ser modelizada (al azar, por estratos, etc).

* [**No probabilístico**]{.hl-purple}: algunos elementos de la población no tienen posibilidad de selección (sesgo de exclusión), o su probabilidad no puede ser conocida.

. . .

🤔 ¿Sería adecuado hacer una encuesta sobre el streamer favorito de los jóvenes a través de una encuesta realizada por teléfono fijo?


---

## Sesgos en el muestreo

![](img/sampling-bias.jpg)

. . .

[**Sesgo de selección**]{.hl-yellow}: aparece cuando no se tiene en cuenta la forma en la que se han recogido los datos.

--- 


## Sesgos en el muestreo


![](img/dewey.jpg)

El ejemplo más famoso es el caso [**«Dewey defeats Truman» (Dewer derrota a Truman)**]{.hl-yellow}, el titular con el que abrió el Chicago Tribune en 1948, el mismo día en el que Truman ganó al repúblicano Dewer en las elecciones de 1948: sin esperar a los resultados, se basaron en una encuesta telefónica (sin contar con el sesgo que, en aquella época, solo la clase alta tenía teléfono).

---



## Sesgos en el muestreo

![](img/superviviente.jpg)

¿Dónde reforzarías los aviones?

---

## Sesgos en el muestreo

![](img/superviviente.jpg)



El [**sesgo del superviviente**]{.hl-yellow} (un tipo de sesgo de selección) aparece cuando se toma una muestra de un fenómeno ignorando si los individuos elegidos tienen las mismas opciones respecto al mismo.


---

## Repaso: medidas de centralización

* [**Media**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la media muestral $\overline{x}$ se define como la **suma de todos los valores dividida por el tamaño muestral**

$$\overline{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$

. . .

[**Geométricamente**]{.hl-purple}: es el **valor «más cercano» de todos los datos a la vez** (minimiza las distancias al cuadrado)

---

## Media muestral


[**VENTAJAS**]{.hl-green}

* Fácil de calcular y entender
* Fácil y eficiente de programar
* Siempre existe (para cuantitativas)

. . .

[**DESVENTAJAS**]{.hl-red}

* No es un valor de los datos (la media de {1, 2, 3, 4} es 2.5)
* **Poco robusta** (valores atípicos le afectan mucho)
* Solo se puede definir para variables cuantitativas


---

## Repaso: medidas de centralización

* [**Mediana**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la mediana muestral se define como el **valor que es mayor o igual que al menos el 50%**, y menor igual que al menos el 50% de los datos

$$Me_{x} = \arg \min_{x_i} \left\lbrace F_i > 0.5 \right\rbrace, \quad Me_x = e_{i-1} + \frac{0.5 - F_{i-1}}{F_i - F_{i-1} }a_i$$

La mediana es el [**valor de en medio**]{.hl-purple} si ordenamos los datos (y si se pueden ordenar...)

---

## Mediana muestral


[**VENTAJAS**]{.hl-green}

* Suele ser un valor de la muestra
* Un poco más robusta que la media


. . .

[**DESVENTAJAS**]{.hl-red}

* Muy ineficiente (requiere un algoritmo de ordenación)
* Solo definida para  cuantitativas o cualitativas ordinales

---


## Repaso: medidas de centralización

* [**Moda**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la moda muestral se define como el **valor o valores más repetidos** (en caso de que existan).

$$Mo_x = \arg \max_{x_i} f_i, \quad Mo_x = e_{i-1} + \frac{d_i - d_{i-1}}{\left(d_i - d_{i-1} \right) + \left(d_i - d_{i+1} \right)}a_i$$


. . .

[**Gráficamente**]{.hl-purple}: representa el «pico» de un diagrama de barras o un histograma

---

## Moda muestral


[**VENTAJAS**]{.hl-green}

* Es un valor de la muestra
* Muy robusta
* **Se puede calcular para cualquier** cuanti o cuali



. . .

[**DESVENTAJAS**]{.hl-red}

* No siempre existe (amodal) y pueden existir varias (bimodal, trimodal, etc)
* Poco usada en inferencia


---

## Repaso: medidas de centralización

![](img/ine-salarios-oculto.jpg)

**¿Cuál es la mediana, la media y la moda?**

---

## Repaso: medidas de centralización

![](img/ine-salarios.jpg)

---

## Repaso: medidas de dispersión

![](img/iker-jimenez.jpg)

¿Qué tiene que ver la imagen con la dispersión?


---

## Repaso: medidas de dispersión

![](img/extremos.jpg)

El cambio climático no solo es porque aumente la [**temperatura media (centralización)**]{.hl-yellow} sino por la aparición cada vez más frecuente de fenómenos extremos 


Aumento de la [**variabilidad**]{.hl-yellow} → aumento de la [**DISPERSIÓN**]{.hl-yellow}

---

## Repaso: medidas de dispersión

[**¿Cómo medir lo que se alejan los datos de la media?**]{.hl-yellow}

. . .

![](img/primera-idea-varianza.jpg)

Una **primera idea** podría ser [**medir la distancia de cada dato al centro**]{.hl-yellow}, es decir, restar cada dato de la media, y después realizar su promedio.

---

![](img/wait-for-it.jpg)

---

## Repaso: medidas de dispersión

Imagina que tenemos la siguiente muestra $X = \left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$.

**¿Cuánto vale la media?**

. . .

La media vale 0 y la distancia a ella es...la propia muestra $\left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$. **¿Cuál es el promedio de dichas distancias?**

. . .

Pues...de nuevo vale 0.

. . .

Si la dispersión es 0...[**¿no hay dispersión?**]{.hl-red} ¿No debería de dar 0 solo cuando los datos sean constantes?

---

## Repaso: medidas de dispersión

Para **evitar que se cancelen** los signos lo que haremos será calcular el [**promedio PERO de las distancias al cuadrado**]{.hl-yellow}, la conocida como [**varianza**]{.hl-yellow}

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 $$

. . .


::: callout-warning
# Cuidado

Tomar el valor absoluto (para evitar que se cancelen los signos) suele ser una mala idea en matemáticas (no es derivable como función).
:::

---

## Repaso: medidas de dispersión


[**Problema**]{.hl-red}: si los datos están en metros, la varianza estará en...metros cuadrados


. . .

:::: columns
::: {.column width="50%"}

¿Tiene sentido medir la dispersión de nuestra estatura en baldosas?

:::

::: {.column width="50%"}

![](img/albert-rivera.jpg)
:::

::::

---

## Repaso: medidas de dispersión

Para tener una [**medida de dispersión en las unidades de los datos**]{.hl-yellow} calcularemos la [**desviación típica**]{.hl-yellow}, como la raíz cuadrada de la varianza

$$s_{x} = \sqrt{s_{x}^{2}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2} = \sqrt{\overline{x^2} - \overline{x}^2}$$


---

## Repaso: medidas de dispersión

Todavía tenemos un pequeño problema.

Imagina que queremos **comparar la dispersión de dos conjuntos** de datos, estaturas de personas y diámetros de núcleos de células. Y Supongamos que las medias son 170 cm y 5 micrómetros, y la desviación típica de 1 cm y 1.5 micrómetros.

[**¿Qué conjunto de datos es más disperso?**]{.hl-yellow}

. . .

Para tener una  **medida de dispersión adimensional** definiremos el [**coeficiente de variación**]{.hl-yellow} 

$$CV_{x} = \frac{s_{x}}{\left| \overline{x} \right|}$$

---

## Repaso: medidas de localización

Las [**medidas de posición o localización**]{.hl-yellow} nos localizan los datos, siendo valores que **nos dividen un conjunto ordenado** en subconjuntos del mismo tamaño (ejemplo: mediana es percentil 50).

* **Percentil**: valores $P_{\alpha}$ del conjunto ordenado que dejan por debajo, al menos, el $\alpha$% de datos y $\alpha$% por encima.       
* **Decil**: valores $D_{\alpha}$ que dividen los datos en 10 partes iguales.

* **Cuartil**: valores $C_{\alpha}$ o $q_{\alpha}$ que dividen los datos en 4 partes iguales.

---

## Repaso: covarianza y correlación

[**¿Qué es en realidad la varianza?**]{.hl-yellow}

. . .

La [**varianza es el promedio de las desviaciones al cuadrado**]{.hl-yellow} (respecto a la media), apareciendo dos veces dicha desviación: puede ser entendida como una [**medida que cuantifica la relación de una variable CONSIGO MISMA**]{.hl-yellow}

. . .

[**¿Y si quiésemos medir la relación de una variable X respecto a otra variable Y (en lugar de consigo misma)?**]{.hl-purple}


---

## Repaso: covarianza y correlación

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 \quad \text{(varianza)}$$


La idea detrás de la [**covarianza**]{.hl-yellow} es justo esa: sustituir una de esas desviaciones de la X por la desviación de la Y.


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

---

## Repaso: covarianza y correlación

Es importante entender algunas [**propiedades de la covarianza**]{.hl-yellow}

. . .

* [**Signo**]{.hl-purple}: la covarianza puede ser tanto positiva como negativa como 0: al eliminar el cuadrado de la varianza, ya no es necesario que sea positiva


. . .

* [**¿Qué cuantifica?**]{.hl-purple} La covarianza mide la [**relación LINEAL**]{.hl-red} (en torno a una recta) entre dos variables

. . .

* [**¿Qué dice su signo?**]{.hl-purple} El signo de la covarianza nos indicará la [**dirección de la dependencia lineal**]{.hl-yellow}: si es positiva, la relación será creciente (cuando X crece, Y crece); si es negativa, la relación será decreciente (cuando X crece, Y decrece)


---

## Repaso: covarianza y correlación

Al igual que pasaba con la varianza, la [**covarianza depende de las unidades y magnitudes**]{.hl-yellow} de los datos, así que lo que haremos será [**estandarizar la covarianza**]{.hl-yellow}. Definiremos la [**coeficiente correlación lineal (de Pearson)**]{.hl-yellow} como la covarianza dividida entre el producto de las desviaciones típicas (adimensional)


$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

. . .

Tiene el [**mismo signo que la covarianza**]{.hl-yellow} (el denominador es siempre positivo) y sus [**valores siempre están entre -1 y 1**]{.hl-yellow}

* más cerca de -1 o 1 → relación lineal más fuerte
* más cerca de 0 → ausencia de relación **LINEAL**

---

## Repaso: covarianza y correlación

![](img/correlaciones.jpg)


# Clase 2: importancia del dataviz {#clase-2}

[**¿Basta con calcular la correlación para cuantificar si un ajuste es adecuado?**]{style="color:#444442;"}


---

## Conjunto anscombe

En el paquete `{datasets}` se encuentra el dataset conocido como [**cuarteto de Anscombe**]{.hl-yellow}, un dataset que cuenta con 4 conjuntos de datos.

```{r}
anscombe_tb <- as_tibble(datasets::anscombe)
anscombe_tb
```

. . .

[**Primera tarea: convierte a tidydata**]{.hl-yellow}

---

## Conjunto anscombe

```{r}
anscombe_x <- 
  anscombe_tb |>
  pivot_longer(cols = x1:x4, names_to = "dataset", values_to = "x") |> 
  select(-contains("y")) |> 
  mutate(dataset = str_remove_all(dataset, "x"))

anscombe_y <- 
  anscombe_tb |>
  pivot_longer(cols = y1:y4, names_to = "dataset", values_to = "y") |> 
  select(-contains("x"), -dataset)

anscombe_tidy <-
  anscombe_x |> mutate("y" = anscombe_y$y)
anscombe_tidy
```

---

## Conjunto anscombe

[**¿Qué características muestrales tienen?**]{.hl-yellow} Calcula la media, varianza, desv. típica, covarianza y correlación en cada dataset

. . .

```{r}
anscombe_tidy |> 
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

. . .

Si te fijas [**todos los datasets tienen los mismos momentos**]{.hl-yellow} muestrales, incluso tendrían el mismo ajuste de regresión...¿serán el mismo dataset desordenado?

---

## Conjunto anscombe

[**Visualiza los 4 datasets**]{.hl-yellow}

. . .

```{r}
#| code-fold: true
ggplot(anscombe_tidy) +
  geom_point(aes(x = x, y = y, color = dataset), size = 3, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~dataset) + theme_minimal()
```

---

## Conjunto anscombe

```{r}
#| echo: false
ggplot(anscombe_tidy) +
  geom_point(aes(x = x, y = y, color = dataset), size = 3, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~dataset) + theme_minimal()
```


Por suerte o por desgracia [**no todo son matemáticas**]{.hl-red}: antes de pensar que modelo es mejor para nuestros datos, es importantísimo realizar un [**análisis exploratorio**]{.hl-yellow} de los datos (incluyendo visualización)

---

## Datasaurus

Podemos visualizarlo de manera aún más extrema con el dataset `datasaurus_dozen` del paquete `{datasauRus}` (ver más en <https://www.research.autodesk.com/publications/same-stats-different-graphs/>)

```{r}
library(datasauRus)
datasaurus_dozen |>
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

---

## Datasaurus

```{r}
#| code-fold: true
ggplot(datasaurus_dozen |> filter(dataset != "wide_lines"),
       aes(x = x, y = y, color = dataset)) +
  geom_point(size = 2.5, alpha = 0.75) +
  geom_smooth(method = "lm", se = FALSE)  +
  facet_wrap(~dataset, ncol = 3) +
  theme_minimal()
```

---

# Clase 3: profundizando en la correlación {#clase-3}

[**Matrices de correlación y covarianza. Correlación vs causalidad vs dependencia**]{style="color:#444442;"}


---

## Correlación lineal: sin agrupar


Como decíamos, la idea detrás de la [**covarianza**]{.hl-yellow} es una "varianza" entre dos variales (la varianza es una covarianza de una variable consigo misma), midiendo el **promedio de lo que se desvía cada una respecto a su media**

$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

. . .

Para calcularla en `R` basta con usar la función `cov()`

```{r}
starwars |> 
  drop_na(mass, height) |> 
  summarise(cov(mass, height))
```

---

## Correlación lineal: sin agrupar

Vamos a practicar una vez más como [**hacerlo a mano con el siguiente ejercicio**]{.hl-yellow}.

---

## Cor lineal: sin agrupar

En la tabla inferior se han recopilado (del 2013 al 2022) la **temperatura media en el mes de abril en Madrid (variable X, en ºC)** y el **número de días (variable Y) en el que el nivel de ozono superó las 0.20 ppm (partes por millón)**

* ¿Cuál fue media de días en los que se superó umbral de ozono de 0.20 ppm?
* ¿Cuál fue media de días en los que se superó umbral de ozono en los años que la temperatura media en marzo superó los 17.4ºC?
* ¿Cuál es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)

---


## Correlación lineal: sin agrupar

Repite el ejercicio con pocas líneas de código `R`

* ¿Cuál fue la media de días en los que se superó el umbral de ozono de 0.20 ppm?
* ¿Cuál fue la media de días en los que se superó el umbral de ozono en los años que la temperatura media en marzo superó los 17.4ºC?
* ¿Cuál es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)


---


## Correlación lineal: sin agrupar

Realiza lo que consideres tanto a mano como en `R`

* ¿Existe alguna **relación de dependencia entre las variables**? ¿De qué tipo? ¿Cómo de fuerte o débil es dicha relación? ¿En qué dirección es dicha relación?


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

---

## Correlación lineal: sin agrupar

No sé si te has fijado qué sucede cuando intentamos [**calcular la covarianza/correlación de varias variables**]{.hl-yellow}, por ejemplo vamos a calcular la (cuasi)covarianza de todas las variables numéricas de starwars.

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

. . .

Podemos usar la función `cov()` sin más, fuera de un resumen, obteniendo lo que se conoce como [**matriz de (cuasi)covarianzas**]{.hl-yellow} y que tendrá un papel fundamental en estadística ya que contiene la información (= varianza) del dataset.

---

## Matriz de covarianzas


```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

Además de ser [**simétrica**]{.hl-yellow}...¿qué tenemos en la [**diagonal**]{.hl-yellow}?

. . .

La [**matriz de (cuasi)covarianzas**]{.hl-yellow} se denota como $\Sigma$ y sus elementos se define como $\Sigma_{ii} = s_{x_i}^{2}$ para la diagonal y $\Sigma_{ij} = \Sigma_{ji} = s_{x_i x_j}$ fuera de ella.

. . .

::: callout-important
## Importante

Recuerda que los softwares estadísticos nos devuelven siempre la [**cuasi covarianza**]{.hl-purple}, dividido entre $n-1$ y no entre $n$. La cuasivarianza y la cuasicovarianza son los [**mejores estimadores muestrales (insesgados)**]{.hl-yellow} de los respectivos parámetros poblaciones

:::

---

## Matriz de correlaciones

De la misma manera con `cor()` podemos [**calcular la matriz de correlaciones**]{.hl-yellow} (en este caso sin el `cuasi` ya que se cancelan denominadores)

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cor()
```

. . .

La [**matriz de correlaciones**]{.hl-yellow} se denota como $R$ y sus elementos se define como $r_{ii} = 1$ para la diagonal y $r_{ij} = r_{x_ix_j}$ fuera de ella, y nos proporciona la dependencia lineal entre variables ya de manera **estandarizada**.

---

## Matriz de correlaciones

[**¿Se te ocurre alguna manera de calcular la matriz de correlaciones a partir de la de covarianzas?**]{.hl-yellow}

---

## Correlación lineal: datos agrupados

Imagina ahora que queremos calcular la covarianza/correlación de los siguientes datos que representan 24 tiradas de dados

![](img/dados.png)

¿Cómo calcular la covarianza/correlación agrupando los datos?

---

## Correlación lineal: datos agrupados

![](img/tabla-bi-freq.png)
---

## Correlación lineal: datos agrupados

---

## Correlación vs dependencia


Podemos tener [**variables incorreladas**]{.hl-red}, con correlación nula, pero que [**exista dependencia entre ellas**]{.hl-green}: la covarianza/correlación [**SOLO CAPTURA relaciones lineales**]{.hl-yellow}, nada más.

. . .

Veamos un ejemplo sencillo con $X = \left\lbrace -1, 0, 1 \right\rbrace$ y $Y = X^2 =  \left\lbrace 1, 0, 1 \right\rbrace$. 

* La media de ambas es nula
* La media del producto es la media de $XY = \left\lbrace -1, 0, 1 \right\rbrace$, que es de nuevo nula
* Así la covarianza $\overline{x*y} - \overline{x}*\overline{y}$ es nula a pesar de tener la mayor dependencia posible (dependencia funcional)

---

## Correlación vs dependencia

![](img/covarianza-no-lineal.png)

En relaciones no lineales como la de la imagen, la **correlación estará cercana a cero** (ya que no hay relación lineal) pero existe una [**dependencia**]{.hl-yellow}. Diremos que [**dos variables son dependientes entre sí**]{.hl-yellow} cuando existe un **patrón numérico que las relaciona**

. . .

* [**Independencia implica incorrelación**]{.hl-green}
* [**Incorrelación NO implica independencia**]{.hl-red}

---

## Correlación vs dependencia

![](img/escenarios-covarianza.png)

---

## Dependencia vs causalidad

![](img/helados-moreno.jpg)

Si $X$ e $Y$ fuesen variables dependientes (lineales o no)...[**¿implicaría que el ascenso/descenso de una provoca el ascenso/descenso de la otra?**]{.hl-yellow}

. . .

Imagina por ejemplo dos variables: nivel de bronceado (X) y consumo de helados (Y). ¿Son dependientes? Aparentemente sí ya que su comportamiento es similar. **¿Una causa la otra?**

---

## Dependencia vs causalidad

![](img/helados-moreno.jpg)

Diremos que dos variables tienen una [**relación causal o de causalidad**]{.hl-yellow} cuando una variable es la CAUSA del comportamiento de la otra, algo que no sabremos solo con estadística (sino con conocimiento experto, en este caso de nutricionistas y médicos)

. . .

[**Correlación NO IMPLICA causalidad**]{.hl-red} (al menos no tiene por qué)

---

## Dependencia vs causalidad


![](img/chocolate-correlacion.jpg)


Este fenómeno es conocido como [**correlaciones espúreas**]{.hl-yellow}, y aparece cuando dos variables presentan una [**relación matemática**]{.hl-green} pero sin [**ningún tipo de relación causal o lógica**]{.hl-red}. Puedes ver más en <https://www.tylervigen.com/spurious-correlations>

---

## Dependencia vs causalidad

![](img/confounder.jpg)

Dicho patrón matemático puede deberse al mero azar o la existencia de lo que se conoce como [**variables de confusión**]{.hl-yellow} (en el caso del helado, el verano es el que causa ambas).

La rama que se dedica al [**estudio de la causalidad**]{.hl-yellow}, combinando las ramas de la **estadística, filosofía, sociología y psicología** se le conoce como [**inferencia causal**]{.hl-yellow} y es fundamental en un análisis más profunde de las relaciones entre las variables (sobre todo en campos como la economía o la sociología)


# Clase 4: primeras regresiones {#clase-4}

[**Historia regresión. Aprendizaje supervisado. Regresión lineal univariante**]{style="color:#444442;"}

---

## Historia de la reg

> Hay una regla universal: cualquier pariente tuyo es probablemente más mediocre que tú»

La [**historia de regresión**]{.hl-yellow} se remonta a **Francis Galton**, primo de Charles Darwin, que además de estadístico fue psicólogo, geógrafo y, por desgracia, el primer eugenésico (de hecho acuñó el termino)

. . .

También fue el primero en proponer métodos de clasificación de huellas en medicina forense e incluso se le atribuye el primer mapa meteorológico de la historia

---

## Regresión y Darwin

Galton mostró fascinación por «El origen de la especies» de su primo. Sin embargo, [**Galton no se centraba en los mejor adaptados sino en los que él llama mediocres**]{.hl-yellow}

. . .

Según Galton, las sociedades estaban fomentando la mediocridad, interfiriendo en la selección natural, así que empezó a **estudiar si el talento era o no hereditario**.


. . .

¿Su conclusión? El [**talento se disipaba**]{.hl-yellow} a lo largo de las generaciones

---

## Regresión a la mediocridad

En 1886 publicó «Regression towards mediocrity in hereditary stature», un artículo que cambiaría la estadística: fue el [**primer uso documentado de lo que hoy conocemos como recta de regresión**]{.hl-yellow} 

. . .

:::: columns
::: {.column width="38%"}

![](img/reg_dalton.jpg)

:::

::: {.column width="62%"}

Galton analizó la estatura de 205 hijos y padres, observando que, de nuevo, los valores extremos se disipaban: a lo largo de las generaciones había una [**regresión (un retroceso) a la mediocridad (entendida como la media)**]{.hl-yellow} 


:::
::::

Hijos de altos eran algo más bajitos, e hijos de bajitos eran algo más altos.

---

## Regresión a la mediocridad

:::: columns
::: {.column width="60%"}

![](img/reg-galton-coef.jpg)

:::

::: {.column width="40%"}

Galton no solo observó que las [**estaturas «regresaban» a un valor medio sino que lo hacían con un patrón**]{.hl-yellow}, con un factor constante de $2/3$: si los padres se desviaban $+3$ por encima de la media, los hijos se desviaban solo $(2/3)*3 = +2$ por encima de la media.

:::
::::

---

## Aprendizaje estadístico

La regresión lineal es el modelo más simple de lo que se conoce como [**aprendizaje estadístico (Machine Learning)**]{.hl-yellow}, en concreto del conocido como [**aprendizaje supervisado**]{.hl-yellow}

![](img/ml.jpg)


---

## Ciencia de Datos


La [**ciencia de datos**]{.hl-yellow} es precisamente la rama que integra las matemáticas, la estadística, la probabilidad, el Machine Learning e incluso el Big Data

![](img/stats-ml.jpg)

---

## Aprendizaje ¿supervisado?

![](img/unsupervised-learning.jpg)

---

## Aprendizaje ¿supervisado?

En el campo del Machine Learning hay principalmente dos tipos de metodologías:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje supervisado**]{.hl-purple}: tendremos dos tipos de variables, la [**variable dependiente (output/target)**]{.hl-yellow} que se quiere predecir/clasificar, normalmente denotada como $Y$, y las [**variables independientes (inputs) o explicativas o predictoras**]{.hl-yellow}, que contienen la información disponible. Ejemplos: regresión, knn, árboles, etc.
 

:::

::: {.column width="50%"}

![](img/supervised.jpg)
:::
::::

---

## Aprendizaje ¿supervisado?

En el campo del Machine Learning hay principalmente dos tipos de metodologías:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje no supervisado**]{.hl-purple}: no existe la distinción entre target y variables explicativas ya que [**no tenemos etiquetados los datos**]{.hl-yellow}, no sabemos a priori la respuesta correcta. El aprendizaje no supervisado [**buscará patrones**]{.hl-yellow} basados en similitudes/diferencias. Ejemplos: PCA, clustering, redes neuronales, etc.

:::


::: {.column width="50%"}

![](img/unsupervised.jpg)

:::
::::

---

## Clasificación vs predicción

Como hemos comentado, la [**regresión lineal**]{.hl-yellow} se enmarca dentro del [**predicción supervisada**]{.hl-yellow}

* [**Predicción**]{.hl-purple}: la [**variable objetivo es una variable cuantitativa continua**]{.hl-yellow} (por ejemplo, precio, glucosa, peso, etc).

* [**Clasificación**]{.hl-purple}: la [**variable objetivo es una variable cualitativa**]{.hl-yellow} (por ejemplo, especie de flor, ausencia/presencia de enfermedad, si/no, etc) o **cuantitativa discreta** (por ejemplo, número de accidentes). La etiqueta tomará un valor dentro del conjunto de modalidades permitidas, pudiendo ser **binaria** (si/no) o **multiclase** (A, B, C, D).

&nbsp;

📚 Ver «The elements of Statistical Learning» (Hastie et al., 2008)

# Clase 5: ajuste de regresión {#clase-5}

[**Interpretación de coeficientes. Método mínimos cuadrados**]{style="color:#444442;"}

---

## Modelo predictivo

Dentro del marco de la [**predicción supervisada**]{.hl-yellow} un modelo tendrá siempre la siguiente forma:

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$

* $X$ serán los [**datos**]{.hl-yellow}

* $f(\cdot)$ será nuestro [**modelo**]{.hl-yellow}, es decir, el [**valor esperado de $Y$**]{.hl-yellow} con la información que tenemos.

* $\mathbf{X} = \left(X_1, \ldots, X_p \right)$ serán nuestras [**predictoras o variables independientes**]{.hl-yellow}

* $\varepsilon$ será el [**error o ruido**]{.hl-yellow}, una [**variable aleatoria de media 0**]{.hl-yellow} $E \left[\varepsilon | \boldsymbol{X} = x \right] = 0$ (el error debería ser reducido a **algo aleatorio (irreducible)**, aunque en estadística SIEMPRE nos vamos a equivocar).

---

## Modelo predictivo

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$


El objetivo es intentar [**estimar dicha variable objetivo**]{.hl-yellow} minimizando al máximo el error cometido (la parte que podemos evitar).

. . .

El [**modelo estimado**]{.hl-yellow} se definirá como

$$\widehat{Y} := \widehat{E \left[Y | \boldsymbol{X} = x \right]}  = \widehat{f}\left(X_1, \ldots, X_p \right), \quad \widehat{\varepsilon} = \widehat{Y} - Y$$

* $\widehat{Y}$ serán las [**estimaciones**]{.hl-yellow}, definidas como la estimación del [**valor esperado de $Y$**]{.hl-yellow} con la información que tenemos.

* $\widehat{f}$ será el [**modelo estimado**]{.hl-yellow}

* $\widehat{\varepsilon}$ los [**errores estimados**]{.hl-yellow} (con media muestral igual a cero).


---

## Reg. lineal univariante

En el caso concreto de la [**regresión lineal**]{.hl-yellow} nuestro modelo será un **hiperplano lineal** (en el caso de una variable, una simple recta):

$$E \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p$$

. . .

Su estimación será por tanto

$$\widehat{Y} := \widehat{E \left[Y | \boldsymbol{X} = x \right]} = \widehat{f}(\mathbf{X}) = \widehat{\beta_0} + \widehat{\beta_1} X_1 + \ldots + \widehat{\beta_p} X_p $$

* El modelo $\widehat{f}(\cdot)$ será una [**recta/hiperplano lineal**]{.hl-yellow}

* En el caso de la regresión lineal [**univariante**]{.hl-yellow} tendremos por tanto $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1X$ ($p = 1$)

* El objetivo será obtener la estimación de los $\widehat{\beta}$ tal que **minimicemos el error**

---

## Varianza residual

Como ya hemos comentado, una manera de cuantificar lo que nuestro modelo se equivoca es lo que llamamos [**varianza residual o error cuadrático medio**]{.hl-yellow} (varianza del error), que podemos estimar como sigue (ya que la [**media muestral de los errores será cero**]{.hl-purple})

$$s_{r}^{2} = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \left( \widehat{\varepsilon}_{i} - \overline{\widehat{\varepsilon}}_{i}\right)^2 = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} $$
¿Cómo quedaría la fórmula si **desarrollo lo que vale los errores estimados** $\widehat{\varepsilon}$?

. . .


$$s_{r}^{2} := s_{r}^{2}\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)= \frac{1}{n} \sum_{i=1}^{n} \left(Y_i - \widehat{Y}_i \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\widehat{\beta}_0 + \widehat{\beta}_1X_i \right)\right]^2$$

---

## Método mínimos cuadrados

El [**método de los mínimos cuadrados**]{.hl-yellow}, originariamente planteado a la vez por Legendre y Gauss, consiste en establecer que los [**parámetros óptimos**]{.hl-yellow} serán aquellos $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$ que minimicen dicha suma de cuadrados (que minimicen la varianza residual)

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} s_{r}^{2}\left(\beta_0, \beta_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} n*s_{r}^{2}\left(\beta_0, \beta_1 \right)$$

Vamos a desarrollar ese cuadrado...

--- 


## Método mínimos cuadrados

$$s_{r}^{2}\left(\beta_0, \beta_1 \right) = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\beta_0 + \beta_1 X_i \right)\right]^2$$ 
Vamos a desarrollar ese cuadrado...

. . .

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$




---

## Método mínimos cuadrados

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$

¿Cómo encontrar el mínimo de una función?

. . .

Efectivamente, derivando.

$$\frac{\partial s_{r}^{2}}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{0} + 2 \beta_1 X_i - 2Y_i \right] = 2 \left( \beta_{0} + \beta_{1} \overline{x} - \overline{y} \right)$$ 

$$\frac{\partial s_{r}^{2}}{\partial \beta_1} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{1}X_{i}^{2} + 2 \beta_0 X_i - 2Y_iX_i \right] = 2 \left( \beta_{1} \overline{x^2}+\beta_{0}\overline{x} - \overline{xy} \right)$$ 

---

## Método mínimos cuadrados


Si lo agrupamos en un sistema y lo [**igualamos a cero**]{.hl-yellow} para encontrar el óptimo tenemos

$$\left\{\beta_{0} + \beta_{1} \overline{x}   = \overline{y} \atop \overline{x} \beta_{0} + \overline{x^2} \beta_{1} = \overline{xy} \right.$$ 

¿Cómo resolver un sistema?

. . .

Haciendo la regla de Cramer tenemos que

$$\widehat{\beta}_1 = \frac{\overline{xy} - \overline{x}*\overline{y}}{\overline{x^2} - \overline{x}^2} = \frac{s_{xy}}{s_{x}^{2}}$$

$$\widehat{\beta}_0 = \frac{\overline{y}*\overline{x^2} - \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2}  = \overline{y} + \frac{\overline{y}*\overline{x}^2- \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2} = \overline{y} - \widehat{\beta}_1 \overline{x}$$


---

## Estimación reg. univariante


Los [**estimadores de los parámetros de una regresión lineal univariante**]{.hl-yellow} serán por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

. . .

Es importante [**distinguir lo población de lo muestral**]{.hl-yellow}

* Los parámetros $\left( \beta_0, \beta_1 \right)$ son desconocidos, los [**parámetros poblacionales**]{.hl-yellow}

* Los parámetros $\left( \widehat{\beta}_0, \widehat{\beta}_1 \right)$ son estimados a partir de los datos, son [**variables aleatorias**]{.hl-yellow} ya que han sido calculados en función de una muestra aleatoria $\left\lbrace \left(x_i, y_i \right) \right\rbrace_{i=1}^{n}$ de la población $\left(X, Y \right) $

---

## Estimación reg. univariante

Los [**estimadores de los parámetros de una regresión lineal univariante**]{.hl-yellow} serán por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

¿Cuál es su [**interpretación**]{.hl-yellow}?

. . .

* [**Ordenada en el origen**]{.hl-yellow}: también llamado **intercepto**, y denotado como $\beta_0$ su valor real, es el valor de $Y$ cuando $X=0$. Es decir, $\widehat{\beta}_0$ se puede interpretar como la estimación $\widehat{Y}$ cuando $X = 0$ (cuando dicha estimación tenga sentido).


. . .

* [**Pendiente**]{.hl-yellow}: denotado com $\beta_1$ su valor real, cuantifica el incremento de $Y$ cuando $X$ aumenta una unidad. Es decir, $\widehat{\beta}_1$ se puede interpretar como la variación de la **estimación $\widehat{Y}$ cuando $X$ tiene un incremento unitario**.

---

## Regresión en R

Para hacer un [**ajuste de regresión lineal univarainte en R**]{.hl-yellow} solo necesitamos utilizar la función `lm()` (de linear model) con dos argumentos:

* `data = ...`: los datos de los que queremos sacar el ajuste (sin ausentes de momento)
* `formula = ...`: la fórmula del ajuste (y frente a x, expresado como `y ~ x`)

```{r}
datos <- 
  starwars |>
  drop_na(mass, height) 
ajuste_lineal <- lm(data = datos, formula = mass ~ height)
```

---

## Regresión en R

Una vez realizado el ajuste podemos [**resumirlo**]{.hl-yellow} con `summary()`

```{r}
ajuste_lineal |> summary()
```

. . .

¿Qué representa cada bloque de la salida?

---

## Regresión en R

```{r}
ajuste_lineal |> summary()
```

* `Call: ...`: la orden que hemos ejecutado

* `Residuals: ...`: un resumen en forma de [**cuartiles de los residuales**]{.hl-yellow}, es decir, de los [**errores**]{.hl-yellow} (fíjate que aunque la media de los errores siempre es 0, no tiene porque serlo la mediana, de hecho que no lo sea ya nos indica que normales no van a ser seguramente).


---

## Regresión en R

```{r}
ajuste_lineal |> summary()
```


* `Coefficients: ...`: de momento solo nos interesa la columna [**Estimate**]{.hl-yellow} que nos da las estimaciones de los parámetros. En la fila `Intercept` siempre irá $\widehat{\beta}_0$, y el resto de filas tendrá el nombre de la variable predictora a la que multiplica el parámetro (en este caso la fila `height` corresponde a la estimación $\widehat{\beta}_1$).

---


## Regresión en R

```{r}
ajuste_lineal |> summary()
```

En nuestro caso particular, el [**ajuste de regresión**]{.hl-yellow} viene dado por $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1 X = -11.4868 + 0.6240*X$, donde $X$ es la estatura: por cada cm extra que mida un personaje, el modelo estima que el peso aumenta 0.6240 kg, y la estimación de peso para un persona que mida 0 cm es de -11.4868 kg (obviamente esta estimación mucho sentido no tiene)

---

## Regresión en R

```{r}
ajuste_lineal |> summary()
```

* `Multiple R-squared: ...` (de momento lo demás lo ignoramos): es el conocido como $R^2$ o bondad de ajuste. Hablaremos de él en las próximas clases, pero de momento, nos basta saber que es una [**métrica de calidad del ajuste**]{.hl-yellow} que va de 0 (peor) a 1 (mejor).

---

## Predicción en R

Una vez estimado el modelo (sus coeficientes) podemos usarlo, bien para  [**estimar valores que el modelo conoce**]{.hl-yellow} (de los que aprendió) o para [**predecir nuevos valores**]{.hl-yellow} de los que el modelo no ha aprendido

. . .

Simplemente necesitamos la función `predict(modelo, nuevos_datos)`, usando como argumentos el **modelo entrenado** y un [**nuevo dataset que contenga una/s columna/s llamadas igual que nuestras variables predictores**]{.hl-yellow} con los valores de la predictora para los que queremos predecir

```{r}
predicciones <- 
  tibble("height" = c(15, 81.3, 153.1, 189.3, 210.3),
         "mass_predict" = predict(ajuste_lineal, tibble("height" = height)))
predicciones
```

---

## Rango de fiabilidad

Aunque hablaremos en profundidad de la bondad de ajuste, supongamos que tenemos un **modelo extremadamente bueno**, con una bondad de ajuste $R^2 = 0.99$. ¿Sería fiable nuestro modelo anterior para predecir el peso de un personaje con $X = 15$ cm?

. . .

```{r}
predicciones
```

Si te fijas la [**¡predicción del peso es negativa!**]{.hl-red}: por muy bebé que sea, algo pesará. ¿Por qué sucede esto?

---

## Rango de fiabilidad


![](img/rango_1.png)
![](img/rango_2.png)

Nuestro [**modelo solo puede aprender de los datos que conoce**]{.hl-yellow}: solo podremos predecir para valores nuevos de las predictoras [**dentro del rango de las predictoras con las que entrenamos**]{.hl-yellow} el modelo

Da igual lo bueno que sea el modelo: las [**predicciones fuera del rango de las predictoras de entrenamiento**]{.hl-red} no serán fiables ya que el modelo no sabe lo que sucede fuera.

# Clase 6: diagnosis  {#clase-6}

[**Hipótesis e inferencia del modelo**]{style="color:#444442;"}

---

## Repaso


* [**Modelo lineal**]{.hl-yellow}: descrito como $Y = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p X_p + \varepsilon$, buscamos estimar el **valor esperado**  $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p x_p$, donde $\boldsymbol{X}$ es la población y $x$ la muestra.

. . .

* [**Caso univariante**]{.hl-yellow}: $p = 1$ tal que $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 x$.

. . .

* [**Estimación**]{.hl-yellow}: $\widehat{Y}$ en función de $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$. De todas las rectas posibles, queremos la [**recta de regresión mínimos-cuadrados**]{.hl-yellow}: **minimiza la varianza residual** (calculamos sus derivadas parciales, igualamos a cero y resolvemos el sistema). Esos estimadores serán $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$  y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$.

. . .

* [**Predicción**]{.hl-yellow}: podemos estimar o predecir valores con la recta $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_0 X$, siempre y cuando $X = x$ esté dentro del rango de valores de entrenamiento del modelo.

---

## Diagnosis

Hasta ahora no hemos pedido nada a la muestra $\left\lbrace \left( X_i, Y_i \right) \right\rbrace_{i=1}^{n}$, ¿para qué necesitaríamos [**hipótesis**]{.hl-yellow} entonces?

. . .

La razón es que, hasta ahora, lo único que hemos podido realizar es una [**estimación puntual**]{.hl-yellow} de los parámetros, pero dado que dichos estimadores serán variables aleatorias, necesitaremos realizar [**inferencia estadística**]{.hl-yellow} sobre ellos (recuerda: los parámetros son simpleme estimaciones para esa muestra de la población, de forma que dada otra muestra, la recta será distinta).

. . .

Para poder cuantificar la [**variabilidad y precisión de nuestras estimaciones**]{.hl-yellow} necesitaremos que los datos cumplan ciertas [**hipótesis probabilísticas**]{.hl-purple}: lo interesante no es la estimación puntual de los parámetros a partir de la muestra sino lo que [**podamos inferir de ellos a la población**]{.hl-green}

---

## Diagnosis

En el caso de la regresión lineal univariante pediremos [**4 hipótesis**]{.hl-yellow}

. . .

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1 x$

. . .

2. [**Homocedasticidad**]{.hl-green}: el modelo no podrá explicar toda la información (a veces se equivocará por arriba y otras por abajo), pero necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow}, tal que $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \boldsymbol{X} = x \right] = cte < \infty$, que no varíe según aumenta o disminuye la $X$.

. . .

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$

. . .

4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser independientes entre sí (el error en una observación no depende de otras). En particular, serán **incorrelados**

$${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] - E \left[\varepsilon_i \right] E \left[\varepsilon_j \right] = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$$

---


## Diagnosis


Las 4 hipótesis se pueden [**resumir de manera teórica**]{.hl-yellow} en

$$Y | \boldsymbol{X} = x \sim N \left(\beta_0 + \beta_1x, \sigma_{\varepsilon}^2 \right)$$

. . .

Su [**versión muestral**]{.hl-purple} sería simplemente

$$y_i | \boldsymbol{X} = x_i \sim N \left(\beta_0 + \beta_1x_i, \sigma_{\varepsilon}^2 \right), \quad i=1,\ldots,n$$

---

## Inferencia de los parámetros

Las hipótesis nos permiten decir (lo demostraremos más adelante) que los [**parámetros estimados siguen una distribución (condicionada) normal**]{.hl-yellow} de [**media el parámetro**]{.hl-purple}  a estimar y de [**varianza el conocido como standard error**]{.hl-purple}

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

. . .

Por las propiedades de la normal y el teorema central del límite 

$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)}| \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$

---

## Inferencia de los parámetros


$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

Veremos más adelante porqué pero de momento...estos son los $SE$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

¿Qué **propiedades** tienen estos estimadores?


---

## Inferencia de los parámetros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

* [**Insesgados**]{.hl-yellow}: ambos son [**estimadores insesgados**]{.hl-green} ya que la esperanza de la estimación es el valor a estimar. $E \left[ \widehat{\beta}_j \right] = \beta_j$

. . .
 
* [**Precisión vs tamaño muestral**]{.hl-yellow}: cuando $n \to \infty$, sus varianzas tienden a cero, es decir, $SE \left(\widehat{\beta}_j\right)^2 \to^{n \to \infty} 0$ ya que $n$ está dividiendo. Traducción: [**a más datos, mayor precision**]{.hl-green} (menos varianza tendrán los estimadores si repetimos la toma de muestras)


---

## Inferencia de los parámetros


$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$

. . .

* [**Precisión vs var residual**]{.hl-yellow}: cuanto más grande sea la varianza del error $\sigma_{\varepsilon}^{2}$, $SE \left(\widehat{\beta}_j\right)^2$ crecerá (es decir, [**más ruido implicará más imprecisión**]{.hl-red})

. . .

* [**Precisión vs varianza de X**]{.hl-yellow}: cuanto más grande sea la varianza de la predictora $s_{x}^2$, $SE \left(\widehat{\beta}_j\right)^2$ decrecerá, tal que $SE \left(\widehat{\beta}_j\right)^2 \to^{s_{x}^2 \to \infty} 0$. Dicho de otra forma: cuánta [**más información (varianza) contenga nuestra tabla, mayor precisión**]{.hl-green}.

. . .

* [**Precisión vs media X**]{.hl-yellow}: solo afecta a la estimación de $\beta_0$, cuya [**precisión decrece cuando la media aumenta**]{.hl-red}. Esto tiene sentido ya que cuánto más grande en media sean los datos, menos fiable será la predicción para $X=0$.

---

## Inferencia de los parámetros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


Pero tenemos un problema: la [**varianza residual poblacional**]{.hl-yellow} (varianza población del error) $\sigma_{\varepsilon}^{2}$ es [**desconocida**]{.hl-yellow}. Como suele pasar, la [**sustituiremos por un estimador (insesgado)**]{.hl-yellow} de la misma (ya demostaremos...), donde $p$ es el número de variables (lo que R llama `std error` en la salida)

. . .

$$\widehat{\sigma_{\varepsilon}^{2}} = \frac{n}{n-p-1} s_{r}^{2} = \frac{1}{n-p-1} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 =_{p=1} \frac{1}{n-2} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2$$
---

## Inferencia de los parámetros



$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$


¿Qué suele suceder cuando en una normal **desconocemos la varianza real y la sustituimos por su estimador insesgado**?

. . .

Si [**sustituimos el error standard por su estimador**]{.hl-yellow} (sustituyendo la varianza residual por su estimador insesgado), obtenemos que sigue una **t-Student**

$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$
donde $\widehat{SE} \left(\widehat{\beta}_0\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right]$ y $\widehat{SE} \left(\widehat{\beta}_1\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n s_{x}^2}$

---

## Inferencia de los parámetros


```{r}
ajuste_lineal |> summary()
```

* `std error`: es lo que hemos denotado como $\widehat{SE} \left(\widehat{\beta}_j\right)^2$

. . .

* `t value`: es el estadístico $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ que hemos dicho que sigue una t-Student (fíjate que hemos puesto $\beta_j = 0$)

## Inferencia de los parámetros


```{r}
ajuste_lineal |> summary()
```

* `Pr(>|t|)`: representa un p-valor. ¿De qué contraste se te ocurre?

. . .

* `Residual standard error`: es el estimador insesgado de la varianza residual, que hemos denotado como $\widehat{\sigma_{\varepsilon}^{2}}$

---

## Inferencia de los parámetros

:::: columns
::: {.column width="55%"}

$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$

:::

::: {.column width="45%"}

![](img/tablas-intervalos.png)

:::
::::

Si te acuerdas de inferencia, esto implica que los [**intervalos de confianza de nuestros parámetros**]{.hl-yellow} son

$$IC \left(\alpha, \widehat{\beta}_j \right) = \left[\widehat{\beta}_j - t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right), \widehat{\beta}_j + t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right) \right]$$
donde $t_{n-p-1; \alpha/2}$ es el cuantil $1-\alpha/2$ (deja a la izquierda una probabilidad de $1-\alpha/2$).

---


## Inferencia de los parámetros

Podemos calcular los intervalos de confianza de los parámetros en `R` con `confint()`

```{r}
confint(ajuste_lineal, level = 0.99)
confint(ajuste_lineal, level = 0.95)
confint(ajuste_lineal, level = 0.9)
```

---

## Inferencia de los parámetros

[**¿Qué significa realmente un intervalo de confianza?**]{.hl-yellow}

. . .

La **probabilidad de que el parámetro real** caiga dentro [**NO EXISTE**]{.hl-red}: el parámetro es desconocido pero fijo, no aleatorio, así que no sentido calcular su probabilidad.

. . .

Si obtenemos para un parámetro un intervalo $[-1, 1]$ al 95%, no significa que $P(parametro \in [-1, 1]) = 0.95$: significa que [**si tomamos 1000 muestras distintas de la población**]{.hl-yellow} y calculamos para cada una el intervalo de confianza, [**aproximadamente 950 intervalos de confianza contendrán dentro el parámetro real**]{.hl-yellow}

. . .

Un [**intervalo al 95% implica que habrá una frecuencia esperada de 0.95**]{.hl-yellow} de que intervalos que no conocemos (porque se derivan de muestras que no hemos tomado) contengan al parámetro real, pero [**no es la probabilidad de que tu intervalo calculado contenga a dicho parámetro**]{.hl-red}: nos habla la precisión de nuestra metodología de estimación, no del parámetro.


---

## Inferencia de los parámetros

[**Deberes**]{.hl-yellow}. Dada una población normal $\mu = 3$ y $\sigma = 1.2$, crea un código que genere 500 muestras distintas (tamaño $n = 100$ cada una), de manera que para cada una apliques un `t.test()` para calcular un IC para $\mu$. Tras ello, gráfica los intervalos como se muestra en la imagen (haz uso de `geom_segment()`)

```{r}
#| echo: false
mu <- 3
sigma <- 1.2
m <- 500
conf_int <- tibble("id" = 1:m, "low" = NA, "high" = NA)

for (i in 1:m) {
  
  sample <- rnorm(n = 100, mean = mu, sd = sigma)
  hyp_test <- t.test(x = sample)
  conf_int[i, 2] <- hyp_test$conf.int[1]
  conf_int[i, 3] <- hyp_test$conf.int[2]
}
conf_int <-
  conf_int |> 
  mutate(true_param = mu >= low & mu <= high)

ggplot(conf_int) +
  geom_segment(aes(y = id, yend = id, x = low, xend = high,
                   color = true_param, linewidth = true_param)) +
  geom_vline(xintercept = mu) +
  scale_linewidth_manual(values = c(1.1, 0.2)) +
  theme_minimal() +
  guides(linewidth = "none") +
  labs(x = "Valores del intervalo",
       y = "id intervalo",
       color = "¿Contiene mu real?")
```

---


## Inferencia de los parámetros

Y si tenemos inferencia, tenemos contrastes: ¿te acuerdas de los p-valores que devuelve la tabla para cada parámetro?

. . .

Para cada parámetro se realiza un [**contraste de significancia**]{.hl-yellow}: ¿cuánta evidencia hay en mis datos para poder decir que el [**valor estimado de mi parámetro es distinto de 0**]{.hl-yellow}?

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

. . .

El estadístico usado es el [**t value**]{.hl-yellow} que devuelve `R` definido como $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ y que, [**BAJO EL SUPUESTO DE QUE LA HIPÓTESIS NULA SEA CIERTA**]{.hl-purple}, sigue una t-Student $t_{n-p-1}$

---

## Inferencia de los parámetros

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

Este contraste significancia en realidad **contesta a la siguiente pregunta**

¿Tiene mi variable $X_j$ un [**efecto lineal SIGNIFICATIVO**]{.hl-yellow} en mi variable $Y$?

. . .

&nbsp;

Esa pregunta puede ser respondida desde un punto de vista **frecuentista** haciendo uso de los [**p-valores**]{.hl-yellow}: si $p-value < \alpha$, se suele rechazar la nula (rechazamos que la variable no tenga efecto lineal significativo, es decir, aceptamos que sí lo tiene); en caso contrario no se suele rechazar (que **no es lo mismo que aceptarla**). Pero...

---

## Paréntesis: p-valor

[**¿Qué representa un p-valor?**]{.hl-yellow}

. . .

Hablaremos más adelante de ellos pero este es un [**pequeño resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>

. . .

* [**Habla sobre los datos**]{.hl-yellow}. Los p-valores nos pueden servir como indicadores de **cómo de incompatible son los datos respecto a un modelo/hipotesis/explicación asumida**: habla sobre los datos, no sobre la veracidad de la hipotesis nula per se o la probabilidad de que los datos hayan salido tan extremos por azar. 

---

## Paréntesis: p-valor

[**¿Qué representa un p-valor?**]{.hl-yellow}

Hablaremos más adelante de ellos pero este es un [**pequeño resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>



* [**Realidades complejas, decisiones complejas**]{.hl-yellow}. Cuidado con reducir a decisiones binarias realidades complejas. La regla del p-valor es una herramienta más, pero no debe ser la única en la que nos basemos para decidir. [**Otros aspectos a considerar**]{.hl-yellow}: calidad de las medidas, diseño del estudio, evidencia externa en la literatura respecto a la causalidad subyacente, validez de las hipótesis planteadas, etc

---

## Paréntesis: p-valor

[**¿Qué representa un p-valor?**]{.hl-yellow}

Hablaremos más adelante de ellos pero este es un [**pequeño resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>


* [**No hagas cherry picking**]{.hl-red}: muestra de manera transparentes que has probado, que ha salido y que no, y no te quedes solo con lo que sale bien o los p-valores que te convenga (**p-hacking**).

---

## Paréntesis: p-valor

[**¿Qué representa un p-valor?**]{.hl-yellow}

Hablaremos más adelante de ellos pero este es un [**pequeño resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>


* [**Significancia estadística**]{.hl-yellow} no implica significacia real (científica, humana, económica, etc). Obtener [**p-valores muy pequeños no implica un mayor efecto**]{.hl-red} que otros p-valores no tan pequeños (y viceversa). [**Cualquier efecto, por pequeño que sea, puede derivar en p-valores pequeños si el tamaño uestral o la precisión de las medidas es suficiente alto**]{.hl-yellow}, y al contrario (efectos evidentes pueden derivar en p-valores altos si $n$ es pequeño)

---

## Paréntesis: p-valor

[**¿Qué representa un p-valor?**]{.hl-yellow}

Hablaremos más adelante de ellos pero este es un [**pequeño resumen respecto al consenso**]{.hl-yellow} recogido en 2016 por la ASA (American Statistical Association) en su trabajo <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>


* [**Alternativas**]{.hl-purple}: intervalos de confianza, de credibilidad, métodos bayesianos, false discovery rates.

&nbsp;

📚 Ver más en <https://www.tandfonline.com/doi/epdf/10.1080/00031305.2016.1154108?needAccess=true>, <https://anabelforte.com/2020/11/15/contraste/> y <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877414/>

---

## Inferencia de los parámetros



```{r}
ajuste_lineal |> summary()
```

Si te fijas ambos p-valores están por encima de $\alpha = 0.05$ (umbral adoptado habitualmente) los que nos dice que [**no hay evidencias de los datos sean compatibles con afirmar que haya efectos lineales significativos**]{.hl-yellow}

. . .

¿Y si probamos a quitar $\beta_0$ (es decir, la respuesta está centrada)?

---

## Inferencia de los parámetros

Para ello basta añadir un `-1` antes de las variables predictoras.

```{r}
lm(data = datos, formula = mass ~ -1 + height) |> summary()
```

Fíjate que ahora, amén que [**hay un efecto lineal significativo de la altura**]{.hl-yellow} y de tener $R^2$ mayor, $\beta_1$ tiene una precisión mayor. En este caso [**solo podía quitar uno**]{.hl-yellow} (perderíamos $X$), pero veremos más adelante cómo decidir cuál quitar si tuviésemos varias variables.

# Clases 7 y 8: caso práctico  {#clase-7-8}

[**¿De qué depende el precio del vino?**]{style="color:#444442;"}

---

## Caso práctico

Vamos a poner en práctica lo aprendido con el dataset `wine.csv` disponible en el campus.

```{r}
datos <- read_csv(file = "./datos/wine.csv")
datos
```

---

## Caso práctico


El conjunto de datos está formado por 27 observaciones (cosechas de vino rojo Burdeos) y 7 variables

* `Year`, `Age`: año de la cosecha y número de años en barrica.
* `Price`: precio en 1990-1991 de cada cosecha (en **escala log**)
* `WinterRain`: lluvia (en mm) que cayó ese año en invierno.
* `AGST`: crecimiento medio de la temperatura (en grados Celsius) durante la temporada.
* `HarvestRain`: lluvia (en mm) que cayó ese año durante la cosecha.
* `FrancePop`: población (miles de habitantes) de Francia.

Ver más en detalles en <https://doi.org/10.1080/09332480.1995.10542468>

. . .

[**El objetivo: predecir el precio**]{.hl-yellow}


---

## Caso práctico

```{r}
datos
```

Para predecir el precio vamos a usar (de momento) una [**regresión lineal univariante**]{.hl-yellow}, donde $Y = precio$ y deberemos elegir la predictora $X$ más apropiada.

---

## Pasos a seguir

1. [**Análisis exploratorio inicial**]{.hl-yellow}:
  - ¿Las variables son [**numéricas (continuas)**]{.hl-purple}?
  - ¿Tienen [**problemas de rango**]{.hl-purple} (por ejemplo, pesos negativos)? ¿Tienen datos ausentes?
  - ¿Cómo se [**distribuyen las variables**]{.hl-purple}? Ideas: resúmen numérico, histogramas/densidades, boxplots, gráficos de violín, etc
  - ¿Hay [**datos atípicos**]{.hl-purple}?

---

## Pasos a seguir

2. [**Análisis de dependencia**]{.hl-yellow}:
  - ¿Qué [**predictora está más correlacionada (linealmente) con la variable objetivo**]{.hl-purple} a predecir? ¿Existe otro tipo de dependencia (pendiente implementar en `R`)?
  - ¿Cómo se [**relacionan las predictoras**]{.hl-purple} entre sí? ¿Están correlacionadas? Ideas: matriz de correlaciones, diagramas de dispersión vs Y, corrplots, etc

. . .

3. [**Formulación**]{.hl-yellow} del modelo

. . .

4. [**Fase de estimación**]{.hl-yellow}:
  - ¿Cuánto valen los [**parámetros estimados**]{.hl-purple}? ¿Cómo queda el ajuste?
  - ¿Qué [**interpretación**]{.hl-purple} tienen?


---

## Pasos a seguir

5. [**Fase de diagnosis**]{.hl-yellow} (paquetes `{performance}` y `{olsrr}`):
  - ¿Cumplen los datos las [**hipótesis parámetricas**]{.hl-purple} requeridas para poder hacer inferencia? 
  - ¿Cómo modificar los datos para que se cumplan?
  - Análisis de residuales
  
. . .

6. [**Fase de inferencia**]{.hl-yellow}:
  - ¿Qué [**variabilidad**]{.hl-purple} tienen las estimaciones de nuestro parámetros?
  - ¿Las predictoras/intercepto tienen un [**efecto lineal significativo**]{.hl-purple}?
  - ¿Debemos [**re-entrenar el modelo**]{.hl-purple} sin alguno de ellos?

---

## Pasos a seguir

7. [**Fase de evaluación**]{.hl-yellow}:
  - ¿Es significativo el modelo? [**ANOVA: análisis de la varianza**]{.hl-purple}
  - ¿Qué información de la predictora explica el modelo? [**Parámetros de bondad de ajuste**]{.hl-purple} ($R^2$ por ejemplo)

. . .

8. [**Fase de predicción**]{.hl-yellow}


---

## Análisis exploratorio inicial


1. [**Análisis exploratorio inicial**]{.hl-yellow}:
  - ¿Las variables son [**numéricas (continuas)**]{.hl-purple}?
  - ¿Tienen [**problemas de rango**]{.hl-purple} (por ejemplo, pesos negativos)? ¿Tienen datos ausentes?
  
&nbsp;

Para responder a dichas preguntas lo más sencillos es hacer uso de la función `skim()` del paquete `{skimr}`

```{r}
#| eval: false
library(skimr)
datos |> skim()
```

. . .

En este caso [**no tenemos ausentes ni problemas de codificación**]{.hl-green}


---

## Análisis exploratorio inicial

  - ¿Cómo se [**distribuyen las variables**]{.hl-purple}?
  - ¿Hay [**datos atípicos**]{.hl-purple}?


&nbsp;

Para ello podemos [**visualizar la distribución de cada variable**]{.hl-yellow} haciendo uso de densidades, histogramas y/o boxplots (por ejemplo), pero si queremos ahorrar tiempo en visualizar convertimos nuestro dataset en [**tidydata**]{.hl-yellow}

```{r}
datos_tidy <-
  datos |> 
  pivot_longer(cols = everything(), names_to = "variable", values_to = "values")
datos_tidy
```

---

## Análisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(x = values)) +
  geom_density(aes(color = variable, fill = variable),
               alpha = 0.75) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

---

## Análisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(x = values)) +
  geom_histogram(aes(color = variable, fill = variable),
               alpha = 0.75, bins = 10) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

---

## Análisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(x = values)) +
  geom_histogram(aes(color = variable, fill = variable),
               alpha = 0.75, bins = 10) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

---

## Análisis exploratorio inicial

```{r}
#| code-fold: true
ggplot(datos_tidy, aes(y = values, x = variable)) +
  geom_boxplot(aes(color = variable, fill = variable),
               alpha = 0.75) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

[**No hay valores atípicos (respecto a los percentiles)**]{.hl-green}

---

## Análisis exploratorio inicial

Podemos incluso gráficar todos los scatter plot sin transformar los datos con `facet_matrix()` del paquete `{ggforce}`

```{r}
#| code-fold: true
library(ggforce)
ggplot(datos) +
  geom_point(aes(x = .panel_x, y = .panel_y)) +
  facet_matrix(vars(everything())) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  theme_minimal()
  
```

---

## Análisis exploratorio inicial

Podemos también visualizar con un [**scatter plot**]{.hl-yellow} todas las variables y además sus correlaciones, con el paquete `{GGally}` y la función `ggpairs(), sin necesidad de convertir a tidydata.

```{r}
#| code-fold: true
library(GGally)
ggpairs(datos) +
  theme_minimal()
```
   
---


## Análisis de dependencia

2. [**Análisis de dependencia**]{.hl-yellow}:
  - ¿Qué [**predictora está más correlacionada (linealmente) con la variable objetivo**]{.hl-purple} a predecir? ¿Existe otro tipo de dependencia (pendiente implementar en `R`)?
  - ¿Cómo se [**relacionan las predictoras**]{.hl-purple} entre sí? ¿Están correlacionadas? Ideas: matriz de correlaciones, diagramas de dispersión vs Y, corrplots, etc

. . .

Este paso será crucial en el [**contexto multivariante**]{.hl-yellow} pero en este caso simplemente vamos a ver como se relacionan linealmente las predictoras entre sí, y cuál de ellas es la **más adecuada para predecir linealmente** `precio`

---

## Análisis de dependencia

El primer paso es la [**matriz de correlaciones**]{.hl-yellow} con la función `cor()` o con la función `correlate()` del paquete `{corrr}` (importa en tibble más visual)

```{r}
library(corrr)
datos |> correlate()
```

. . .

* [**Respecto a Y**]{.hl-yellow}: predictoras con mayor cor lineal son `AGST` (más calor, menos cosechas, sube el precio) y `HarvestRain` (más lluvias, más cosechas, baja el precio, ¡el signo importa!)

* [**Dependencia entre predictoras**]{.hl-yellow}: las variables `Age`, `Year` y `FrancePop` presentan la misma información.


---

## Análisis de dependencia

También podemos usar `corrplot()` del paquete `{corrplot}`, al que le pasamos una matriz de correlaciones clásica y nos la [**visualiza**]{.hl-yellow}.

```{r}
library(corrplot)
datos |> cor() |> corrplot(method = "ellipse")
```

Puedes ver distintas opciones de visualización en <https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html>

---

## Análisis de dependencia

Otra opción es [**visualizar con un scatter plots todas las predictoras vs Y**]{.hl-yellow}, pivotando antes nuestro dataset (solo pivotamos las predictoras)

:::: columns
::: {.column width="65%"}

```{r}
#| code-fold: true
datos_tidy <-
  datos |> pivot_longer(cols = -Price, names_to = "variable",
                        values_to = "values")
ggplot(datos_tidy, aes(x = values, y = Price)) + 
  geom_point(aes(color = variable), alpha = 0.7) +
  geom_smooth(method = "lm") +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal()
```

:::

::: {.column width="35%"}

No solo comprobamos que las rectas con más pendiente son `AGST` y `HarvestRain`, además los puntos parecen poder ajustarse a una recta sin otro patrón identificable.

:::
::::

Esto es importante hacerlo ya que debemos [**descartar posibles correlaciones espúreas**]{.hl-red} (ver ejemplo datasaurus)

---

## Formulación del modelo

Una vez que hemos decidido que dos predictoras usaemos, vamos por tanto a plantear [**dos modelos univariantes**]{.hl-yellow}

$$Price = \beta_0 + \beta_1*AGST + \varepsilon$$
$$Price = \beta_0 + \beta_1*HarvestRain + \varepsilon$$

---

## Fase de estimación


4. [**Fase de estimación**]{.hl-yellow}:
  - ¿Cuánto valen los [**parámetros estimados**]{.hl-purple}? ¿Cómo queda el ajuste?
  - ¿Qué [**interpretación**]{.hl-purple} tienen?


&nbsp;

Para ello ejecutaremos ambos modelos con `lm()`

```{r}
ajuste_AGST <- lm(data = datos, formula = Price ~ AGST)
ajuste_harvest <- lm(data = datos, formula = Price ~ HarvestRain)
```


---

## Fase de estimación

[**Ajuste con AGST**]{.hl-yellow}

```{r}
ajuste_AGST |> summary()
```

* $\beta_0=$ `r round(ajuste_AGST$coefficients[1], 3)`: predicción del precio cuando $AGST = 0$ es de -3 (recuerda que está en escala logartímica)

* $\beta_1=$ `r round(ajuste_AGST$coefficients[2], 3)`: por cada grado de aumento, el precio sube `r round(ajuste_AGST$coefficients[2], 3)` (escala log).

---

## Fase de estimación

[**Ajuste con AGST**]{.hl-yellow}

```{r}
#| echo: false
ajuste_AGST |> summary()
```

* [**Residuales**]{.hl-yellow}: además de media cero, parecen presentar una distribución simétrica con la mediana en torno al cero. Además se tiene que $\widehat{\sigma}_{\varepsilon}^{2}=\frac{1}{n-2}\sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = 0.4819$ (estimador insesgado de la varianza residual) y $R^2 = 0.4456$ (bondad de ajuste)


---

## Fase de estimación

[**Ajuste con harvestRain**]{.hl-yellow}


```{r}
ajuste_harvest |> summary()
```

* $\beta_0=$ `r round(ajuste_harvest$coefficients[1], 3)`: predicción del precio cuando la lluvia fue nula es de 7.679 (recuerda que está en escala logartímica)

* $\beta_1=$ `r round(ajuste_harvest$coefficients[2], 3)`: por cada litro de lluvia, precio baja `r round(1000*ajuste_AGST$coefficients[2], 3)` (escala log).

---

## Fase de estimación

[**Ajuste con harvestRain**]{.hl-yellow}

```{r}
#| echo: false
ajuste_harvest |> summary()
```

* [**Residuales**]{.hl-yellow}: además de media cero, parecen presentar una distribución simétrica con la mediana en torno al cero. Además se tiene que $\widehat{\sigma}_{\varepsilon}^{2}=\frac{1}{n-2}\sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = 0.5577$ (algo más grande que el otro ajuste) y $R^2 = 0.2572$ (algo más pequeño que el otro ajuste) -> de momento es mejor el primer modelo.

---

## Fase de diagnosis

5. [**Fase de diagnosis**]{.hl-yellow} (paquetes `{performance}` y `{olsrr}`):
  - ¿Cumplen los datos las [**hipótesis parámetricas**]{.hl-purple} requeridas para poder hacer inferencia? 
  - ¿Cómo modificar los datos para que se cumplan?
  - Análisis de residuales
  
&nbsp;

Recuerda que [**necesitamos verificar antes las hipótesis**]{.hl-yellow} para poder hacer inferencia con los parámetros, así que vamos a ello con el paquete `{performance}` y el paquete `{olsrr}`

---

## Fase de diagnosis

```{r}
library(performance)
check_model(ajuste_AGST)
```

---

## Diagnosis: linealidad

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es $E \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1 x$


```{r}
#| code-fold: true
check_model(ajuste_AGST)
```

Si te fijas el gráfico que se refiere a ello está [**visualizando residuales vs valores estimados**]{.hl-yellow}: está volviendo a plantear un segundo modelo de regresión donde ahora $\widehat{\varepsilon}_i = \gamma_0 + \gamma_1 \widehat{y}_i$

---

## Diagnosis: linealidad

```{r}
linealidad <- lm(data = tibble("fitted" = ajuste_AGST$fitted.values,
                               "residuals" = ajuste_AGST$residuals),
                 formula = residuals ~ fitted)
linealidad |> summary()
```

Si te fijas [**ambos parámetros no son significativamente distintos de 0**]{.hl-green}: no presentan una tendencia (lineal al menos)

---

## Diagnosis: linealidad

Más adelante probaremos alguna otra cosa pero de momento nos basta con eso. También podemos [**visualizar nosotros ese scatter plot residuales vs estimaciones**]{.hl-yellow}

```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "residuals" = ajuste_AGST$residuals),
       aes(x = fitted, y = residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm") +
  theme_minimal()
```

---

## Diagnosis: homocedasticidad

2. [**Homocedasticidad**]{.hl-green}: necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow}, tal que $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \boldsymbol{X} = x \right] = cte < \infty$.

```{r}
check_heteroscedasticity(ajuste_AGST)
```

. . .

El gráfico titulado `Homogeneity of variance` nos visualiza la raíz cuadrada del valor absoluto de los residuos estandarizados frente a las predicciones (se conoce como [**gráfico de escala-localización**]{.hl-yellow})

---

## Diagnosis: homocedasticidad

Si visualizamos los [**residuales**]{.hl-yellow} deberían estar en torno a 0, dentro de una banda constante (varianza constante)


```{r}
#| code-fold: true
ggplot(tibble("id" = 1:length(ajuste_AGST$residuals),
              "residuals" = ajuste_AGST$residuals),
       aes(x = id, y = residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

---


## Diagnosis: homocedasticidad

Si visualizamos el gráfico de [**escala-localización**]{.hl-yellow} deberíamos obtener un diagrama de dispersión cuya recta de regresión saliese casi plana en torno al 1.


```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "sqrt_std_residuals" = sqrt(abs((ajuste_AGST$residuals - mean(ajuste_AGST$residuals)) / sd(ajuste_AGST$residuals)))),
       aes(x = fitted, y = sqrt_std_residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

Según el gráfico no deberíamos asumir homocedasticidad. **¿Por qué el contraste no la rechaza?**

---

## Diagnosis: homocedasticidad


```{r}
#| code-fold: true
ggplot(tibble("fitted" = ajuste_AGST$fitted.values,
              "sqrt_std_residuals" = sqrt(abs((ajuste_AGST$residuals - mean(ajuste_AGST$residuals)) / sd(ajuste_AGST$residuals)))),
       aes(x = fitted, y = sqrt_std_residuals)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```

Con el poco tamaño muestral que tenemos, es complicado tener evidencias que refuten la hipótesis nula (y el gráfico puede estar parcialmente diseñado). Por eso es la [**hipótesis más difícil de cumplir**]{.hl-yellow}. Lo importante es que en [**la recta de regresión al dibujar los residuos no se aprecia una banda cuya anchura se modifique groseramente**]{.hl-green}, más o menos constante


---

## Diagnosis: normalidad

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$

Con la función `ols_test_normality()` del paquete `{olsrr}` podemos obtener diferentes contrastes de normalidad

```{r}
library(olsrr)
ols_test_normality(ajuste_AGST)
```

Nos centraremos en los contrastes de `Shapiro-Wilk`, `Kolmogorov-Smirnov` y `Anderson-Darling`: [**no se rechaza normalidad**]{.hl-yellow}

---

## Diagnosis: normalidad

Además del contraste podemos visualizar con `stat_qq()` y `stat_qq_line()` el conocido como [**Q-Q plot**]{.hl-yellow}: enfrenta los cuantiles de una muestra con los cuantiles de una normal teórica, teniendo que **obtener los puntos en torno a una recta** (especilamente en el centro).


```{r}
#| code-fold: true
ggplot(tibble("residuals" = ajuste_AGST$residuals)) +
  stat_qq(aes(sample = residuals)) +
  stat_qq_line(aes(sample = residuals))
```


---

## Diagnosis: independencia

4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser independientes entre sí (el error en una observación no depende de otras). En particular, serán **incorrelados**

$${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] - E \left[\varepsilon_i \right] E \left[\varepsilon_j \right] = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$$

```{r}
check_autocorrelation(ajuste_AGST)
```

Por último, `check_autocorrelation()` comprueba como efectivamente los [**residuales/errores son independientes**]{.hl-yellow}, haciendo un test de autocorrelación (nos tiene que salir lo contrario a una serie temporal, que el error i no depende del i-1).

---

## Diagnosis: independencia

Otra forma de verlo es [**visualizando los residuos respecto a su versión con retardo**]{.hl-yellow} (por ejemplo, $\left(\widehat{\varepsilon}_1, \widehat{\varepsilon}_2, \widehat{\varepsilon}_3, \ldots, \widehat{\varepsilon}_{n-1} \right)$ vs $\left(\widehat{\varepsilon}_2, \widehat{\varepsilon}_3, \ldots, \widehat{\varepsilon}_n \right)$ 

```{r}
#| code-fold: true
ggplot(tibble("lag1" = ajuste_AGST$residuals[-length(ajuste_AGST$residuals)],
              "residuals" = ajuste_AGST$residuals[-1]),
       aes(x = residuals, y = lag1)) +
  geom_point(size = 3, alpha = 0.7) + 
  geom_smooth(method = "lm") + 
  theme_minimal()
```

---

## Fase de diagnosis

```{r}
check_model(ajuste_AGST)
```

En nuestro caso se cumplen todas las hipótesis (algunas más fuertemente que otras).

[**Repite el proceso con el otro modelo**]{.hl-yellow}

---

## Fase de diagnosis

```{r}
check_predictions(ajuste_AGST)
```

Nos faltan dos gráficas por comentar:

* `Posterior Predictive Checks`: [**simula distintas variables respuesta**]{.hl-yellow} suponiendo que el modelo fuese cierto (añadiendo ruido aleatorio) y lo compara con la muestra. [**Si lo observado se distancia mucho de las simulaciones**]{.hl-red} es que el modelo planteado no ajusta bien a la muestra.

---

## Fase de diagnosis

```{r}
check_outliers(ajuste_AGST)
```

* `Influential Observations`: nos permite identificar [**observaciones influyentes**]{.hl-yellow}, marcando aquellas (con su id de fila) que se salgan fuera de la banda definida por la conocida como **distancia de Cook** denotada como $D_i$ (realiza, para cada observación, la suma de todos los cambios de la regresión cuando la observación $i$ es retirada: si hay muchos cambios al cambiar una observación, es que era muy influyente)

. . .

Diferencia dos tipos: 

* [**outliers**]{.hl-yellow}: valor atípico de la **respuesta** pudiendo perturbar la varianza residual
* [**high-leverage points**]{.hl-yellow}: valor atípico en alguna de las **predictoras**

---

## Fase de inferencia

6. [**Fase de inferencia**]{.hl-yellow}:
  - ¿Qué [**variabilidad**]{.hl-purple} tienen las estimaciones de nuestro parámetros?
  - ¿Las predictoras/intercepto tienen un [**efecto lineal significativo**]{.hl-purple}?
  - ¿Debemos [**re-entrenar el modelo**]{.hl-purple} sin alguno de ellos?

&nbsp;

Una vez verificadas las hipótesis lo que haremos será [**inferir conclusiones de la población en función de la muestra**]{.hl-yellow}

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Variabilidad**]{.hl-yellow} de las estimaciones de nuestro parámetros

  - $\widehat{SE} \left( \widehat{\beta}_0 \right)$ igual a 2.344 por lo que (aprox) $\widehat{\beta_0} \sim N(-3.547, 2.344)$
  - $\widehat{SE} \left( \widehat{\beta}_1 \right)$ igual a 0.143  por lo que (aprox) que $\widehat{\beta_1} \sim N(0.643, 0.143)$
  

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Estadístico**]{.hl-yellow} del contraste

  - $\frac{\widehat{\beta}_0 - 0}{\widehat{SE} \left( \widehat{\beta}_0 \right)}$ igual a -1.5 (valor que tendrías que buscar en las tablas a mano)
  - $\frac{\widehat{\beta}_1 - 0}{\widehat{SE} \left( \widehat{\beta}_1 \right)}$ igual a 4.483 (valor que tendrías que buscar en las tablas a mano)
  
  
  
---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

* [**Efecto (lineal)**]{.hl-yellow}: si nos fijamos en la tabla, el p-valor de $\beta_0$ es 0.146052. Si adoptamos $\alpha = 0.05$ como suele ser habitual, el contraste $H_0:~\beta_0 = 0$ vs $H_1:~\beta_0 \neq 0$ nos dice que [**no podemos rechazar de forma significativa la hipótesis nula**]{.hl-red} (no sucede con $\beta_1$, si sucediese no habría modelo)

---

## Fase de inferencia

```{r}
ajuste_AGST |> summary()
```

[**¿Y si quitamos dicho parámetro?**]{.hl-yellow}

. . .

Para quitarlo añadimos un -1 al modelo

```{r}
ajuste_AGST_sin_beta0 <- lm(data = datos, formula = Price ~ -1 + AGST)
```
 
---

## Re-aprendiendo

```{r}
ajuste_AGST_sin_beta0 |> summary()
```

* La [**bondad de ajuste**]{.hl-yellow} ha pasado de $R^2 = 0.446$ a $R^2 = 0.9953$

* La [**variabilidad de la estimación**]{.hl-yellow} $\widehat{SE} \left( \widehat{\beta}_1 \right)$  ha pasado de 0.143 a 0.005757.

---

## Comparar modelos

Aunque no hemos hablado en profundidad de las **métricas de evaluación** podemos [**comparar los modelos**]{.hl-yellow} con `compare_performance()` del paquete `{performance}`

```{r}
#| echo: false
ajuste_harvest_sin_beta0 <- lm(data = datos, formula = Price ~ -1 + HarvestRain)
```
```{r}
compare_performance(ajuste_AGST, ajuste_AGST_sin_beta0,
                    ajuste_harvest, ajuste_harvest_sin_beta0)
```

# Clases 9: evaluación y predicción {#clase-9}

[**¿Cómo se puede evaluar un modelo? ¿Qué métricas existen? ¿Cómo predecir?**]{style="color:#444442;"}

---

## Repitamos el proceso


Para interiorizar lo aprendido, vamos a repetir todo el proceso con el conjunto `datos_linearreg.csv` (las variables predictoras representan diferentes variables meteorológicas y la variable objetivo `y` la temperatura media en primavera, para distintas ciudades).

```{r}
datos <- read_csv(file = "./datos/datos_linearreg.csv")
datos
```

---

## Regresión lineal

Debes seguir los siguientes pasos de la manera más detallada posible

1. [**Análisis exploratorio inicial**]{.hl-yellow} tanto numérico como visualizando. ¿Son numéricas sin problemas de codificación? ¿Cómo se distribuyen? ¿Hay datos atípicos?

2. [**Análisis de dependencia**]{.hl-yellow}. ¿Qué predictora correlaciona más con la objetivo? ¿Cómo se relacionan las predictoras entre sí?

3. [**Formulación**]{.hl-yellow} del modelo

4. [**Fase de estimación**]{.hl-yellow}. ¿Cuáles son los parámetros? ¿Cómo se interpretan?

5. [**Fase de diagnosis**]{.hl-yellow}

6. [**Fase de inferencia**]{.hl-yellow}. ¿Qué variabilidad tiene la estimación? ¿Hay efecto significativo? 


---

## Fase de evaluación

```{r}
ajuste_lineal <- lm(data = datos, formula = y ~ x1)
ajuste_lineal |> summary()
```

Teníamos pendiente la fase final: [**fase de evaluación**]{.hl-yellow}

--- 

## Fase de evaluación

7. [**Fase de evaluación**]{.hl-yellow}:
  - ¿Es significativo el modelo? [**ANOVA: análisis de la varianza**]{.hl-purple}
  - ¿Qué información de la predictora explica el modelo? [**Parámetros de bondad de ajuste**]{.hl-purple} ($R^2$ por ejemplo)
  - ¿Qué otras métricas o herramientas podemos usar para [**cuantificar la calidad predictora de nuestro ajuste**]{.hl-purple}
  
. . . 

Una de las herramientas más útiles para evaluar nuestro modelo es [**enfrentar los valores ajustados con los valores reales**]{.hl-yellow} (dado que los conocemos al ser aprendizaje supervisado)

---

## Fase de evaluación


```{r}
#| code-fold: true
ggplot(tibble("y" = datos$y, "y_est" = ajuste_lineal$fitted.values),
       aes(x = y, y = y_est)) +
  geom_point(size = 1.2, alpha = 0.75) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  labs(x = "Valores reales", y = "Valores estimados")
```

En el gráfica podemos ver como los [**valores reales vs estimados**]{.hl-green} están muy cercanos a la diagonal: el error cometido es muy pequeño.

---

## Fase de evaluación

Podemos considerar algunas [**métricas para cuantificar el acierto del modelo**]{.hl-yellow}

. . .

* [**SSE**]{.hl-red} (sum of squared errors): definido como la suma de errores al cuadrado

$$SSE = \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 = \sum_{i=1}^{n} \left( Y_i - \widehat{Y}_i \right)^2$$
Fíjate que dado que $\widehat{\sigma}_{\varepsilon} = \widehat{\sigma}_{r} = \frac{n}{n-p-1} s_{r}^{2} =  \frac{1}{n-p-1} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2$ (el que `R` llama `Residual standard error`), tenemos que $SSE = (n-p-1)\widehat{\sigma}_{\varepsilon}$

. . .

* [**MSE**]{.hl-red}: media de lo anterior $MSE = s_{r}^{2} = \frac{1}{n} SSE  = \frac{1}{n} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 = \frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}$

. . .

Ambas métricas nos hablan de la [**varianza del error**]{.hl-red}, es decir, de lo que el [**modelo no es capaz de explicar**]{.hl-red}

---


## Fase de evaluación



* [**SSR**]{.hl-green} (regressions sum of squares): definido como la suma de las desviaciones de cada predicción a su media (al tener estimadores insesgados, la media de las estimaciones $\overline{\widehat{Y}}$ es la misma que la de la variable a estimar $\overline{Y}$)

$$SSR = \sum_{i=1}^{n} \left( \overline{Y} - \widehat{Y}_i \right)^2 = \sum_{i=1}^{n} \left( \overline{\widehat{Y}}_i - \widehat{Y}_i \right)^2$$

. . .


* [**MSR**]{.hl-green}: media de lo anterior $MSR = s_{\widehat{y}}^2 = \frac{1}{n} SSR  = \frac{1}{n} \sum_{i=1}^{n} \left( \overline{Y} - \widehat{Y}_i \right)^2$

. . .

Ambas métricas nos hablan de la [**variación de Y en torno a la regresión**]{.hl-green}, es decir, la variación de $\overline{Y}$ que es explicada por la media condicional estimada $(Y_i|X=x_i) \sim \widehat{\beta}_0 + \widehat{\beta}_1 X_i$, cuantifica la [**información de Y explicada por el modelo**]{.hl-green}

---

## Fase de evaluación


* [**SST**]{.hl-yellow} (total sum of squares): definido como la suma de las desviaciones de la variable objetivo $Y$ a su media.

$$SST =  \sum_{i=1}^{n} \left( Y_i - \overline{Y} \right)^2 $$

. . .

* [**Varianza muestral de Y**]{.hl-yellow}: la media de lo anterior (la varianza de $Y$)

$$s_{y}^2= \frac{1}{n} SST  = \frac{1}{n}\sum_{i=1}^{n} \left( Y_i - \overline{Y} \right)^2$$



. . .


Ambas métricas nos hablan de la [**variación total de Y**]{.hl-yellow}, es decir, la [**información total de nuestra variable objetivo**]{.hl-yellow}

---

## ANOVA

Así tenemos 3 tipos de métricas:

* `SST` y $s_{y}^2$: el [**total de info a explicar**]{.hl-yellow}

* `SSR` y $MSR$: el [**total de info explicada por el modelo**]{.hl-green}

* `SSE` y $MSE$: el [**total de info NO explicada por el modelo**]{.hl-red} (a veces se usa la raíz cuadrada del MSE, conocido como $RMSE$, o el $MAE$, tomando valor absoluto en los errores).

. . .

Se pueden demostrar que, [**SOLO EN EL CASO LINEAL**]{.hl-purple}, desarrollando el sumatorio $SST =  \sum_{i=1}^{n} \left( Y_i - \overline{Y} \right)^2 = \sum_{i=1}^{n} \left( \left(Y_i - \widehat{Y}_i \right) +  \left(  \widehat{Y}_i - \overline{Y} \right) \right)^2$ se llega a que

[**SST**]{.hl-yellow} =  [**SSR**]{.hl-green} + [**SSE**]{.hl-red}

---


## ANOVA

[**SST**]{.hl-yellow} =  [**SSR**]{.hl-green} + [**SSE**]{.hl-red}


. . .

[$s_{y^2}$]{.hl-yellow} = [$s_{\widehat{y}}^2$]{.hl-green} + [$s_{r}^2$]{.hl-red} (equivalente, [$s_{y^2}$]{.hl-yellow} = [$s_{\widehat{y}}^2$]{.hl-green} + [$\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}$]{.hl-red})

. . .

Esta decomposición (solo se cumple en el caso lineal) se conoce como [**ANOVA o análisis de la varianza**]{.hl-yellow} y podemos hacerlo en `R` con `aov()` o `anova()`

:::: columns
::: {.column width="45%"}

```{r}
ajuste_lineal |> aov()
```

:::

::: {.column width="55%"}

```{r}
ajuste_lineal |> anova()
```

:::
::::

---

## ANOVA

```{r}
ajuste_lineal |> aov()
```

| Terms | x1 (predictora) | Residuals |
|:---------:|:-----:|:------:|
| Sum of Squares  | SSR   |    SSE | 
| Deg. of Freedom  (grados libertad)  | p  |  n - p - 1 |

&nbsp;

`Residual standard error`: $\widehat{\sigma}_{\varepsilon}^{2}= \frac{n}{n-p-1} s_{r}^{2} = \frac{1}{n-p-1} \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2}$

---

## ANOVA

```{r}
ajuste_lineal |> anova()
```

|  | Df (grados) | Sum Sq | Mean Sq | F value | Pr(>F) |
|:---------:|:-----:|:------:|:------:|:------:|:------:|
| x1  | p  | SSR | $\frac{SSR}{p}$ | $F-value = \frac{\frac{SSR}{p}}{\frac{SSE}{n-p-1}}$ | p-valor F test
| Residuals  | n - p - 1 | SSE | $\widehat{\sigma}_{\varepsilon}^{2} = \frac{SSE}{n-p-1}$ | | |

El F-value es el estadítico  $F = \frac{\frac{SSR}{p}}{\frac{SSE}{n-p-1}} \sim F_{p, n-p-1}$ asociado al [**contraste de significación global**]{.hl-yellow}

$$H_0:~\beta_1 = \ldots = \beta_p = 0 \quad vs \quad H_1:~\text{existe al menos un} \quad \beta_j \neq 0$$

---

## ANOVA

```{r}
ajuste_lineal |> anova()
```


$$H_0:~\beta_1 = \ldots = \beta_p = 0 \quad vs \quad H_1:~\text{existe al menos un} \quad \beta_j \neq 0$$

El contraste pretende responder a: [**¿existe una dependencia lineal entre $Y$ y el CONJUNTO de predictoras?**]{.hl-yellow} (global, no parámetro a parámetro).

. . .

Si se [**rechaza**]{.hl-yellow} significa que [**existe al menos un predictor cuyo efecto LINEAL sobre Y es significativo**]{.hl-yellow}.

. . .

[**Importante**]{.hl-red}: en el caso de la reg. lineal univariante, $F-value$ y $p-value$ del ANOVA es equivalente al $t-value$ y $p-value$ del contraste de significación para $\beta_1$ (ya que...no hay más).

---

## Bondad de ajuste

El conocido como $R^2$ o [**bondad de ajuste**]{.hl-yellow} o el [**coeficiente de determinación**]{.hl-yellow} es una forma sencilla, y relacionada con el ANOVA, para cuantificar la [**cantidad de información explicada por el modelo**]{.hl-yellow}

. . .

De hecho es, literal, un [**ratio de información explicada**]{.hl-yellow} (lo que `R` llama `Multiple R-squared`, ya hablaremos de su versión ajustada en regresión multivariante)

$$R^2 = \text{Ratio info explicada} = \frac{SSR}{SST}$$

. . .

En el caso lineal, por lo visto en el ANOVA

$$R^2= \frac{SSR}{SST} = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST} = 1 - \frac{s_{r}^{2}}{s_{y}^{2}} =  1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^{2}}{s_{y}^{2}}$$

. . .

Por definición, la [**bondad de ajuste está entre 0 y 1**]{.hl-yellow}.

---

## Bondad de ajuste

[**Deberes: demuestra que $R^2 = r_{y \widehat{y}}^2$ y que en el caso de $p=1$ coincide además con $R^2 = r_{x,y}^2$**]{.hl-yellow}




$$R^2 = r_{y \widehat{y}}^2 =_{p=1} r_{x,y}^2$$

. . .

La bondad de ajuste tiene un [**problema importante**]{.hl-red}: no solo depende del modelo sino que también de los datos. ¿De qué depende? ¿Por qué es [**peligroso usar ciegamente $R^2$ para decidir**]{.hl-red}?



---

## Bondad de ajuste

$$R^2= \frac{SSR}{SST} = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST} = 1 - \frac{s_{r}^{2}}{s_{y}^{2}} =  1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^{2}}{s_{y}^{2}}$$

¿De qué depende?

. . .

* [**Más predictoras**]{.hl-yellow} implica que [**$R^2$ crece**]{.hl-yellow}, ¡incluso aunque dichas [**predictoras no sean significativas**]{.hl-red}! (esto lo arreglaremos con el $R^2$ ajustado en el futuro)

* [**Más ruido, menor $R^2$**]{.hl-yellow}

* [**Ignora si el modelo cumple las hipótesis**]{.hl-yellow}: un modelo con un alto $R^2$ puede dar predicciones nefastas.
  
  
---

## Predictoras vs R2

Vamos a repetir pero [**añadiendo 5 y 22 variables más**]{.hl-yellow}, sin efecto lineal con $Y$.

```{r}
datos_extras <-
  tibble("y" = datos$y, "x1" = datos$x1, "x2" = rnorm(1e3), "x3" = rnorm(1e3),
         "x4" = rnorm(1e3), "x5" = rnorm(1e3), "x6" = rnorm(1e3), "x7" = rnorm(1e3),
         "x8" = rnorm(1e3), "x9" = rnorm(1e3), "x10" = rnorm(1e3), "x11" = rnorm(1e3),
         "x12" = rnorm(1e3), "x13" = rnorm(1e3), "x14" = rnorm(1e3), "x15" = rnorm(1e3),
         "x16" = rnorm(1e3), "x17" = rnorm(1e3), "x18" = rnorm(1e3), "x19" = rnorm(1e3),
         "x20" = rnorm(1e3), "x21" = rnorm(1e3), "x22" = rnorm(1e3), "x23" = rnorm(1e3))
ajuste_6pred <- lm(data = datos_extras, formula = y ~ x1 + x2 + x3 + x4 + x5 + x6)
ajuste_23pred <- lm(data = datos_extras, formula = y ~ .)
compare_performance(ajuste_lineal, ajuste_6pred, ajuste_23pred)
```

Con `compare_performance()` podemos comparar métricas de modelos.

---

## Ruido vs R2

Hemos dicho que el ruido afecta...¿qué crees que **pasará si fijamos la parte determinística y solo modificamos el ruido**? Vamos a [**simular 6 modelos**]{.hl-yellow}, con exactamente los mismos $\beta_0$ y $\beta_1$ (es decir, **mismo ajuste**) pero con diferente varianza en el ruido (supongamos que $X \sim N(3, 1.5)$).

$$\text{Modelo 1: } Y = -1.2 + 3.2X + N(0, 0.25)$$
$$\text{Modelo 2: } Y = -1.2 + 3.2X + N(0, 1)$$

$$\text{Modelo 3: } Y = -1.2 + 3.2X + N(0, 1.5)$$

$$\text{Modelo 4: } Y = -1.2 + 3.2X + N(0, 2)$$

$$\text{Modelo 5: } Y = -1.2 + 3.2X + N(0, 4)$$


$$\text{Modelo 6: } Y = -1.2 + 3.2X + N(0, 8)$$

---

## Ruido vs R2


```{r}
#| code-fold: true
x <- rnorm(n = 1000, mean = 3, sd = 1.5)
eps1 <- rnorm(n = 1000, mean = 0, sd = 0.25)
eps2 <- rnorm(n = 1000, mean = 0, sd = 1)
eps3 <- rnorm(n = 1000, mean = 0, sd = 1.5)
eps4 <- rnorm(n = 1000, mean = 0, sd = 2)
eps5 <- rnorm(n = 1000, mean = 0, sd = 4)
eps6 <- rnorm(n = 1000, mean = 0, sd = 8)
datos <- tibble("x" = x, "y1" = -1.2 + 3.2*x + eps1,
                "y2" = -1.2 + 3.2*x + eps2, "y3" = -1.2 + 3.2*x + eps3,
                "y4" = -1.2 + 3.2*x + eps4, "y5" = -1.2 + 3.2*x + eps5,
                "y6" = -1.2 + 3.2*x + eps6)
ajuste_1 <- lm(data = datos, formula = y1 ~ x)
ajuste_2 <- lm(data = datos, formula = y2 ~ x)
ajuste_3 <- lm(data = datos, formula = y3 ~ x)
ajuste_4 <- lm(data = datos, formula = y4 ~ x)
ajuste_5 <- lm(data = datos, formula = y5 ~ x)
ajuste_6 <- lm(data = datos, formula = y6 ~ x)
```

```{r}
compare_performance(ajuste_1, ajuste_2, ajuste_3, ajuste_4, ajuste_5, ajuste_6)
```

---

## Ruido vs R2

En todos los casos el [**ajuste debe ser (aprox) el mismo**]{.hl-yellow} ya que el modelo de regresión busca [**modelizar los efectos lineales no aleatorios**]{.hl-yellow} entre la variable objetivo y los predictores.

. . .

[**Moraleja**]{.hl-green}: tener un $R^2$ no implica que el modelo sea malo, ya que la [**cantidad de información no modelizable**]{.hl-yellow} puede deberse a una [**cantidad alta de ruido (algo aleatorio no predecible)**]{.hl-red}. Por ello es importante usar más herramientas que un mero coeficiente para valorar un ajuste (por ejemplo, en campos como la sociología o la economía la bondad de ajuste será generalmente bajo)

. . .

&nbsp;

[**Deberes**]{.hl-yellow}: ¿cómo ilustrar gráficamente que a mayor varianza del ruido, menor es $R^2$? Diseña un estudio de simulación para ello con distintos modelos y gráfica la caída de $R^2$.

---

## Diagnosis vs R2

Entonces, si tenemos un modelo con un alto $R^2$, [**¿no hace falta que cumpla las hipótesis?**]{.hl-yellow}

. . .

Vamos a simular un modelo que [**incumple**]{.hl-red}

* Linealidad

* Homocedasticidad

. . .

La variable predictora $x_i = 0.01 + 0.01*(i-1)$ ($i = 1,\ldots, n = 200$) será la siguiente

```{r}
x <- seq(0.01, 2, l = 200)
```


[**¿Cómo podríamos crear una $Y$ cuya relación con $X$ sea no lineal?**]{.hl-yellow}

---

## Diagnosis vs R2

[**¿Cómo podríamos crear una $Y$ cuya relación con $X$ sea no lineal?**]{.hl-yellow}

Tenemos muchas maneras, por ejemplo:

$$Y = X + X^2 + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

$$Y = \log(X^2) - cos(X) + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

$$Y = \frac{1}{X + 1} + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

$$Y = 1 - 2X(1 + 0.25 \sin(4 \pi X)) + \varepsilon, \quad \varepsilon \sim N(0, \sigma_{\varepsilon})$$

. . .

Simula este último modelo (con $\sigma_{\varepsilon} = 0.5$) y realiza el ajuste

---

## Diagnosis vs R2

```{r}
x <- seq(0.15, 2, l = 200)
eps <- rnorm(n = 200, sd = 0.5)
y <- 1 - 2 * x * (1 + 0.25*sin(4 * pi * x)) + eps
datos <- tibble("y" = y, "x" = x)
ajuste_lineal <- lm(data = datos, formula = y ~ x)
ajuste_lineal |> summary()
```

. . .

[**¿Cómo podríamos hacer que la hipótesis de homocedasticidad no se cumpla**]{.hl-yellow}


---

## Diagnosis vs R2

Vamos a considerar que la [**varianza del error no es cte, crece según aumenta x**]{.hl-yellow}

$$y_i = f(x_i) + \varepsilon_i, \quad \varepsilon_i \sim N(0, \sigma_{\varepsilon,i}), \quad \sigma_{\varepsilon,i} = 0.25 * x_{i}^2 \quad i=1, \ldots, 200$$

```{r}
#| code-fold: true
x <- seq(0.01, 2, l = 200)
eps <- rnorm(n = 200, sd = 0.25 * x^2)
y <- 1 - 2 * x * (1 + 0.25*sin(4 * pi * x)) + eps
datos <- tibble("y" = y, "x" = x, "sigma" =  0.25 * x^2)
ajuste_lineal <- lm(data = datos, formula = y ~ x)
ajuste_lineal |> summary()
```

---

## Diagnosis vs R2

```{r}
#| code-fold: true
ggplot(datos, aes(x = x, y = y)) +
  geom_point(aes(color = sigma), size = 3, alpha = 0.75) +
  geom_smooth(method = "lm") +
  theme_minimal()
```

Aunque el $R^2$ es bastante alto, el modelo no tiene sentido, dando [**estimaciones cada vez más erradas**]{.hl-red} según nos movemos hacia la derecha en el eje X.


---

## Cosicas a examen

Algunas cosas que podrían caer en la primera entrega:

. . .

* Realizar un [**análisis exploratorio inicial**]{.hl-yellow} lo más completo posible. Elegir de manera justificada [**la mejor predictora**]{.hl-yellow}

. . .

* Saber [**ajustar e INTERPRETAR**]{.hl-yellow} un modelo de regresión.

. . .

* Realizar e interpretar la [**fase de diagnosis**]{.hl-yellow} lo más completa posible para luego interpretar (si procede) la [**inferencia**]{.hl-yellow} del modelo.

. . .

* Saber hacer una [**correcta fase de evaluación**]{.hl-yellow} dle modelo.

. . .

* Saber [**simular modelos de regresión**]{.hl-yellow} (tanto que cumplan las hipótesis como que no las cumplan).


# Clase 10: predicción y resumen {#clase-10}

[**Fase de predicción. Resumen**]{style="color:#444442;"}


---

## Resumen

Un breve resumen de lo aprendido sobre reg. lineal univariante

* [**Modelo supervisado de predicción**]{.hl-yellow}: hay una variable objetivo $Y$ continua (numérica) cuyo valor real conocemos

. . .

* [**La visualización importa**]{.hl-yellow}: no fies tu análisis solo a los parámetros matemáticos, la visualización ayuda a entender los datos.

. . .

* [**Relación entre variables**]{.hl-yellow}: buscamos predictoras muy correladas (linealmente) con $Y$ y lo más incorreladas/independientes entre ellas.

. . .

* [**Modelo**]{.hl-yellow}: el modelo parámetrico se resume en $Y = \beta_0 + \beta_1 X + \varepsilon$, donde $\varepsilon$ es una variable aleatoria (ruido).


. . .

* [**Estimación**]{.hl-yellow}: la estimación viene modelizada bajo la hipótesis de linealidad

$$E[Y|X=x] = \beta_0 + \beta_1 x$$




# Clase 11: entrega I {#clase-11}


[**Entrega I**]{style="color:#444442;"}

# Clase 12: incumpliendo hipótesis {#clase-12}

[**¿Cómo simular datos que incumplan las hipótesis?**]{style="color:#444442;"}

---

## Incumpliendo hipótesis

Los contenidos vistos en esta clase se subirán resumidos en formato notebook.


# Clase 13: reg. multivariante {#clase-13}

[**Introducción a la regresión multivariante. Formulación del modelo y estimación**]{style="color:#444442;"}

## Formulación multivariante

De aquí en adelante llamaremos [**modelo multivariante**]{.hl-yellow} a todo modelo en el que $p > 1$ (es decir, tenemos más de una variable predictora).


$$Y = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad E \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right)$$
tal que $E \left[ \varepsilon | \left( X_1 = x_1, \ldots, X_p = x_p \right) \right] = 0$.

. . .

En el caso del [**modelo lineal multivariante**]{.hl-yellow} se traducirá por tanto en


$$E \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p, \quad \widehat{Y} = \widehat{\beta_0} + \displaystyle \sum_{j=1}^{p} \widehat{\beta_j} X_j $$

El objetivo seguirá siendo obtener la estimación de los $\widehat{\beta}$ tal que [**minimicemos el error**]{.hl-yellow}

---

## Formulación matricial


Su formulación muestral la podemos expresar mediante la [**matriz de diseño**]{.hl-yellow}

$$\mathbf{X} =\begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}$$

tal que $Y_i = \beta_0 + \sum_{j=1}^{p} \beta_j X_j + \varepsilon_i$, para todo elemento de la muestra $i=1,\ldots, n$


$$ \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix}_{(p+1)\times1} + \begin{pmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n \end{pmatrix}_{n\times1}$$


---

## Formulación matricial

La estimación será por tanto


$$\widehat{\mathbf{Y}} = \begin{pmatrix} \widehat{Y}_1 \\ \widehat{Y}_2 \\ \vdots \\ \widehat{Y}_n \end{pmatrix}_{n\times1} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \begin{pmatrix} 1 & X_{11} & \ldots & X_{1p} \\ 1 & X_{21} & \ldots & X_{2p} \\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \ldots & X_{np}
\end{pmatrix}_{n\times(p+1)}\begin{pmatrix} \widehat{\beta}_1 \\ \widehat{\beta}_2 \\ \vdots \\ \widehat{\beta}_p \end{pmatrix}_{(p+1)\times1}$$


---

## Varianza residual

Como sucedía antes, el objetivo será minimizar la [**varianza residual o error cuadrático medio**]{.hl-yellow} (varianza del error), o lo que hemos llamado $SSE$

$$SSE \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \displaystyle \sum_{i=1}^{n} \left(Y_i - \beta_0 - \beta_1 X_{i1} - \ldots - \beta_p X_{ip} \right)^2 $$
¿Cómo quedaría **matricialmente**?

. . .

$$SSE \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = \varepsilon^{T} \varepsilon$$

Así, aplicando el [**método de los mínimos cuadrados**]{.hl-yellow}, los [**parámetros óptimos**]{.hl-yellow} serán aquellos $\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p\right)$ que minimicen dicha suma

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p \right) = \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)$$


Y repetimos la idea: [**calcularemos la derivada respecto a $\widehat{\boldsymbol{\beta}}$ e igualamos a cero**]{.hl-yellow}

--- 


## Método mínimos cuadrados

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1, \ldots, \widehat{\beta}_p \right) = \arg \min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)$$

Derivando matricialmente tenemos (recuerda: $\left(A B \right)^{T} = B^{T} A^{T}$)

$$\begin{eqnarray}\frac{\partial SSE \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=& \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} \nonumber \\  &=& \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} + \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \frac{\partial \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}}{\partial \boldsymbol{\beta}} \nonumber \\  &=& -\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T} \mathbf{X} - \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \mathbf{X}^{T} \nonumber \\  &=& -\mathbf{X}^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)  - \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) \mathbf{X}^{T} \nonumber \\ &=& -2\mathbf{X}^{T} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = 0 \end{eqnarray}$$

---

## Método mínimos cuadrados


$$\begin{eqnarray}\frac{\partial SSE \left( \boldsymbol{\beta} \right)}{\partial \boldsymbol{\beta}} &=&  -2\mathbf{X}^{T} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = 0 \end{eqnarray}$$

Si descartamos el $-2$ como constante, tenemos que

$$\mathbf{X}^{T} \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) = 0 \longrightarrow \mathbf{X}^{T}  \mathbf{Y} = \mathbf{X}^{T} \mathbf{X} \boldsymbol{\beta}$$

Si ahora despejamos $\beta$  multiplicando en ambos lados por $\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}$ tenemos finalmente

$$\widehat{\boldsymbol{\beta}}=\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$$

---

## Estimación

Fíjate que cuando $p = 1$ tenemos que

$$\widehat{\mathbf{Y}} = \begin{pmatrix} \widehat{Y}_1 \\ \vdots \\ \widehat{Y}_n \end{pmatrix}_{n\times1} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \begin{pmatrix} 1 & X_{11} \\\vdots & \vdots \\ 1 & X_{n1} \end{pmatrix}_{n \times 2}\begin{pmatrix} \widehat{\beta}_0 \\ \widehat{\beta}_1 \end{pmatrix}_{2\times1}$$

y que por tanto $\widehat{\boldsymbol{\beta}}= \left(\begin{pmatrix} 1 & \ldots & 1 \\ X_{11}  & \ldots & X_{n1} \end{pmatrix} \begin{pmatrix} 1 & X_{11} \\ 1 & X_{21}  \\ \vdots & \vdots \\ 1 & X_{n1} \end{pmatrix} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$

[**Deberes**]{.hl-red}: sería interesante que para el examen supieses demostrar que cuando $p=1$ esa expresión acaba en 

$$\widehat{\beta}_1  = \frac{s_{xy}}{s_{x}^{2}}, \quad \widehat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$$

---

## Estimación

$$\widehat{\boldsymbol{\beta}}=\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$$

* La [**estimación**]{.hl-yellow} será por tanto

$$\widehat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\beta}}=\mathbf{X}\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y} = \mathbf{H}\mathbf{Y}$$

* La matriz $\mathbf{H} = \mathbf{X}\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}$ se conoce como [**hat matrix o matriz de proyección**]{.hl-yellow} (ya que hace que las estimaciones $\hat{y}$ sean en realidad los valores $y$ proyectados verticalmente sobre el plano de regresión ajustado).



---

## Interpretación

Al igual que pasaba antes será importante [**distinguir lo población de lo muestral**]{.hl-yellow}

* Los parámetros $\left( \beta_0, \beta_1, \ldots, \beta_p \right)$ son desconocidos, los [**parámetros poblacionales**]{.hl-yellow}

* Los parámetros $\left( \widehat{\beta}_0, \widehat{\beta}_1 , \ldots, \widehat{\beta}_p\right)$ son estimados a partir de los datos, son [**variables aleatorias**]{.hl-yellow} ya que han sido calculados en función de una muestra aleatoria.

&nbsp;

[**¿Cómo se interpretan ahora los parámetros?**]{.hl-yellow}

---

## Interpretación

$$\widehat{\mathbf{Y}} = \mathbf{X}\widehat{\boldsymbol{\beta}}= \widehat{\beta}_0 + \sum_{j=1}^{p} \widehat{\beta}_j X_j$$

* [**Ordenada en el origen**]{.hl-yellow}: también llamado **intercepto**, y denotado como $\beta_0$ su valor real, es el valor de $Y$ cuando $X_1 = \ldots = X_p = 0$. Es decir, $\widehat{\beta}_0$ se puede interpretar como la estimación $\widehat{Y}$ cuando [**TODAS las predictoras son nulas**]{.hl-purple}


. . .

* [**Pendientes**]{.hl-yellow}: denotadas como $\beta_j$, para $j=1,\ldots, p$, su valor real, cuantifica el incremento de $Y$ cuando solo $X_j$ aumenta una unidad. Es decir, $\widehat{\beta}_j$ se puede interpretar como la variación de la estimación $\widehat{Y}$ cuando [**$X_j$ tiene un incremento unitario y el RESTO DE PREDICTORAS permanecen fijas**]{.hl-purple}.

---

## Diagnosis

En el caso multivariante las [**4 hipótesis**]{.hl-yellow} se convierten en

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es 

$$E \left[Y | \left( \boldsymbol{X}_1 = x_1, \ldots, \boldsymbol{X}_p = x_p \right) \right] = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$$

2. [**Homocedasticidad**]{.hl-green}: necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow} $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \left( \boldsymbol{X}_1 = x_1, \ldots, \boldsymbol{X}_p = x_p \right) \right] = cte < \infty$

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$


4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser en independientes, y en particular **incorrelados** ${\rm Cor}_{\varepsilon_i \varepsilon_j} = E \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$

$$Y | \left( \boldsymbol{X}_1 = x_1, \ldots, \boldsymbol{X}_p = x_p \right) \sim N \left(\beta_0 + \beta_1x_1 + \ldots + \beta_p x_p, \sigma_{\varepsilon}^2 \right)$$


---

## Inferencia 

Ahora las hipótesis nos permiten decir  que los [**parámetros estimados siguen una distribución normal multivariante**]{.hl-yellow} de [**media el vector de parámetros**]{.hl-purple}  a estimar y de [**matriz de covarianzas la varianza residual por $\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}$**]{.hl-purple}

$$\widehat{\boldsymbol{\beta}} \sim N_{p+1}\left(\boldsymbol{\beta}, \sigma_{\varepsilon}^2 \left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}\right)$$

. . .

Esa normal multivariante, componente a componente, deriva en

$$\widehat{\beta}_j\sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad \frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)} \sim N \left(0, 1 \right), \quad \frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} \sim t_{n-p-1}$$

donde $SE \left(\widehat{\beta}_j\right)^2 = \sigma_{\varepsilon}^2 v_j$ y $\widehat{SE} \left(\widehat{\beta}_j\right)^2 = \widehat{\sigma}_{\varepsilon}^2 v_j$, con $v_j$ el elemento $j$-ésimo de la diagonal de $\left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}$ (matriz que representa la **variabilidad de predictores**)


---

## Caso práctico: wine.csv

Vamos a volver a usar nuestros datos `wine.csv`

```{r}
datos <- read_csv(file = "./datos/wine.csv")
datos
```

Pero esta vez no vamos a seleccionar ninguna variable previamente. Para [**ajustar un modelo multivariante**]{.hl-yellow} basta con añadir variables `+`

```{r}
ajuste_uni <- lm(data = datos, formula = Price ~ AGST)
ajuste_2 <- lm(data = datos, formula = Price ~ AGST + HarvestRain)
ajuste_full <- lm(data = datos, formula = Price ~ .)
```


---

## Caso práctico: wine.csv

Fíjate que ahora la tabla `coefficients` tiene una [**línea por covariable**]{.hl-yellow} (más $\beta_0$) y además en este caso dice `(1 not defined because of singularities)`: la [**matriz de covarianzas no es invertible**]{.hl-red} ya que el [**determinante es 0**]{.hl-red} (en este caso `Year` es "igual" que `Age`)

```{r}
ajuste_full |> summary()
```

---

## Caso práctico: wine.csv

Una vez eliminada, en la última línea `F-statistic: ... p-value: 2.232e-07`: se está [**rechazando la hipótesis nula**]{.hl-green} del contraste de significación global (existe [**alguna predictora cuyo efecto lineal es significativo**]{.hl-green}). Recuerda que la nula es $H_0:~\beta_1 = \ldots = \beta_5 = 0$


```{r}
ajuste_full <- lm(data = datos |> select(-Age), formula = Price ~ .)
ajuste_full |> summary()
```

---

## Caso práctico: wine.csv

Si nos fijamos en la tabla de coeficientes el [**modelo ajustado**]{.hl-yellow} es

$$\begin{eqnarray}\widehat{Precio} &=& 2.496 - 0.0137 * Year + 0.0012 * WinterRain + 0.614 * AGST \nonumber \\ &-& 0.0038 * HarvestRain - 0.00002 * FrancePop \nonumber \end{eqnarray}$$

```{r}
ajuste_full |> summary()
```

---

## Caso práctico: wine.csv

$$\begin{eqnarray}\widehat{Precio} &=& 2.496 - 0.0137 * Year + 0.0012 * WinterRain + 0.614 * AGST \nonumber \\ &-& 0.0038 * HarvestRain - 0.00002 * FrancePop \nonumber \end{eqnarray}$$

La [**interpretación**]{.hl_yellow} es

* La estimación del modelo cuando todas las covariables son 0 (sin población, sin lluvia ni temperatura, año 0) es de 2.496 (escala log)

* Por ejemplo, si el resto de variables permaneciesen fijas, por cada litro de lluvia que caiga de más durante la cosecha (agosto-septiembre), el modelo estima que el precio baja 3.8 (escala log)

---

## Caso práctico: wine.csv

Si nos fijamos en la tabla de coeficientes tenemos **2 predictoras cuyo efecto lineal no se acepta que sea significativo**: `Year`, `FrancePop` (además de $\beta_0$) ya que el contraste de significación $H_0:~\beta_j = 0$ vs $H_1:~\beta_j \neq 0$ nos devuelve un $p-valor > \alpha$

```{r}
ajuste_full |> summary()
```

---

## Caso práctico: wine.csv

La pregunta que intentaremos resolver en futuras clases es:

[**¿Cómo decidir que predictoras seleccionamos?**]{.hl-yellow} El problema si quitamos todas las no significativas de manera simultánea es que no sabemos qué **efectos puede haber entre las propias predictoras**

```{r}
ajuste_full |> summary()
```

---

## Caso práctico: wine.csv

Fíjate que ahora en la [**fase de diagnosis**]{.hl-yellow} tenemos una **sexta gráfica a chequear**: una gráfica que nos calcula la conocida como `VIF` (Variance Inflaction), que nos [**cuantifica la colinealidad (efectos lineales) entre las predictoras**]{.hl-yellow}

```{r}
check_model(ajuste_full)
```

---

## ANOVA

Ahora `anova()` en realidad nos hace una [**ANOVA secuencial**]{.hl-yellow} (variable a variable)


```{r}
ajuste_full |> anova()
```

---

## ANOVA

Ahora `anova()` en realidad nos hace una [**ANOVA secuencial**]{.hl-yellow} (variable a variable)


|  | Df (grados) | Sum Sq | Mean Sq | F value | Pr(>F) |
|:---------:|:-----:|:------:|:------:|:------:|:------:|
| $x_1$  | 1  | $SSR_1$ | $\frac{SSR_1}{1}$ | $F-value = \frac{\frac{SSR_1}{1}}{\frac{SSE}{n-p-1}}$ | $p_1$
| ...  | ...  | ... | ... | $F-value = \frac{\frac{SSR}{p}}{\frac{SSE}{n-p-1}}$ | p-valor F test
| $x_p$  | 1  | $SSR_p$ | $\frac{SSR_p}{1}$ | $F-value = \frac{\frac{SSR_p}{1}}{\frac{SSE}{n-p-1}}$ | $p_p$
| Residuals  | n - p - 1 | SSE | $\widehat{\sigma}_{\varepsilon}^{2} = \frac{SSE}{n-p-1}$ | | |


---

## ANOVA


```{r}
ajuste_full |> anova()
```


Ahora $SSR_j$, con $j=1,\ldots,p$, representa la **suma de residuos al cuadrado** $SSR$ cuando incluimos la predictora j-ésima $X_j$, frente a cuando no lo hacemos, tal que

$$SSR_j = SSR \left(X_1, \ldots, X_j \right) - SSR \left(X_1, \ldots, X_{j-1} \right)$$

Así los p-valores $p_j$ son individuales (los mismos que los de la tabla de coeficientes).



# Clase 14: selección de modelos {#clase-14}

[**Selección secuencial de modelos**]{style="color:#444442;"}

---

## Bondad de ajuste

Como ya hemos hablado, el conocido como $R^2$ o [**bondad de ajuste**]{.hl-yellow} o el [**coeficiente de determinación**]{.hl-yellow} es una forma sencilla, y relacionada con el ANOVA, para cuantificar la [**cantidad de información explicada por el modelo**]{.hl-yellow} definida como

$$R^2 = \text{Ratio info explicada} = \frac{SSR}{SST} =_{lineal} 1 - \frac{SSE}{SST} = 1 - \frac{s_{r}^{2}}{s_{y}^{2}} =  1 - \frac{\frac{n-p-1}{n}\widehat{\sigma}_{\varepsilon}^{2}}{s_{y}^{2}}$$


Por definición, la [**bondad de ajuste está entre 0 y 1**]{.hl-yellow} pero además [**presenta un problema en el caso multivariante**]{.hl_red}


* [**Más predictoras**]{.hl-yellow} implica que [**$R^2$ crece**]{.hl-yellow} (al aumentar $p$), ¡incluso aunque dichas [**predictoras no sean significativas**]{.hl-red}!

---
  
## R2 ajustado

Para evitar dicho problema vamos a definir la conocida como [**bondad de ajuste ajustada**]{.hl-yellow} 

$$R_{adj}^2 = 1 - \frac{\frac{SSE}{n-p-1}}{\frac{SST}{n-1}} = 1 - \frac{SSE}{SST}\frac{n-1}{n-p-1}  = 1 - \frac{\widehat{\sigma}_{\varepsilon}^{2}}{SST}(n-1)$$


. . .

Así el $R_{adj}^2$ [**no depende del número de predictoras de manera directa**]{.hl-green} (si sigue dependiendo del ruido, de manera que descenderá solo cuando incrementar $p$ implica reducir el error, es decir, variables con un efecto significativo). Se cumple además que que si $p=1$

$$\lim_{n\to \infty} R_{adj}^2 = \lim_{n\to \infty} \left(1 - \frac{\widehat{\sigma}_{\varepsilon}^{2}}{SST}(n-1) \right) = \lim_{n\to \infty} \left(1 - \frac{\widehat{\sigma}_{\varepsilon}^{2}}{SST}(n-p-1) \right) = R^2$$


---

## R2 ajustado

Para ver mejor su diferencia debes realizar el siguiente [**estudio de simulación**]{.hl-yellow} (si quieres fija semilla `set.seed(12345)`):

. . .

1. Considera dos predictoras $X_1 \sim N(0, 1)$ y $X_2 \sim N(0, 1)$ de tamaño muestral $n = 200$.  Considera el ruido como $\varepsilon \sim N(0, \sigma = 12)$

. . .

2. Simula el modelo $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \varepsilon_i$ con $\beta_0 = 0.01$, $\beta_1 = 1.5$ y $\beta_2 = -1.5$

. . .

3. Realiza el ajuste con todas las predictoras y calcula el $R^2$ y el $R_{adj}^2$ (lo necesitamos guardar, así usa la fórmula no la salida del `lm()`)

. . .

4. Repite este proceso $M = 300$ veces (vuelve a simular las variables, vuelve a construir el modelo, y obtén de nuevo las bondades de ajuste, de manera que tengamos $300$ de cada uno)

---

## R2 ajustado


Lo que debería salirte si fijas `set.seed(12345)`

```{r}
#| echo: false
M <- 300
n <- 200
R2 <- R2_adj <- rep(NA, M)
p <- 2

for (i in 1:M) {
  
  x_1 <- rnorm(n)
  x_2 <- rnorm(n)
  eps <- rnorm(n, mean = 0, sd = 12)
  
  y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
  datos <- tibble("y" = y, "x_1" = x_1, "x_2" = x_2)
  ajuste <- lm(data = datos, y ~ .)
  
  R2[i] <- 1 - var(ajuste$residuals) / var(datos$y)
  R2_adj[i] <-
    1 - (var(ajuste$residuals) / var(datos$y)) * ((n - 1) / (n - p - 1))
}

library(ggridges)
ggplot(tibble("id" = 1:M, "R2" = R2, "R2_adj" = R2_adj) |> 
         pivot_longer(cols = -id, names_to = "R2_type",
                      values_to = "values")) +
  geom_density_ridges(aes(x = values, y = R2_type,
                          color = R2_type, fill = R2_type),
                      alpha = 0.5) +
  ggthemes::scale_color_colorblind() +
  ggthemes::scale_fill_colorblind() +
  coord_flip() +
  theme_minimal()
```


---

## R2 ajustado

Repite el estudio de simulación  añadiendo 

1. Una nueva predictora "basura" $X_3 \sim N(0, 2)$ tal que $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + 10^{-10} X_{i3} + \varepsilon_i$.

2. Dos nuevas predictora "basura" $X_3 \sim N(0, 2)$ y $X_4 \sim N(0, 2)$ tal que $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + 10^{-10} X_{i3} + 10^{-10} X_{i4} + \varepsilon_i$.

...

3. 195 nuevas predictora "basura" $X_j \sim N(0, 2)$  tal que $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \displaystyle \sum_{j=3}^{195} 10^{-10} X_{ij} + \varepsilon_i$.


Ahora debes obtener $300*195$ valores de $R^2$ y $R_{adj}^2$ (300 simulaciones por cada nueva predictora que añadimos)

---

## R2 ajustado

![](img/predictoras-R2.png)


```{r}
#| echo: false
#| eval: false
M <- 300
n <- 200
p <- 195
R2 <- R2_adj <- matrix(NA, M, p)

set.seed(12345)
for (i in 1:M) {
  
  x_1 <- rnorm(n)
  x_2 <- rnorm(n)
  eps <- rnorm(n, mean = 0, sd = 12)
  y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
  
  X <- matrix(NA, n, p)
  for (j in 1:p) {
    
    X[, j] <- rnorm(n, mean = 0, sd = 2)
    y <- y + 1e-10 * X[, j]
    datos <- tibble("y" = y, "x_1" = x_1, "x_2" = x_2, X[, 1:j])
    ajuste <- lm(data = datos, y ~ .)
    
    R2[i, j] <- 1 - var(ajuste$residuals) / var(datos$y)
    R2_adj[i, j] <-
      1 - (var(ajuste$residuals) / var(datos$y)) *
      ((n - 1) / (n - (j + 2) - 1))
  }
}

mean_R2 <- R2 |> colMeans()
mean_R2_adj <- R2_adj |> colMeans()

gg <- ggplot()
for (i in 1:M) {
  gg <-
    gg +
    geom_line(data =
                 tibble("p" = 1:p, "R2" = R2[i, ],
                        "R2_adj" = R2_adj[i, ]) |>
                 pivot_longer(cols = -p, names_to = "R2_type",
                              values_to = "values"),
               aes(x = p, y = values, color = R2_type),
              linewidth = 0.01, alpha = 0.1) +
    geom_line(data = tibble("p" = 1:p, "R2" = mean_R2, "R2_adj" = mean_R2_adj) |>
                 pivot_longer(cols = -p, names_to = "R2_type",
                              values_to = "values"),
              aes(x = p, y = values, color = R2_type), linewidth = 1.5)
}
gg1 <- 
  gg +
  theme_minimal() +
  scale_y_continuous(limits = c(-0.5, 1.1)) +
  labs(x = "Número de predictoras basura")
```

---

## Multicolinealidad

Uno de los mayores problemas de los [**modelos lineales multivariantes**]{.hl-yellow} es el conocido como [**problema de colinealidad**]{.hl-red}

. . .

Imagina un modelo $Y = \beta_0 + \beta_1 X_1 + \beta_2X_2 + \beta_3 X_3$, donde $X_3$ es definida como (literal) la suma de $X_1$ y $X_2$.

```{r}
n <- 200
x_1 <- rnorm(n, sd = 2)
x_2 <- rnorm(n, sd = 2)
x_3 <- x_1 + x_2
eps <- rnorm(n)

y <- 0.5 - 2*x_1 + 3*x_2 -2.5*x_3
datos <- tibble(y, x_1, x_2, x_3)
ajuste <- lm(data = datos, formula = y ~ .)
```

---

## Multicolinealidad

Como ya vimos en el ejemplo de `wine.csv`, cuando tenemos [**dos o más predictoras que dependen linealmente entre sí**]{.hl-yellow} la matriz $\left(X^{T} X \right)^{-1}$ no se puede invertir ya que es singular (determinante igual a 0), así que debe eliminar una de las ecuaciones para que el problema sea de rango completo

Esto se conoce como [**colinealidad exacta**]{.hl-yellow}

```{r}
ajuste |> summary()
```

---

## Multicolinealidad

La colinealidad exacta es solo el ejemplo más extremo de lo que se conoce como [**problema de colinealidad**]{.hl-red}: un problema que aparece cuando varias predictoras están **altamente correladas**. 
Un [**problema de colinealidad**]{.hl-yellow} tiene principalmente dos consecuencias:

. . .

* [**Reduce la precisión de los estimadores**]{.hl-red} ya que la matriz $X^{T} X$, según aumenta la dependencia, tiene un determinante cada vez más cercano a 0, por lo que $\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}$, que siempre depende de $1/det\left( \mathbf{X}^{T}\mathbf{X} \right)$, tendrá valores cada vez más grandes. Dado que $\widehat{\beta}_j\sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right)$, con $SE \left(\widehat{\beta}_j\right)^2 = \sigma_{\varepsilon}^2 v_j$ siendo $v_j$ el elemento $j$-ésimo de la diagonal de $\left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}$, implica que [**a mayor colinealidad, mayor varianza de las estimaciones**]{.hl_yellow}

. . .


* [**Distorsiona los efectos**]{.hl-red} de los predictores sobre la respuesta

---

## Multicolinealidad

¿Pero qué sucede cuando esa [**colinealidad no es tan obvia**]{.hl-yellow}? Veamos un ejemplo sencillo: simula el siguiente modelo (para $n = 100$)

$$Y = 1 + 0.5 X_1 + 2 X_2 - 3 X_3 - X_4 + \varepsilon, \quad \varepsilon \sim N(0, 1)$$


* $X_1 \sim N(0, 1)$, $X_2 = 0.5*X_1 + N(0, 1)$ y $X_3 = 0.5*X_2 + N(0, 1)$

* $X_4 = -X_1 + X_2 + N(0, 0.5)$

```{r}
#| code-fold: true
set.seed(12345)
n <- 100

x1 <- rnorm(n)
x2 <- 0.5 * x1 + rnorm(n)
x3 <- 0.5 * x2 + rnorm(n)
x4 <- -x1 + x2 + rnorm(n, sd = 0.5)
eps <- rnorm(100)

y <- 1 + 0.5 * x1 + 2 * x2 - 3 * x3 - x4 + eps
datos <- tibble(y, x1, x2, x3, x4)
```

¿Cómo serán sus **correlaciones (lineales)**?

---

## Multicolinealidad

A priori [**no se observa un problema obvio de colinealidad**]{.hl-yellow} ya que no hay dos predictoras altamente correladas.

```{r}
datos |> 
  correlate()
```

---

## Multicolinealidad

Por la definición realizada es obvio que hay una [**dependencia lineal entre todas las predictoras**]{.hl-yellow}, pero al ser una relación lineal más compleja (con ruido de por medio y distintas predictoras interactuando a la vez), una simple [**revisión de las correlaciones es necesaria pero no suficiente**]{.hl-yellow} ya que puede ocultar problemas de colinealidad que sí existen.

```{r}
datos |> 
  cor() |> 
  corrplot(method = "color", addCoef.col = "#121212")
```


---

## Multicolinealidad

Necesitamos por tanto una forma de cuantificar dicha multicolinealidad, y para ello usaremos el conocido como [**factor de inflación de la varianza (VIF)**]{.hl-yellow}

. . .

Para cada coeficiente $\beta_j$, y su estimador $\widehat{\beta}_j$, se define el VIF como

$$VIF\left(\widehat {\beta_j} \right) = \frac{1}{1 - R^2_{X_j | X_{-j}}}, \quad R_{j}^2:= R^2_{X_j | X_{-j}}$$

donde $R_{j}^2$ representa la [**bondad de ajuste $R^2$ cuando predecimos con una regresión lineal multivariante la predictora $X_j$**]{.hl-yellow} en función del resto de predictoras (sin la variable objetivo)

$$X_j = \gamma_0 + \gamma_1 X_1 + \ldots + \gamma_{j-1} X_{j-1} + \gamma_{j+1} X_{j+1} + \ldots + \gamma_j X_j$$

---

## Multicolinealidad


$$VIF\left(\widehat {\beta_j} \right) = \frac{1}{1 - R^2_{X_j | X_{-j}}}, \quad R_{j}^2:= R^2_{X_j | X_{-j}}$$

La idea es [**cuantificar como de relacionado está cada predictor**]{.hl-yellow} respecto al resto (de manera conjunta, no dos a dos). Nota: **siempre es mayor que 1**

. . .


* Si $VIF\left(\widehat {\beta_j} \right)$ [**cercano a 1**]{.hl-yellow}, entonces $R_{j}^{2}$ cercano a 0 --> variabilidad del predictor $X_j$ no se puede explicar con la combinación lineal de otras

. . .


* Si $VIF\left(\widehat {\beta_j} \right)$ [**mayor que 5**]{.hl-yellow}, entonces $1 - R_{j}^{2} < 0.2$, ergo $R_{j}^{2} > 0.8$ --> problema moderado de colinealidad

. . .


* Si además $VIF\left(\widehat {\beta_j} \right)$ [**mayor que 10**]{.hl-yellow}, entonces $R_{j}^{2} > 0.9$ --> problema grave de colinealidad 

---

## Multicolinealidad

Para calcular el $VIF$ podemos o bien hacer regresión con cada parámetro, de manera manual, o bien hacer uso de `check_collinearity()` del paquete `{performance}`

```{r}
ajuste <- lm(data = datos, y ~ .)
performance::check_collinearity(ajuste)
```

---

## Multicolinealidad

Si te fijas $VIF \left(\widehat{\beta}_2 \right) = 10.41$, que es justamente el $1/(1-R^2)$ si hacemos la regresión $X_2$ vs el resto de predictoras

```{r}
check_collinearity(ajuste)$VIF
ajuste_X2 <- lm(data = datos |> select(-y), x2 ~ .)
ajuste_X2 |> summary()
```

---

## Multicolinealidad

El VIF podemos además [**visualizarlo en una sexta gráfica**]{.hl-model} cuando hacemos `check_model()`

```{r}
check_model(ajuste)
```

---

## Multicolinealidad

El VIF podemos además [**visualizarlo de manera manual**]{.hl-model}

```{r}
#| code-fold: true
VIF <- tibble("variable" = c("x1", "x2", "x3", "x4"),
              "VIF"= check_collinearity(ajuste)$VIF)
ggplot(VIF) +
  geom_col(aes(x = variable, y = VIF, fill = VIF),
           alpha = 0.75) +
  scale_fill_gradient2(low = "#1B8A54", mid = "#F7E865",
                       high = "#CE2424", midpoint = 5) +
  theme_minimal()
```


---

## Selección de modelos

Por lo tanto parece que obvio que en un **modelo de regresión multivariante** vamos a tener que que [**seleccionar distintas variables**]{.hl-yellow} para solventar esos problemas de colinealidad

. . .

Llamaremos [**modelo saturado**]{.hl-yellow} al modelo con todas las $p$ predictoras, sin seleccionar.

. . .

Y como hemos visto, [**no podremos simplemente comparar 2 a 2 correlaciones o p-valores**]{.hl-red} ya que el efecto de una variable nos puede afectar en otras. ¿Qué hacer?


---

## Selección de modelos

Un protocolo de actuación habitual podría ser el siguiente:

1. Hacemos el [**ajuste del modelo saturado**]{.hl-yellow}

2. Calculamos el [**VIF de cada estimador**]{.hl-yellow}

3. Adoptamos un umbral (por ejemplo, $VIF > 10$), de manera que [**toda predictora que lo supere se elimina ya de antemano**]{.hl-yellow}

4. Volvemos a chequear el VIF. Si hay que eliminar, volver al paso 3.

5. Del resto de predictoras se hace una [**selección más fina**]{.hl-yellow}. Algunas opciones: BIC/AIC, regresión penalizada (LASSO, ridge, elastic net), PCA.

---

## Selección de modelos

Lo que nos dice el VIF y lo visto hasta ahora es que, incluso aunque tengan efecto, [**añadir predictoras complejizando el modelo no es gratis**]{.hl-yellow}, ya que lo hacemos a costa de [**sobreajustar el modelo**]{.hl-yellow} (la varianza de los estimadores se incrementa considerablemente)

¿Cuál es el [**número mínimo/máximo de predictoras**]{.hl-yellow} que podremos incluir?

. . .

* El número mínimo de predictoras será $p = 1$
* El número máximo será $p = n-2$, o dicho de otra forma, necesito al menos $n \geq p+2$ observaciones. Piensa en $p=1$: si quiero una recta, necesito al menos 2 puntos para que la recta existe y al menos 3 para poder calcular la varianza residual estimada (acuérdate que se divide entre $n-p-1$)

. . .

De manera resumida: dado un modelo con $p$ predictores, necesitaremos estimar [**$p+2$ incógnitas**]{.hl-yellow} ($p+1$ coeficientes y la varianza residual).

---

## BIC y AIC

[**¿Cómo seleccionar los predictores más adecuados para el CONJUNTO del ajuste?**]{.hl-yellow}

. . .

Los métodos más conocidos son los conocidos como [**selección de modelos stepwise (paso-a-paso)**]{.hl-yellow}, que de manera **iterativa**, va incluyendo y descartando distintos predictores, y comparando su calidad, para decidir que **combinación de parámetros** es la más óptima.

. . .

La idea es [**seleccionar el modelo más óptimo**]{.hl-yellow} en función de un [**criterio de información**]{.hl-yellow} que combina la calidad del modelo con el número de predictoras empleadas: vamos a [**penalizar el uso de variables que no mejore suficiente el modelo**]{.hl-yellow}

---


## BIC y AIC

Los dos criterios de información más famosos son el [**Bayesian Information Criterion (BIC)**]{.hl-yellow} y el [**Akaike Information Criterion (AIC)**]{.hl-yellow} definidos ambos como

$$AIC/BIC_{modelo} = -Calidad + \underbrace{\text{npar(modelo)} * \text{penalización}}_{\text{Complejidad}}$$

* $\text{npar(modelo)}$ es el [**número de parámetros a estimar del modelo**]{.hl-yellow}, que en el caso que nos ocupa es $\text{npar(modelo)} = p+2$ (coeficientes + var residual)

* $\text{Calidad}$ una medida que nos diga como de bueno es nuestro ajuste: al tener signo negativo, buscamos el [**menor valor de AIC/BIC**]{.hl-yellow}

* $\text{penalización}$: cuando sube $p$, entonces suba el $AIC/BIC$

El objetivo será probar [**distintos modelos y quedarnos con el que tenga AIC/BIC más pequeño**]{.hl-yellow}

---

## BIC y AIC

Es habitual que la [**calidad del modelo**]{.hl-yellow} venga cuantificada por $2\ell(\text{modelo})$, donde $\ell(\text{modelo})$ es lo que se conoce como [**log-verosimilitud del modelo**]{.hl-yellow} 

$$\ell(\text{modelo}) = \log \left(P \left(\text{datos} |  \left(\text{parámetros}, \text{modelo} \right) \right) \right)$$

es decir, suponiendo que el **modelo fuese correcto** y los **parámetros valiesen la estimación obtenida**, ¿cómo de [**probable es que mis datos hayan sido los que han sido**]{.hl-yellow}?

. . .

Así nuestros criterios quedan como

$$AIC/BIC_{modelo} = -2\ell(\text{modelo}) + \underbrace{(p+2) * \text{penalización}}_{\text{Complejidad}}$$

---

## BIC y AIC

La diferencia entre ambos está en la **penalización usada**


$$BIC(modelo_p) = -2\ell(\text{modelo}) + \underbrace{(p+2) * \log(n)}_{\text{Complejidad}}$$

$$AIC (modelo_p) = -2\ell(\text{modelo}) + \underbrace{(p+2) * 2}_{\text{Complejidad}}$$

El [**criterio BIC es más agresivo**]{.hl-yellow} seleccionando variables ya que, si $n$ crece, $\log(n) >> 2$ (penaliza más el sobreajuste) y, además, [**depende del tamaño muestral**]{.hl-yellow}

---

## BIC y AIC

Vamos a ver un **pequeño ejemplo en R** con `wine.csv`, aplicando un modelo saturado (habiendo hecho un filtro inicial por VIF), un modelo solo con `AGST`,  otro con `AGST + HarvestRain` y otro con `AGST + HarvestRain + Year`, y calcular su BIC/AIC con las funciones `BIC()` y `AIC()`

```{r}
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, formula = Price ~ .)
ajuste_1 <- lm(data = datos, formula = Price ~ AGST)
ajuste_2 <- lm(data = datos, formula = Price ~ AGST + HarvestRain)
ajuste_3 <- lm(data = datos, formula = Price ~ AGST + HarvestRain + Age)
```

:::: columns
::: {.column width="50%"}
```{r}
BIC(ajuste_saturado)
BIC(ajuste_1)
```
:::

::: {.column width="50%"}
```{r}
BIC(ajuste_2)
BIC(ajuste_3)
```
:::
::::

---

## BIC y AIC

Vamos a ver un **pequeño ejemplo en R** con `wine.csv`, aplicando un modelo saturado (habiendo hecho un filtro inicial por VIF), un modelo solo con `AGST`,  otro con `AGST + HarvestRain` y otro con `AGST + HarvestRain + Year`, y calcular su BIC/AIC con las funciones `BIC()` y `AIC()`

:::: columns
::: {.column width="50%"}
```{r}
AIC(ajuste_saturado)
AIC(ajuste_1)
```
:::

::: {.column width="50%"}
```{r}
AIC(ajuste_2)
AIC(ajuste_3)
```
:::
::::

---

## BIC y AIC

Podemos usar `compare_performance()`  ($AICc = AIC +\frac{2(p+2)^{2} + 2(p+2)}{n-(p+2)-1}$ es una versión corregida para tamaños muestrales pequeños)

```{r}
performance::compare_performance(ajuste_saturado, ajuste_1, ajuste_2, ajuste_3)
```

. . .

En ambos criterios la conclusión es que de los 4 modelos, el [**"mejor" es el 3º ya que AIC/BIC más bajos**]{.hl-yellow} aunque tenga un $R_{adj}^2$ menor: la mejora que produce meter todas las variables no es suficiente para lo que se complica el modelo.

---

## BIC y AIC

En realidad lo correcto sería antes chequear si [**podemos eliminar alguna variable**]{.hl-yellow} ya de manera preliminar usando el VIF

```{r}
check_collinearity(ajuste_saturado)
```

---

## BIC y AIC

Según el VIF debemos ya eliminar de antemano `Year` y `FrancePop` así que lo hacemos y volvemos calcularlo para las restantes

```{r}
datos_VIF <- datos |> select(-Year, -FrancePop)
ajuste_saturado <- lm(data = datos_VIF, formula = Price ~ .)
check_collinearity(ajuste_saturado)
```

---

## BIC y AIC

Tras este primer **filtro grosero** volvemos a implementar los 3 modelos

```{r}
ajuste_saturado <- lm(data = datos_VIF, formula = Price ~ .)
ajuste_1 <- lm(data = datos_VIF, formula = Price ~ AGST)
ajuste_2 <- lm(data = datos_VIF, formula = Price ~ AGST + HarvestRain)
ajuste_3 <- lm(data = datos_VIF, formula = Price ~ AGST + HarvestRain + Age)
compare_performance(ajuste_saturado, ajuste_1, ajuste_2, ajuste_3)
```

Ahora **nos indica que el mejor modelo es el saturado** (pero que ya no tiene 6 variables sino 4): la mejora de bondad de ajuste compensa por incrementar una sola variable (entre ajuste3 y saturado)

---

## BIC y AIC

Fíjate que además al haber quitado dos variables con alta dependenica lineal del resto las [**hipótesis se cumplen**]{.hl-yellow} (antes no)

```{r}
check_model(ajuste_saturado)
```

---

## BIC y AIC

Vamos a repetirlo con un ejemplo simulado $Y = 0.01 + 1.5*X_1 -1.5X_2 + \varepsilon$, donde $X_1,X_2 \sim N(0,1)$ y $\varepsilon \sim N(0, 2)$, y al que vamos añadiendo predictoras basura $X_{2+j} \sim N(0, 2)$ con $\beta_{2+j} = 10^{-8}$, probando cuando $j=1$, $j = 5$, $j = 10$, $j=25$, $j=50$ y $j = 100$. El mejor modelo es el que solo tiene las **dos predictoras con un efecto real** sobre y.

```{r}
#| code-fold: true
n <- 200
p <- c(1, 5, 10, 25, 50, 100)

set.seed(12345)
x_1 <- rnorm(n)
x_2 <- rnorm(n)
eps <- rnorm(n, mean = 0, sd = 2)
y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
ajuste_0 <- lm(data = tibble("y" = y, "x_1" = x_1, "x_2" = x_2), y ~ .)

X <- matrix(rnorm(n * max(p), mean = 0, sd = 2), nrow = n)
ajuste <- list()
for (j in 1:length(p)) {
  
  ynew <- y + 1e-8 * sum(X[, 1:p[j]])
  datos <- tibble("y" = ynew, "x_1" = x_1, "x_2" = x_2, X[, 1:p[j]])
  ajuste[[j]] <- lm(data = datos, y ~ .)
}
compare_performance(ajuste_0, ajuste[[1]], ajuste[[2]],
                    ajuste[[3]], ajuste[[4]], ajuste[[5]])
```



---


## Sobreajuste

Algo importante a tener en cuenta es que, aunque ambos criterios nos ayudan a seleccionar modelos, ambos [**funcionan de manera aceptable bajo la hipótesis**]{.hl-yellow} de que $n >> p +2$: en caso contrario, si $n$ se acerca a $p +2$, el sobreajuste seguirá produciéndose

. . .

Dado que la penalización es más grande en el BIC, el [**criterio BIC nos garantiza una más temprana detección del sobreajuste**]{.hl-yellow}

---

## Sobreajuste

Lo anterior se puede ilustrar calculando BIC e AIC del anterior estudio de simulación de $R^2$ vs $R_{adj}^2$: fíjate como el [**BIC tarda más en bajar**]{.hl-yellow}, tal que $BIC(p = 195) > BIC(p=2)$ (nos haría quedarnos con el modelo sin sobreajustar) mientras que $AIC(p = 156) < AIC(p=2)$ (y para todos los que van detrás), por lo que acabaríamos eligiendo el modelo más sobreajustado.

![](img/BIC-AIC.png)

```{r}
#| echo: false
#| eval: false
M <- 300
n <- 200
p <- 195
BIC_values <- AIC_values <- matrix(NA, M, p)

set.seed(12345)
for (i in 1:M) {
  
  x_1 <- rnorm(n)
  x_2 <- rnorm(n)
  eps <- rnorm(n, mean = 0, sd = 12)
  y <- 0.01 + 1.5*x_1 - 1.5*x_2 + eps
  
  X <- matrix(NA, n, p)
  for (j in 1:p) {
    
    X[, j] <- rnorm(n, mean = 0, sd = 2)
    y <- y + 1e-10 * X[, j]
    datos <- tibble("y" = y, "x_1" = x_1, "x_2" = x_2, X[, 1:j])
    ajuste <- lm(data = datos, y ~ .)
    
    BIC_values[i, j] <- BIC(ajuste)
    AIC_values[i, j] <- AIC(ajuste)
  }
}

mean_BIC <- BIC_values |> colMeans()
mean_AIC <- AIC_values |> colMeans()

gg <- ggplot()
for (i in 1:M) {
  gg <-
    gg +
    geom_line(data =
                 tibble("p" = 1:p, "BIC" = BIC_values[i, ],
                        "AIC" = AIC_values[i, ]) |>
                 pivot_longer(cols = -p, names_to = "criterio",
                              values_to = "values"),
               aes(x = p, y = values, color = criterio),
              linewidth = 0.01, alpha = 0.1) +
    geom_line(data = tibble("p" = 1:p, "BIC" = mean_BIC,
                            "AIC" = mean_AIC) |>
                 pivot_longer(cols = -p, names_to = "criterio",
                              values_to = "values"),
              aes(x = p, y = values, color = criterio), linewidth = 1.5)
}
gg1 <- 
  gg +
  theme_minimal() +
  labs(x = "Número de predictoras basura")
```

---

## Consistencia

Otra [**enorme ventaja del BIC**]{.hl-yellow} es que se ha demostrado matemáticamente que es [**consistente**]{.hl-yellow}: si el tamaño muestral fuese lo suficientemente grande, el [**BIC garantiza elegir el modelo correcto que genera los datos**]{.hl-green}. Matemáticamente se cumple que, si tenemos una serie de modelos $M_1, \ldots, M_m$, y el modelo real que genera los datos $M_0$ (que pretendemos estimar), entonces

$$P\left[\arg\min_{k=0,\ldots,m}\text{BIC}(\widehat{M}_k)=0\right]\to 1, \quad  n \to \infty$$

Esto solo suponiendo que el modelo subyacente sea lineal claro... Esto [**no sucede con el AIC**]{.hl-red} (para vuestro yo del futuro: en <https://doi.org/10.2307/2290328.> se prueba que el AIC es equivalente a usar validación cruzada leave-one-out, el cual es inconsistente)

---

## stepAIC

Y aquí nos puede nacer una duda: si tengo muchos predictores, [**¿tengo que calcular el BIC/AIC de todas las combinaciones posibles?**]{.hl-yellow}

. . .

Sí, pero lo hará por nosotros `MASS::stepAIC()`, donde en el parámetro `k = ...` le indicamos la penalización (si `k = 2` es AIC y si `k = log(n)` es el BIC)

```{r}
#| eval: false
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, Price ~ .)
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```

---

## stepAIC

```{r}
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, Price ~ .)
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```


---

## stepAIC

Lo que obtendremos será una [**selección secuencial de modelos**]{.hl-yellow}, de manera que el irá probando las combinaciones, nos **muestra el ajuste y el valor del BIC/AIC** y se parará cuando el AIC/BIC no mejore.

```{r}
#| eval: false
datos <- read_csv(file = "./datos/wine.csv")
ajuste_saturado <- lm(data = datos, Price ~ .)
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```


---

## stepAIC

```{r}
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "forward")
```

El argumento `direction = ...` puede tomar los valores `"both"`, `"forward"` y `"backward"` (por defecto) que nos determina la [**dirección de búsqueda**]{.hl-yellow}:


* `direction = "forward"`: empieza con el modelo proporcionado y va [**añadiendo**]{.hl-yellow} predictoras haciendo modelos cada vez más complejos.

---

## stepAIC

```{r}
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "forward")
```


Fíjate que ahora  como le hemos dado el modelo saturado y `direction = "forward"`, entonces no hace nada. Para controlar esto podemos incluir la variable `scope = list(lower = mod_easy, upper = mod_complex)`, donde **podemos pasarle dos modelos**p: los modelos más complejos y más sencillo **pentre los que queremos que se mueva en la búsqueda**.

---

## stepAIC

```{r}
#| eval: false
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "backward")
```

* `direction = "backward"`: empieza con el modelo proporcionado y va [**eliminando**]{.hl-yellow} predictoras haciendo modelos cada vez más sencillos.


```{r}
#| eval: false
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)))
```

* `direction = "both"`: empieza con el modelo proporcionado y va [**añadiendo y eliminando**]{.hl-yellow} predictoras según sea más conveniente (pudiendo añadir/eliminar una variable que previamente e había eliminado/añadido)




---

## stepAIC

```{r}
MASS::stepAIC(ajuste_saturado, k = log(nrow(datos)), direction = "forward")
```

[**Aviso**]{.hl-red}: en realidad `MASS::stepAIC()` no calcula exactamente el mismo BIC/AIC que las funciones `BIC()` e `AIC()`, sino que les suma una constante $n(\log(2\pi) + 1) + \log(n)$ para BIC y $n(\log(2\pi) + 1) + 2$ para AIC

Dado que es una constante nos da igual para comparar modelos, pero hace que [**NO podamos comparar salidas**]{.hl-red} de `BIC()` y `AIC()` con salidas de `MASS::stepAIC()`.



# Clase 15: casos prácticos {#clase-15}

[**Casos prácticos: datos de viviendas de Boston y seatpos dataset**]{style="color:#444442;"}

---

## Casos prácticos

Realiza todo el ajuste completo multivariante con los datasets:

* `seatpos` del paquete `{faraway}`: datos de 38 conductores donde el objetivo es predecir `hipcenter`, la posición del asiento del conductor, en función de distintas variables (ver `? faraway::seatpos`)

* `Boston` del paquete `{MASS}`:  datos de 560 suburbios de Boston, en el que se han medido 14 variables en cada uno, con el objetivo de predecir `medv` el precio mediano de inmuebles (en millones de dolares), en función de variables estructurales (`rm` y `age`), variables de vecindario (`crim`, `zn`, `indus`, `chas`, `tax`, `ptratio`, `black` y `lstat`), variables de accesibilidad (`dis` y `rad`) y variables de calidad del aire (`nox`)


# Clase 16: variables cualitativas {#clase-16}

[**¿Cómo introducir predictoras cualitativas?**]{style="color:#444442;"}


---

## Cosas que faltan por añadir

* ejemplo seatpos

* ejemplo one-hot-encoding

* ejemplo boston



# Clase n:

---

## Regresión penalizada

Hagamos un resumen: [**¿cuál era el objetivo de la regresión ordinaria?**]{.hl-yellow} 

. . .

Asumiendo que 

$$E \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \displaystyle \sum_{j=1}^{p} \beta_j X_j, \quad \widehat{Y} = \widehat{\beta_0} + \displaystyle \sum_{j=1}^{p} \widehat{\beta_j} X_j, \quad E \left[ \varepsilon | \left( X_1 = x_1, \ldots, X_p = x_p \right) \right] = 0 $$

El objetivo ha sido obtener la estimación de los $\widehat{\beta}$ tal que [**minimicemos la varianza residual o suma de errores al cuadrado**]{.hl-yellow} $SSE$

$$SSE \left(\boldsymbol{\beta} \right) =  \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} = \displaystyle \sum_{i=1}^{n} \left(Y_i - \beta_0 - \beta_1 X_{i1} - \ldots - \beta_p X_{ip} \right)^2 = \left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right)^{T}\left( \mathbf{Y} - \mathbf{X} \boldsymbol{\beta} \right) $$

. . .

Tras derivar e igualar a cero obteníamos $\widehat{\boldsymbol{\beta}}=\left(\mathbf{X}^{T}\mathbf{X} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$

---

## Regresión penalizada


El estimador $\widehat{\boldsymbol{\beta}}$ es aquel que nos garantiza

$$\widehat{\boldsymbol{\beta}} = \arg \min_{\beta \in \mathbb{R}^{p+1}} SSE \left(\boldsymbol{\beta} \right)$$

. . .


También tenemos garantizado que si se [**cumple las hipótesis**]{.hl-green} tenemos que

$$\widehat{\boldsymbol{\beta}} \sim N_{p+1}\left(\boldsymbol{\beta}, \sigma_{\varepsilon}^2 \left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}\right)$$

lo que implica que $E \left[ \widehat{\boldsymbol{\beta}}\right] = \boldsymbol{\beta}$, es decir, tenemos un [**estimador insesgado**]{.hl-yellow}.

. . .

El [**sesgo en una estimación**]{.hl-yellow} es un [**error sistemático**]{.hl-yellow} en la misma. Pero hay otra componente importante que hasta ahora solo hemos mencionado pero que no hemos tenido en cuenta: la [**varianza de la estimación**]{.hl-yellow}

---

## Balance sesgo-varianza

Si descomponemos el error cuadrático medio de una estimación obtenemos que


$$\begin{eqnarray}MSE \left(\widehat{Y} \right) &=& E \left[ \left( \widehat{Y} - Y \right)^2 \right] = E \left[ \left( \left(\widehat{Y} - E \left[ \widehat{Y}\right] \right) - \left(E \left[ \widehat{Y}\right] - Y \right)\right)^2 \right] \nonumber \\ &=& E \left[  \left(\widehat{Y} - E \left[ \widehat{Y}\right] \right)^2 \right] +\left( E \left[  \widehat{Y}\right] - Y \right)^2 + 2*cosas \nonumber \\ &=& \underbrace{\left( E \left[  \widehat{Y}\right] - Y \right)^2}_{Sesgo^2}+\underbrace{Var \left[ \widehat{Y} \right]}_{Varianza} + ruido\end{eqnarray}$$

. . .

El [**eror cuadrático medio podemos descomponerlo**]{.hl-yellow} en una suma de sesgo (al cuadrado), varianza y ruido. El último es intrínseco a los datos, por lo que la metodología de estimación puede bajar el sesgo (subiendo la varianza) o bajar la varianza (subiendo el sesgo).

---

## Balance sesgo-varianza

:::: columns
::: {.column width="40%"}

* [**Bajoajuste (underfitting)**]{.hl-yellow}: modelos muy simples proporcionan un **sesgo muy grande** y **poca varianza** ya que la predicción siempre será muy parecida (errores altos en train).

* [**Sobreajuste (overfitting)**]{.hl-yellow}: modelos muy complicados proporcionan un **sesgo bajo** pero al ser tan complejas proporcionarán una **mayor varianza** para cada intento (errores altos en test).

:::

::: {.column width="60%"}

![](img/bias_varianc_tradeoff.jpg)

Extraída de <https://mlu-explain.github.io/bias-variance/>
:::
::::

---


## Balance sesgo-varianza

Si lo visto en las estimaciones lo trasladamos a los  [**coeficientes estimados**]{.hl-yellow} tenemos

$$\begin{eqnarray}MSE \left(\widehat{\beta}_j \right) =  \underbrace{\left( E \left[  \widehat{\beta}_j \right] - \beta_j \right)^2}_{Sesgo^2}+\underbrace{Var \left[ \widehat{\beta}_j \right]}_{Varianza} + ruido \end{eqnarray} $$

. . .

En este caso además sabemos que $Var \left[ \widehat{\beta}_j \right] = SE \left(\widehat{\beta}_j\right)^2 = \sigma_{\varepsilon}^2 v_j$, con $v_j$ el elemento $j$-ésimo de la diagonal de $\left( \mathbf{X}^{T}\mathbf{X} \right)^{-1}$.

. . .

La forma más sencilla de reducir drásticamente la varianza residual es aumentando el número de parámetros, es decir sobreajustando. Dicho de otra forma: para conseguir [**simplificar modelos vamos a reducir la varianza de las estimaciones**]{.hl-yellow}, y para ello lo que haremos será [**introducir un pequeño sesgo**]{.hl-yellow}

---



## Regresión penalizada



$$\begin{eqnarray}MSE \left(\widehat{\beta}_j \right) &=&  \underbrace{\left( E \left[  \widehat{\beta}_j \right] - \beta_j \right)^2}_{Sesgo^2}+\underbrace{Var \left[ \widehat{\beta}_j \right]}_{Varianza} + ruido = cte\end{eqnarray} $$

La idea será [**sesgar los coeficientes**]{.hl-yellow} de tal manera que se busque obtener [**coeficientes que, en caso de no ser tan importantes como otros, tiendan a cero**]{.hl-yellow}, un sesgo hacia lo que se conoce como [**sparsity**]{.hl-purple}.

. . .

Para ello haremos uso de lo que se conoce como [**shrinkage methods**]{.hl-yellow}, y  en particular hablaremos de [**regresión penalizada**]{.hl-yellow}

---

## Regresión penalizada

Si en el caso de la regresión lineal ordinaria teníamos

$$\widehat{\boldsymbol{\beta}} = \arg \min_{\beta \in \mathbb{R}^{p+1}} SSE \left(\boldsymbol{\beta} \right)$$
 
 
 Ahora incluiremos una [**penalización $\lambda \geq 0$**]{.hl-red}
 
 $$\widehat{\boldsymbol{\beta}}_{\lambda} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left\| \beta_{-1} \right\| \right), \quad \lambda \geq 0$$
 
 donde $\left\| \beta \right\|$ será [**alguna norma del vector de coeficientes (excluyendo el intercepto)**]{.hl-yellow} (distancia del vector a 0) y $\lambda$ deberemos determinar su valor óptimo.
 
. . .

Por tanto ahora el objetivo no será solo [**minimizar los errores al cuadrado**]{.hl-yellow} sino también [**minimizar la penalización**]{.hl-yellow}, y la única forma de hacerlo (dado que una norma siempre devuelve algo positiva) es que los [**coeficientes sean lo más pequeño posibles**]{.hl-yellow}

---

## Penalización


$$\widehat{\boldsymbol{\beta}}_{\lambda} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left\| \beta_{-1} \right\| \right), \quad \lambda \geq 0$$
 
 ¿Qué sucede cuando $\lambda$ **crece o decrece**?
 
 . . .
 
 * Si $\lambda \to 0$ --> estamos en el caso de **mínimos cuadrados ordinarios** (no hay penalización)
 
 * Si $\lambda \to \infty$ --> $\beta_j \to 0$ para todo $j \geq 0$ (el modelo tiende a desaparecer)
 
---

## Familia de normas
 
$$\widehat{\boldsymbol{\beta}}_{\lambda} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left\| \beta_{-1} \right\| \right), \quad \lambda \geq 0$$
 
Una de las [**familias de normas**]{.hl-yellow} más habituales son las [**normas $\ell^p$**]{.hl-yellow}

$$\left\| x \right\|_{p} = \left\| \left(x_1, \ldots, x_k \right)\right\|_{p} = \left( \sum_{j=1}^{k} \left| x_{j} \right|^{p} \right)^{1/p}, \quad x \in \mathbb{R}^{k}$$

. . .

* Si $p=1$ --> $\left\| x \right\|_{1}  =  \sum_{j=1}^{k} \left| x_{j} \right|$ se conoce como [**norma taxicab o de Manhattan**]{.hl-yellow} (la métrica que usaríamos para recorrer calles en un mapa)

* Si $p=2$ --> $\left\| x \right\|_{2}  =  \left(\sum_{j=1}^{k} x_{j}^{2} \right)^{1/2}$ se conoce como [**norma Euclídea**]{.hl-yellow} (la habitual)

* Si $p=\infty$ --> $\left\| x \right\|_{\infty}  =  \lim_{p \to \infty} \left(\sum_{j=1}^{k} \left| x_{j} \right|^{p} \right)^{1/p} = \displaystyle \max_{j} \left| x_{j} \right|$ se conoce como [**norma infinita o de Chebyshev**]{.hl-yellow} (movimientos de rey en ajedrez)

---


## Regresión elastic-net

 
Dado que las más usadas son la [**norma Manhattan y la Euclídea**]{.hl-yellow}, podemos generalizar la regresión penalizada anterior combinando ambas penalizaciones

 
$$\widehat{\boldsymbol{\beta}}_{\lambda, \alpha} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left(\alpha \left\| \beta_{-1} \right\|_1  + (1-\alpha) \left\| \beta_{-1} \right\|_{2}^{2} \right)\right), \quad \lambda \geq 0,~0 \leq \alpha \leq 1$$

La [**regresión elastic-net**]{.hl-yellow} es una regresión penalizada que combina una [**penalización $\lambda$**]{.hl-red} (que nos cuantifica lo agresivo que queremos hacer tender los coeficientes a cero) y una [**proporción $\alpha$**]{.hl-green} (que nos permite combinar ambas penalizaciones)

. . .

::: callout-important
## Importante

Dado que la penalización depende de la norma de los coeficientes, su magnitud será importante, por lo que [**deberemos estandarizar antes los predictores**]{.hl-yellow}.

:::

---
 
## Ridge y lasso

 
$$\widehat{\boldsymbol{\beta}}_{\lambda, \alpha} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left(\alpha \left\| \beta_{-1} \right\|_1  + (1-\alpha) \left\| \beta_{-1} \right\|_{2}^{2} \right)\right), \quad \lambda \geq 0,~0 \leq \alpha \leq 1$$

Hay dos escenarios de penalización principalmente conocidos:

* [**Regresión ridge**]{.hl-yellow}: cuando $\alpha = 0$ (penalización cuadrática)

$$\widehat{\boldsymbol{\beta}}_{\lambda, ridge} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda  \left\| \beta_{-1} \right\|_{2}^{2} \right), \quad \lambda \geq 0$$

* [**Regresión LASSO**]{.hl-yellow}: cuando $\alpha = 1$ (penalización absoluta)


$$\widehat{\boldsymbol{\beta}}_{\lambda, LASSO} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda  \left\| \beta_{-1} \right\|_1  \right), \quad \lambda \geq 0$$

---

## Ridge y lasso

* [**Regresión ridge**]{.hl-yellow}: cuando $\alpha = 0$ (penalización cuadrática)

$$\widehat{\boldsymbol{\beta}}_{\lambda, ridge} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda  \left\| \beta_{-1} \right\|_{2}^{2} \right), \quad \lambda \geq 0$$

Una de las [**ventajas de la regresión ridge**]{.hl-green} es que podemos calcular su [**expresión explícita**]{.hl-green} (algo que no sucede en la regresión lasso, solo se puede obtener mediante simulación numérica)

$$\widehat{\boldsymbol{\beta}}_{\lambda, ridge} = \left(\mathbf{X}^{T}\mathbf{X} + \lambda \mathbf{I}_{p \times p} \right)^{-1}\mathbf{X}^{T}\mathbf{Y}$$
donde $\mathbf{I}_{p \times p}$ es la matriz identidad.


---

## Ridge y lasso

* [**Regresión LASSO**]{.hl-yellow}: cuando $\alpha = 1$ (penalización absoluta)


$$\widehat{\boldsymbol{\beta}}_{\lambda, LASSO} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda  \left\| \beta_{-1} \right\|_1  \right), \quad \lambda \geq 0$$

Una de las [**ventajas de la regresión LASSO**]{.hl-green} es que nos realiza una [**selección de variables**]{.hl-green} ya hace [**tender más rápido los coeficientes a cero**]{.hl-yellow}.

&nbsp;

¿Por qué?

--- 

## Regresión elastic-net

 
$$\widehat{\boldsymbol{\beta}}_{\lambda, \alpha} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left(\alpha \left\| \beta_{-1} \right\|_1  + (1-\alpha) \left\| \beta_{-1} \right\|_{2}^{2} \right)\right), \quad \lambda \geq 0,~0 \leq \alpha \leq 1$$

La [**regresión elastic-net**]{.hl-yellow} puede ser también vista como un [**problema de minimización de la suma de los residuos al cuadrado**]{.hl-yellow} con una restricción en los argumentos


$$\widehat{\boldsymbol{\beta}}_{s_\lambda, \alpha} = \arg \min_{\beta \in \mathbb{R}^{p+1}:~\sum_{j=1}^{p} \left(\alpha \left| \beta_{j} \right| + (1-\alpha) \left| \beta_{j} \right|^{2} \right) \leq s_\lambda} SSE \left(\boldsymbol{\beta} \right) , \quad 0 \leq \alpha \leq 1$$

donde $s_\lambda$ es un valor numérico que no depende de $\beta$.

---

## Regresión elastic-net

Por ejemplo, si tenemos solo dos predictoras...

![](img/elastic-net.jpg)

Las elipses nos muestran los valores para los que $SSE \left( \boldsymbol{\beta} \right)$ es constante (lo que queremos minimizar), y las **regiones sombreadas representan el espacio de valores posibles** debido a la penalización (ver más en <https://link.springer.com/book/10.1007/978-1-4614-7138-7>)

---

## LASSO


Si nos fijamos en ambas regiones, dado que el [**LASSO usa una norma absoluta**]{.hl-yellow}, el espacio de restricciones es más "picudo", lo que hará más probable que cuando la **elipse corte con la zona sombreada** lo haga en los vértices (donde uno de los coeficientes es cero) en lugar de en los laterales.

![](img/elastic-net.jpg)

Ver más en <https://link.springer.com/book/10.1007/978-1-4614-7138-7>



---


## Selección de la penalización

En todo este proceso de regresión penalizada hay un "elefante en la habitación": [**¿cómo seleccionar el $\lambda$ más óptimo?**]{.hl-yellow}

. . .


Una de las más formas más habituales para ello es lo que se conoce como [**validación cruzada**]{.hl-yellow}. La idea se basa en un ideal: poder **disponer de muchas muestras** tal que, probando en cada una distintos valores de $\lambda$, consigamos obtener aquel que nos proporcione (en media) el menor error.

. . .

[**¿El problema?**]{.hl-red} Casi nunca disponemos de un gran número de muestras de la misma población, así que una [**solución es "simularlas" nosotros mismos a partir de la muestra original**]{.hl-green}

---

## Validación cruzada

Supongamos que tenemos una muestra tal que contamos con una variable objetivo a predecir en función de $p$ predictoras $\left\lbrace Y_i, \left(X_{i1}, \ldots, X_{ip} \right) \right\rbrace_{i=1,\ldots,n}$, y que pretendemos usar un [**estimador $\widehat{f}_{\lambda}$ que depende de un parámetro $\lambda$**]{.hl-yellow}, tal que

$$\widehat{Y} =\widehat{f}_{\lambda} \left(X_{1}, \ldots, X_{p} \right)$$

La opción más sencilla es lo que se conoce como [**validación simple**]{.hl-yellow}:


---

## Validación simple


$$\widehat{Y} =\widehat{f}_{\lambda} \left(X_{1}, \ldots, X_{p} \right)$$
La opción más sencilla es lo que se conoce como [**validación simple**]{.hl-yellow}:


1. Hacemos una partición de train y test. De los datos de train, realizamos una segunda partición que llamamos [**conjunto de validación**]{.hl-yellow}.

. . .

2. [**Entrenamos solo  con train**]{.hl-yellow} el modelo para distintos valores de $\lambda$

. . .

3. Evaluamos el error que produce cada $\widehat{f}_{\lambda}$ en el [**conjunto de validación**]{.hl-yellow} (un conjunto del que el modelo no ha aprendido) para [**elegir el mejor parámetro**]{.hl-yellow} $\lambda$.

. . .

4. Proporcionamos una evaluación final en la tercera partición (test), que no ha sido usada ni para la estimación ni para la elección del mejor parámetro.

---

## Validación cruzada

La [**validación simple**]{.hl-yellow} tiene un problema: [**depende muchísimo de la partición aleatoria**]{.hl-red} que se realice (si tenemos mala o buena suerta) y, además, puede provocar un [**problema de tamaño muestral**]{.hl-red} si $n$ no es muy grande (ya que necesitamos 3 particiones de los datos).

. . .

Para solventarlo existe una alternativa conocida como [**validación cruzada**]{.hl-yellow}: en lugar de extraer un subconjunto de train, lo que se hace es que **cada observación pueda jugar ambos roles**: el rol de entrenamiento y el rol de validación.


---

## Validación cruzada


La más famosa es la conocida como [**leave-one-out cross-validation (LOOCV)**]{.hl-yellow}:

1. Hacemos una partición de train (con $m$ elementos) y test ($n - m$).

2. De los datos de train $\left\lbrace X_{i} \right\rbrace_{i=1,\ldots,m}$, entrenamos el modelo sin una de las observaciones de entrenamiento $\boldsymbol{X}_{-1} = \left\lbrace X_{i} \right\rbrace_{i=2,\ldots,m}$, y usamos esa observación $X_{1}$ para obtener una métrica del error para cada $\lambda$ (un conjunto de validación de tamaño muestral igual a 1).

3. Repetimos el proceso para cada una de ellas, considerando como entrenamiento $\boldsymbol{X}_{-j} = \left\lbrace X_{i} \right\rbrace_{i=1,\ldots, j-1, j+1, \ldots,m}$ y $X_j$ como conjunto de validación.

4. En total se entrena el modelo con $n$ conjuntos train (de $n-1$ observaciones cada uno) y $n$ conjuntos de validación (de una observación cada uno), obteniendo al final una media del error en entrenamiento y otra en validación, usando esta última para [**elegir el mejor parámetro**]{.hl-yellow} $\lambda$.

---

## Validación cruzada


La anterior idea puede ser generalizada a lo que se conoce como [**leave-k-out cross-validation (LOOCV)**]{.hl-yellow}:

1. Hacemos una partición de train (con $m$ elementos) y test ($n - m$).

2. De los datos de train $\left\lbrace X_{i} \right\rbrace_{i=1,\ldots,m}$, entrenamos el modelo sin $k$ observaciones de entrenamiento $\boldsymbol{X}_{-i_1, \ldots, -i_k}$, y usamos esa $k$ observaciónes $\left\lbrace X_{i_1}, \ldots, X_{i_k} \right\rbrace$ para obtener una métrica del error (un conjunto de validación de tamaño muestral igual a $k$).

3. Repetimos el proceso para cada posible forma de combinar $m$ elementos de $k$ en $k$ 

4. En total se entrena el modelo con $C_{k}^{m}$ conjuntos train (de $m-k$ observaciones cada uno) y $C_{k}^{m}$ conjuntos de validación (de $k$ observaciones cada uno), obteniendo estos últimos para [**elegir el mejor parámetro**]{.hl-yellow} $\lambda$.


&nbsp;

Aunque esta opción nos da una métrica más robusta del error, si hacemos unas sencillas cuentas vemos que puede no ser factible si $m$ y/o $k$ toman valores elevados. Por ejemplo, si $m = 100$ y $k = 30$, tendríamos que entrenar $C_{k}^{m} \simeq 10^{25}$ modelos


---

## Validación cruzada k-folds

Dado que las dos opciones anteriores pueden no servirnos existe una tercera vía, la conocida como [**validación cruzada k-folds**]{.hl-yellow}:

![](img/k-fold.png)

---

## Validación cruzada k-folds

Dado que las dos opciones anteriores pueden no servirnos existe una tercera vía, la conocida como [**validación cruzada k-folds**]{.hl-yellow}:


1. Hacemos una partición de train (con $m$ elementos) y test ($n - m$).

2. De los datos de train $\left\lbrace X_{i} \right\rbrace_{i=1,\ldots,m}$, dividimos dicho conjunto en $k$ particiones ($k$ folds) que denotaremos como $\boldsymbol{X}_{fold_1}, \ldots, \boldsymbol{X}_{fold_k}$ (con $m/k$ observaciones cada una aproximadamente).

3. Entrenamos $k$ modelos, de manera que en la iteración $j$, usaremos las $m-m/k$ observaciones de las particiones $\boldsymbol{X}_{fold_1}, \ldots, \boldsymbol{X}_{fold_{j-1}}, \boldsymbol{X}_{fold_{j+1}}, \ldots, \boldsymbol{X}_{fold_{k}}$ para entrenar y las $m/k$ observaciones de $\boldsymbol{X}_{fold_{j}}$ para validar.

4. De esta manera **cada observación jugará $k-1$ veces el rol de entrenamiento** y **una sola vez el rol de validación**, teniendo $k$ validaciones cuyos resultados promediaremos para [**elegir el mejor parámetro**]{.hl-yellow} $\lambda$.

---

## Selección de la penalización

Así llamaremos $\widehat{\lambda}_{k-CV}$ a la [**penalización óptima elegida a través de validación k-fold**]{.hl-yellow}

$$\widehat{\lambda}_{k-CV} = \arg \min_{\lambda \geq 0} CV_k(\lambda),\quad CV_k(\lambda)=\sum_{j=1}^k  SSE \left(\widehat{\boldsymbol{\beta}_{(j)} } \right)$$
donde $SSE \left(\widehat{\boldsymbol{\beta}}_{(k)} \right)$ es la suma de residuos al cuadrado del $j$-ésimo slot de validación (con los parámetros obtenidos en el entrenamiento sin ese slot).

---

## Selección de la penalización


$$\widehat{\lambda}_{k-CV} = \arg \min_{\lambda \geq 0} CV_k(\lambda),\quad CV_k(\lambda)=\sum_{j=1}^k  SSE \left(\widehat{\boldsymbol{\beta}_{(j)} } \right)$$

Con el objetivo de [**simplificar aún más el modelo**]{.hl-yellow} existe una variante de penalización $\widehat{\lambda}_{k-1SE}$ conocida como [**«one standard error rule»**]{.hl-yellow}: la idea es elegir el [**modelo más sencillo de los modelos aceptanles**]{.hl-yellow}, la penalización más grande posible (es decir, el modelo más sencillo posible) acercándose lo máximo que pueda a $\widehat{\lambda}_{k-CV}$. La idea consiste en estimar $\widehat{\lambda}_{k-CV}$ y calcular su correspondiente $CV_k(\widehat{\lambda}_{k-CV})$  para cada partición de validación(que es en sí una variable aleatoria), para posteriormente [**estimar su variabilidad**]{.hl-yellow} ($SE$ igual a desviación típica dividida entre $\sqrt{n}$) entre todas las particiones de validación, tal que

$$\widehat{\lambda}_{k-1SE}= \max \left\lbrace \lambda \geq 0: CV_k(\lambda) \in \left(CV_k(\widehat{\lambda}_{k-CV})\pm\widehat{\mathrm{SE}}\left(CV_k(\widehat{\lambda}_{k-CV})\right)\right)\right\rbrace$$

---


## Regresión penalizada en R

Vamos a realizar un pequeño ejemplo con el famoso dataset `iris`, al que primero lo preprocesaremos convenientemente para tener solo variables numéricas (la variable objetivo será `Sepal.Length`)

```{r}
#| code-fold: true
datos <- as_tibble(iris)
datos_preproc <-
  datos |> 
  fastDummies::dummy_cols(select_columns = "Species", remove_first_dummy = TRUE) |> 
  select(-Species)

# Ajuste Saturado
ajuste_saturado <- lm(data = datos_preproc, Sepal.Length ~ .)
ajuste_saturado |> summary()
```

---

## Regresión penalizada en R

Tras realizar el ajuste saturado (vamos a hacer una prueba sencilla, no hacemos colinealidad ni diagnosis, solo ajuste), [**seleccionamos modelos con AIC y BIC**]{.hl-yellow}

```{r}
#| eval: false
ajuste_AIC <- MASS::stepAIC(ajuste_saturado, direction = "both", k = 2)
ajuste_BIC <- MASS::stepAIC(ajuste_saturado, direction = "both", k = log(nrow(datos_preproc)))
```

```{r}
#| eval: false
ajuste_AIC |> summary()
ajuste_BIC |> summary()
```

---

## Regresión ridge en R

$$\widehat{\boldsymbol{\beta}}_{\lambda, \alpha} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left(\alpha \left\| \beta_{-1} \right\|_1  + (1-\alpha) \left\| \beta_{-1} \right\|_{2}^{2} \right)\right), \quad \lambda \geq 0,~0 \leq \alpha \leq 1$$

[**¿Cómo realizar una regresión penalizada en R?**]{.hl-yellow}. Para ello vamos a usar el completísimo paquete `{glmnet}`, cuya documentación puedes consultar en <https://glmnet.stanford.edu/articles/glmnet.html#linear-regression-family-gaussian-default>

. . .

Para realizar una ridge regresión, es decir $\alpha = 0$, basta con hacer uso de la función `glmnet()`, dándole en `x` matriz de variables (solo numéricas en formato matriz), en `y` la variable objetivo y `alpha = 0`

```{r}
library(glmnet)

y <- datos_preproc |> select(Sepal.Length) |> as.matrix()
x <- datos_preproc |> select(-Sepal.Length) |> as.matrix()

ajuste_ridge <- glmnet(x = x, y = y, alpha = 0)
```

---


## Regresión ridge en R

[**¿Qué tenemos guardado?**]{.hl-yellow}

* `$lambda`: tenemos los valores de $\lambda$ probados (por defecto, hay un argumento `nlambda` en `glmnet()` que vale 100, y la secuencia de valores los decide en función de otro argumento `lambda.min.ratio` que por defecto vale 0.01)

```{r}
ajuste_ridge$lambda
```

---

## Regresión ridge en R

[**¿Qué tenemos guardado?**]{.hl-yellow}

* `$dev.ratio`: tenemos lo que se conoce como «deviance» para las penalizaciones probadas, que es una generalización de $R^2$ para modelos más complejos (en nuestro caso es equivalente)

```{r}
ajuste_ridge$dev.ratio
```

---

## Regresión ridge en R

Fíjate que a [**mayor penalización (modelos más simples, más sparsity)**]{.hl-yellow}, el $R^2$ disminuye (o lo que es lo mismo, aumenta el SSE)

```{r}
ggplot(tibble("lambda" = log(ajuste_ridge$lambda),
              "R2" = ajuste_ridge$dev.ratio)) +
  geom_line(aes(x = lambda, y = R2)) +
  theme_minimal() + labs(x = "log-lambda", y = "R2")
```


---

## Regresión ridge en R

* `$a0`: tenemos guardados los [**interceptos $\widehat{\beta}_0$**]{.hl-yellow} de las regresiones ajustadas para cada penalización (en este caso 100 valores distintos)

* `$beta`: tenemos guardados en formato matricial los [**coeficientes $\widehat{\beta}_j$**]{.hl-yellow} (para $j \geq 1$) de las regresiones ajustadas para cada penalización (en este caso una matriz de 5 filas (5 predictoras) y 100 columnas (100 penalizaciones)

```{r}
dim(ajuste_ridge$beta)
length(ajuste_ridge$a0)
```

---

## Regresión ridge en R

Fíjate que a [**mayor penalización (modelos más simples, más sparsity)**]{.hl-yellow}, la [**norma Euclídea $\left\| \cdot \right\|_2$ de los coeficientes**]{.hl-yellow} disminuye (ya que penaliza más)


```{r}
ggplot(tibble("lambda" = log(ajuste_ridge$lambda),
              "norma" = sqrt(colSums(ajuste_ridge$beta^2)))) +
  geom_line(aes(x = lambda, y = norma)) +
  theme_minimal() + labs(x = "log-lambda", y = "norma")
```


---

## Selección de lambda en R

Para no tener que elegir manualmente el mejor $\lambda$, la función `cv.glmnet()` nos permite elegir el mejor  $\widehat{\lambda}_{k-CV}$ ([**penalización óptima elegida a través de validación k-fold**]{.hl-yellow})

* `nfolds`: indica el número $k$ de particiones en la validación cruzada.

* `type.measure`: métrica de error usada (por defecto lo que hemos llamado $SSE$ o $MSE$)

```{r}
set.seed(12345)
kfolds_lambda_ridge <- cv.glmnet(x = x, y = y, alpha = 0, nfolds = 20)
```


---

## Selección de lambda en R

```{r}
set.seed(12345)
kfolds_lambda_ridge <- cv.glmnet(x = x, y = y, alpha = 0, nfolds = 20)
```

* `$lambda`: los valores $\lambda$ probados.
* `$lambda.min`: el valor que nos proporciona el promedio de errores en validación más pequeño
* `$cvm`: valores $CV_k(\lambda)$


```{r}
# Lambda mínimo
kfolds_lambda_ridge$lambda.min

# Lambda mínimo manualmente
cv_min <- which.min(kfolds_lambda_ridge$cvm)
kfolds_lambda_ridge$lambda[cv_min]
```

---

## Selección de lambda en R


Es [**importante chequear que el mínimo no sea un extremo**]{.hl-yellow} de la secuencia de $\lambda$ (ya que sino no sabemos si el mínimo está fuera del rango).

```{r}
min(kfolds_lambda_ridge$lambda)
kfolds_lambda_ridge$lambda.min
```

. . .

Para solventarlo podemos [**proporcionar manualmente un grid (normalmente logarítmico)**]{.hl-yellow} de penalizaciones con las que debe probar la función `glmnet(..., lambda = ...)`

```{r}
# grid logarítmico
lambda_seq <- 10^seq(log10(max(kfolds_lambda_ridge$lambda)), log10(0.0001), l = 200)

set.seed(12345)
kfolds_lambda_ridge <-
  cv.glmnet(x = x, y = y, alpha = 0, nfolds = 20, lambda = lambda_seq)
kfolds_lambda_ridge$lambda.min
```

---

## Selección de lambda en R


* `$lambda.1se`: mejor penalización $\widehat{\lambda}_{k-1SE}$ según [**«one standard error rule»**]{.hl-yellow}

```{r}
kfolds_lambda_ridge$lambda.1se
kfolds_lambda_ridge
```

---

## Selección de lambda en R

Con la función `plot()` podemos dibujar de manera automática los distintos valores $CV_k(\lambda)$, resultado de promediar en los $k$-folds de la validación (la incertidumbre se representa con las barras de error). Con `abline()` podemos pintar las lineas horizontales para $\widehat{\lambda}_{k-CV}$ y $\widehat{\lambda}_{k-1SE}$.

```{r}
plot(kfolds_lambda_ridge)
abline(h = min(kfolds_lambda_ridge$cvm) + c(0, min(kfolds_lambda_ridge$cvsd)))
```

---

## Selección de lambda en R

Podemos replicarlo en ggplot ya que en `$cvlo` y `$cvup` tenemos guardado los valores superiores e inferiores en las validaciones probadas (los extremos de las barras de error)

```{r}
#| code-fold: true
ggplot() +
  geom_errorbar(data =
                  tibble("log_lambda" = log(kfolds_lambda_ridge$lambda),
                         "cvlo" = kfolds_lambda_ridge$cvlo,
                         "cvup" = kfolds_lambda_ridge$cvup),
                aes(x = log_lambda,
                    ymin = cvlo, ymax = cvup), color = "gray60") + 
		geom_point(data =
		             tibble("log_lambda" = log(kfolds_lambda_ridge$lambda),
		                    "MSE" = kfolds_lambda_ridge$cvm),
		           aes(x = log_lambda, y = MSE), color = "red") +
		geom_vline(xintercept = log(kfolds_lambda_ridge$lambda.min),
		           linetype = "dashed") +
		geom_vline(xintercept = log(kfolds_lambda_ridge$lambda.1se),
		           linetype = "dashed") +
  theme_minimal()
  
```


---

## Regresión ridge en R


Para [**aplicar el modelo de regresión ridge con la penalización óptima considerada**]{.hl-yellow} se puede hacer uso de `predict(kfolds_lambda_ridge, ...)`, indicándole que quieres «predecir» los coeficientes (`type = "coefficients"`) o con `coef()`


```{r}
predict(kfolds_lambda_ridge, type = "coefficients",
        s = kfolds_lambda_ridge$lambda.1se)
coef(kfolds_lambda_ridge)
```   

---

## Regresión ridge en R


::: callout-important
## Importante

Los coeficientes calculados son los promedios en validación. Si se quiere un ajuste "exacto" en el dataset original...

```{r}
ajuste_ridge_CV <- glmnet(x = x, y = y, alpha = 0,
                          lambda = kfolds_lambda_ridge$lambda.1se)
ajuste_ridge_CV$dev.ratio
ajuste_ridge_CV$a0
ajuste_ridge_CV$beta

predict(kfolds_lambda_ridge, type = "coefficients",
        s = kfolds_lambda_ridge$lambda.1se, newx = x)
```  

:::

---

## Regresión lasso en R


$$\widehat{\boldsymbol{\beta}}_{\lambda, \alpha} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \left( SSE \left(\boldsymbol{\beta} \right) + \lambda \left(\alpha \left\| \beta_{-1} \right\|_1  + (1-\alpha) \left\| \beta_{-1} \right\|_{2}^{2} \right)\right), \quad \lambda \geq 0,~0 \leq \alpha \leq 1$$

La única diferencia para implementar la [**regresión lasso en R**]{.hl-yellow} es $\alpha = 1$ (con la misma función `glmnet()`)

```{r}
ajuste_lasso <- glmnet(x = x, y = y, alpha = 1)
dim(ajuste_lasso$beta)
length(ajuste_lasso$a0)
ajuste_lasso$lambda
```

---

## Regresión lasso en R

Fíjate que, de nuevo, a [**mayor penalización (modelos más simples, más sparsity)**]{.hl-yellow}, el $R^2$ disminuye (o lo que es lo mismo, aumenta el SSE)

```{r}
ggplot(tibble("lambda" = log(ajuste_lasso$lambda),
              "R2" = ajuste_lasso$dev.ratio)) +
  geom_line(aes(x = lambda, y = R2)) +
  theme_minimal() + labs(x = "log-lambda", y = "R2")
```

---

## Regresión lasso en R

De nuevo a [**mayor penalización (modelos más simples, más sparsity)**]{.hl-yellow}, la [**norma absoluta $\left\| \cdot \right\|_1$ de los coeficientes**]{.hl-yellow} disminuye (ya que penaliza más)


```{r}
ggplot(tibble("lambda" = log(ajuste_lasso$lambda),
              "norma" = colSums(abs(ajuste_lasso$beta)))) +
  geom_line(aes(x = lambda, y = norma)) +
  theme_minimal() + labs(x = "log-lambda", y = "norma")
```


---

## Selección de lambda en R

De nuevo podemos hacer uso de `cv.glmnet()` para elegir la ([**penalización óptima elegida a través de validación k-fold**]{.hl-yellow})

```{r}
set.seed(12345)
kfolds_lambda_lasso <- cv.glmnet(x = x, y = y, alpha = 1, nfolds = 20)
min(kfolds_lambda_lasso$lambda)
kfolds_lambda_lasso$lambda.min
kfolds_lambda_lasso$lambda.1se

lambda_seq <- 10^seq(log10(max(kfolds_lambda_ridge$lambda)), log10(0.0001), l = 200)

set.seed(12345)
kfolds_lambda_lasso <- cv.glmnet(x = x, y = y, alpha = 1, nfolds = 20, lambda = lambda_seq)
min(kfolds_lambda_lasso$lambda)
kfolds_lambda_lasso$lambda.min
kfolds_lambda_lasso$lambda.1se
```

---


## Selección de lambda en R

Con la función `plot()` podemos dibujar de nuevo de manera automática los distintos valores $CV_k(\lambda)$: los números que aparecen arriba indican el [**número de predictores distintos de 0**]{.hl-yellow}

```{r}
plot(kfolds_lambda_lasso)
abline(h = min(kfolds_lambda_lasso$cvm) + c(0, min(kfolds_lambda_lasso$cvsd)))
```


---

## Regresión lasso en R

La predicción se realiza de la misma manera (depende de si queremos el promedio en validación o del conjunto de train original): ahora **algunos coeficientes están a 0** lo cual nos puede servir no solo como modelo sino como [**método de selección de variables**]{.hl-yellow} (ejecutando ahora una regresión sin penalizar pero sin las variables cuyos coeficientes son 0).

```{r}
predict(kfolds_lambda_lasso, type = "coefficients",
        s = kfolds_lambda_lasso$lambda.1se, newx = x)
```  

---

## Mínimo global

Uno de los [**problemas de la validación cruzada**]{.hl-red} es que podemos encontrarnos con lo que se conoce como [**L-shaped problem**]{.hl-yellow}: la curva de la cual pretendemos encontrar el mínimo no tiene un mínimo global. Veamos un ejemplo.

```{r}
#| code-fold: true
set.seed(12345)
p <- 100
n <- 300
x <- matrix(rnorm(n * p), n, p)
y <- 1 + rnorm(n)
lambdaGrid <- exp(seq(-10, 3, l = 200))

# Validación leave-1-out
plot(cv.glmnet(x = x, y = y, alpha = 1, nfolds = n, lambda = lambdaGrid))
```

Esto puede ser un indicio de que el **intercepto es significativo y no así el resto de predictores**

---

## Consistencia reg. penalizada



:::: columns
::: {.column width="55%"}

![](img/consistency-lasso.jpg)

:::

::: {.column width="45%"}

Como sucedía entre el BIC/AIC, en [Zhao and Yu (2006)](https://www.jmlr.org/papers/v7/zhao06a.html) se prueba como la [**selección lasso es consistente bajo la one standard rule**]{.hl-yellow} (bajo ciertas condiciones), pero no así cuando $\lambda$ es seleccionado por validación cruzada (incluye demasiados más predictores de los necesarios, como le sucedía al AIC)

Imagen obtenida de <https://bookdown.org/egarpor/PM-UC3M/lm-iii-shrink.html#lm-iii-shrink-varsel>

:::
::::

---

## Deberes

> Diseña un estudio que, con el dataset iris y considerando el modelo general de elastic-net, nos determina el mejor par $\left(\alpha, \lambda \right)$ (de manera conjunta).

# Clase n+1: regresión logística


[**Introducción a los modelos lineales generalizados (glm)**]{style="color:#444442;"}

---

## Regresión logística

Hasta ahora nuestra [**variable objetivo era una variable continua**]{.hl-yellow}. De hecho si echamos la vista atrás, siempre y cuando se cumplan las hipótesis, el modelo lineal cumple que


$$Y | \left( \boldsymbol{X} = \left( x_1, \ldots, x_p \right) \right)  \sim N \left(\beta_0 + \beta_1x_1 + \ldots + \beta_p x_p, \sigma_{\varepsilon}^2 \right)$$

. . .

¿Pero qué sucede cuando nuestra [**variable objetivo es cualitativa**]{.hl-yellow} (o cuantitativa discreta)? 

---

## Regresión logística

Vamos a empezar por el caso más sencillo: una [**variable objetivo binaria**]{.hl-yellow}. Para ello vamos a cargar el dataset `manif.rda` (basta con usar `load()` ya que la extensión `.rda` es un fichero nativo de `R`)

```{r}
load(file = "./datos/manif.rda")
data <- as_tibble(manif)
```

&nbsp;

En este caso la **variable objetivo** será `man` (binaria, asistió una persona o no a un manifestación) y contamos **solo con una predictora, la edad**, que es una cuantitativa continua.

```{r}
data |> count(man)
```

---

## Regresión logística

```{r}
#| code-fold: true
ggplot(manif |> mutate(man = factor(man, labels = c("Sí", "No"))),
       aes(x = edad, y = man)) +
  geom_point(aes(color = man), size = 3, alpha = 0.7) +
  theme_minimal() +
  labs(x = "Edad (años)", y = "¿Asistió a manifestación?",
       color = "Asistencia",
       title = "¿Cómo usar la regresión lineal para CLASIFICAR?",
       subtitle = "Reminder: la regresión lineal debe cumplir unas hipótesis")
```


Dado que `man` es una **variable binaria** podemos pensarla de la misma manera que pensamos en el [**lanzamiento de una moneda**]{.hl-yellow}: tiene dos estados posibles (1 y 0), y cada uno con una [**probabilidad**]{.hl-yellow} asignada

---

## Regresión logística


Si lo equiparamos al lanzamiento de una moneda, podemos asumir que la variable objetivo `man` sigue una [**distribución binomial (con n = 1) o de Bernoulli**]{.hl-yellow} con [**probabilidad de éxito (asistir) $p$**]{.hl-yellow} (y $1-p$ de no asistir) tal que


$$Y | \left( \boldsymbol{X} = \left( x_1, \ldots, x_p \right) \right)  \sim Ber \left(p \right)$$

. . .

Como sucedía en el caso de la regresión lineal, nuestro objetivo por tanto será [**estimar la esperanza de la variable objetivo**]{.hl-yellow} condicionada a la información de las predictoras

$$E \left[ Y | \left( \boldsymbol{X} = \left( x_1, \ldots, x_p \right) \right) \right] = p, \quad \widehat{E} \left[ Y | \left( \boldsymbol{X} = \left( x_1, \ldots, x_p \right) \right) \right] = \widehat{p}$$
Lo [**importante**]{.hl-yellow}: ¿cómo aprovechar lo que sabemos de modelización lineal para estimar $\widehat{p}$?



---

## Regresión logística


La idea más inmediata es, precisamente, [**asignarle a esa esperanza estimada la misma estimación de la regresión**]{.hl-yellow}, es decir, 

$$\widehat{E} \left[ Y | \left( \boldsymbol{X} = \left( x_1, \ldots, x_p \right) \right) \right] = \widehat{p}  = \widehat{\beta}_0 + \widehat{\beta}_1 X_1 + \ldots + \widehat{\beta}_p X_p$$

de manera que, si $G$ son las modalidades que puede adoptar la variable objetivo (en nuestro caso $G = \left\lbrace 0,1 \right\rbrace$)

$$\widehat{y_i} = j \quad \text{si} \quad  \widehat{P}(Y = j | X = \left(x_{1}, \ldots, x_{p} \right)) =  \arg \max_{g \in G} \widehat{P}(Y = g | X = \left(x_{1}, \ldots, x_{p} \right))$$

---

## Regresión logística

¿Qué pasaría si aplicásemos un modelo lineal?

```{r}
set.seed(12345)
library(rsample)
split <- initial_split(data, prop = 0.8, strata = man)
train <- training(split)
test <- testing(split)

ajuste_lineal <- lm(data, formula = man ~ .)
predict(ajuste_lineal, test)
```

. . .

Tenemos un [**problema**]{.hl-red}: si aplicamos una regresión lineal, no hay nada que nos garantice que la probabilidad predicha esté entre 0 y 1. ¡Puede salir [**incluso negativa**]{.hl-red}!

---

## Regresión logística

La idea será [**garantizar que la salida de la regresión lineal**]{.hl-yellow} acabe [**dentro del rango [0,1]**]{.hl-green}, encapsulando esa salida  con una [**función de enlace $g^{-1}:\mathbb{R} \to [0,1]$**]{.hl-yellow} tal que


$$P(Y = 1 | X = \left(x_{1}, \ldots, x_{p} \right)) =  p = g^{-1} \left(\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p\right) $$

$$g(p) = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$$

&nbsp;

¿Qué **condiciones** debe cumplir dicha función? ¿Nos vale cualquiera?

---

## Regresión logística

Necesitamos unas [**mínimas condiciones de regularidad**]{.hl-green}

* [**Invertible**]{.hl-yellow}: la función  $g:~[0,1] \to \mathbb{R}$ debe ser invertible, tal que $g^{-1}:~\mathbb{R} \to [0,1]$

* [**Soporte en [0, 1]**]{.hl-yellow}: la función $g:~[0,1] \to \mathbb{R}$ debe  estar definida para cualquier valor en $[0, 1]$

* [**Codiminio real**]{.hl-yellow}: la función $g^{-1}:~\mathbb{R} \to [0,1]$ debe estar definida para todo valor real (para cualquier salida de una regresión)

* [**Monótona creciente**]{.hl-yellow}:  dado que $\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$ cuantifica el [**efecto de los predictores en la probabilidad de éxito**]{.hl-yellow} de la variable objetivo, la probabilidad debe **aumentar según crezca dicha cantidad**, así que $g^{-1}$ nunca debe decrecer

 
---

## Regresión logística

En función de las distintas funciones de enlace podemos [**obtener distintos modelos**]{.hl-yellow} (siempre y cuando cumplan las 4 propiedades anteriores):

* [**Enlace uniforme (unit)**]{.hl-yellow}: función $g^{−1}(x) = x I_{0< x < 1} + I_{x \geq 1}$ (nos devuelve la salida de la regresión si está entre 0 y 1, y lo trunca a 0 en cualquier otro caso).

. . .

* [**Enlace probit**]{.hl-yellow}: función $g^{−1}(x) = \Phi (x)$ donde $\Phi(x)=\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{u^2}{2}} du$ es la **función de distribución acumulada de una normal **

. . .

* [**Enlace logit**]{.hl-yellow}: función $g^{−1}(x) = logistic(x):= \frac{e^{x}}{1 + e^{x}} = \frac{1}{1 + e^{-x}}$ basada en la distribución acumulada de una distribución logística.

&nbsp;

Esta [**última es la abordaremos en este curso**]{.hl-yellow} por ser la más común y fácil de interpretar

---

## Regresión logística

![](img/link-functions.jpg)

---

## Regresión logística

La función de enlace más usada es la [**función logit**]{.hl-yellow}, que no es más que la [**inversa de la función $logistic$**]{.hl-yellow} que hemos definido arriba. 

&nbsp;

Para **ahorar notación** llamaremos $\eta = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$ a la salida de la regresión y $\widehat{\eta} = \widehat{\beta}_0 + \widehat{\beta}_1 x_1 + \ldots + \widehat{\beta}_p x_p$  a su estimación

$$g(p) = logit(p) = \ln \left( \frac{p}{1-p} \right) = \eta$$

$$p = g^{−1}(\eta) = logistic(\eta):= \frac{1}{1 + e^{-\eta}}$$

---

## Regresión logística

$$g(p) = logit(p) = \ln \left( \frac{p}{1-p} \right) = \eta, \quad p = g^{−1}(\eta) = logistic(\eta):= \frac{1}{1 + e^{-\eta}}$$

Así la **estimación** buscada será

$$\widehat{p} = g^{−1}(\widehat{\eta} ) = \frac{1}{1 + e^{-\widehat{\eta} }} = \frac{1}{1 + e^{-\left(  \widehat{\beta}_0 + \widehat{\beta}_1 x_1 + \ldots + \widehat{\beta}_p x_p\right) }}$$

¿Cómo interpretar la salida de la regresión? ¿Qué sucederá con $\widehat{p}$ cuando $\widehat{\eta} > 0$? ¿Y  $\widehat{\eta}< 0$?

---

## Regresión logística

$$\widehat{p} = g^{−1}(\widehat{\eta} ) = \frac{1}{1 + e^{-\widehat{\eta} }} = \frac{1}{1 + e^{-\left(  \widehat{\beta}_0 + \widehat{\beta}_1 x_1 + \ldots + \widehat{\beta}_p x_p\right) }}$$

* Si $\widehat{\eta} = \widehat{\beta}_0 + \widehat{\beta}_1x_1 +\ldots + \widehat{\beta}_p x_p = 0$, $\widehat{p} = g^{−1}(\widehat{\eta} ) = \frac{1}{1 + e^{-\widehat{\eta} }} = 0.5$ --> la [**probabilidad de éxito es la misma que la de fracaso**]{.hl-yellow}

* Si $\widehat{\eta} = \widehat{\beta}_0 + \widehat{\beta}_1x_1 +\ldots + \widehat{\beta}_p x_p > 0$, $\widehat{p} = g^{−1}(\widehat{\eta} ) = \frac{1}{1 + e^{-\widehat{\eta} }} > \frac{1}{1 + e^{0}} > 0.5$ --> la [**probabilidad de éxito es mayor que la de fracaso**]{.hl-green} --> $\widehat{y} = 1$

* Si $\widehat{\eta} = \widehat{\beta}_0 + \widehat{\beta}_1x_1 +\ldots + \widehat{\beta}_p x_p < 0$, $\widehat{p} = g^{−1}(\widehat{\eta} ) = \frac{1}{1 + e^{-\widehat{\eta} }} > \frac{1}{1 + e^{0}} < 0.5$ --> la [**probabilidad de éxito es menor que la de fracaso**]{.hl-red} --> $\widehat{y} = 0$

---

## Cuotas u odds

$$g(p) = logit(p) = \ln \left( \frac{p}{1-p} \right) = \eta, \quad p = g^{−1}(\eta) = logistic(\eta):= \frac{1}{1 + e^{-\eta}}$$

Como hemos visto antes, la inversa es lo que se conoce como [**función logística**]{.hl-yellow} tal que $g(p) = logit(p) = \ln \left( \frac{p}{1-p} \right) = \eta$ es precisamente la salida de la regresión (lo que nos facilita su interpretación)

. . .

De hecho al cociente entre $p$ y $1-p$ es lo que se conoce como [**odds o cuotas**]{.hl-yellow}: probabilidad de éxito vs probabilidad de fracaso (como las cuotas de las casas de apuestas), cuantificando [**cuántas veces es más o menos probable el éxito que el fracaso**]{.hl-yellow}



---

## Cuotas u odds

Así podemos definir los [**log-odds**]{.hl-yellow} como el logaritmo de las cuotas

$$log-odds(Y)= \ln \left(\frac{p}{1-p} \right) =  \eta = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$$
$$odds (Y) =  \frac{p}{1-p}  = e^{\eta} = e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p} = e^{\beta_0} e^{\beta_1x_1} \ldots e^{\beta_px_p} $$

. . .

* $e^{\beta_0}$: estimación (media) de odds cuando todas las predictoras son cero --> el [**ratio esperado p:1−p (éxito vs fracaso)**]{.hl-yellow} cuando las predictoras se anulan

* $e^{\beta_j}$ para $j\geq 1$: [**incremento MULTIPLICATVO**]{.hl-yellow} medio de odds para un [**incremento ADITIVO unitario de la predictora**]{.hl-yellow} $X_j$ (siempre y cuando el resto permanezcan fijas) --> por cada unidad que se incremente $X_j$, el ratio éxito/fracaso se verá MULTIPLICADO por $e^{\widehat{\beta}_j}$ (si $\beta_j$ es positivo, aumenta; en caso contrario, disminuye).

---

## Ajuste en R

Vamos a realizar el ajuste en `R` a nuestro dataset haciendo uso de la función `glm()`, indicándole además de los datos y la variable objetivo, la **distribución de la variable objetivo (binomial)** y la **función de enlace** (logit)

```{r}
ajuste_logit <- glm(data = train, formula = man ~ ., family = binomial(link = "logit"))
ajuste_logit
```

El ajuste obtenido es $\widehat{\eta} = 2.592 - 0.089 X$, es decir,

$$\widehat{p} = \widehat{P} \left(Y = 1 | X = x\right) = \frac{1}{1 + e^{-\left(2.592 - 0.089 X \right)}}, \quad \widehat{odds} (Y) =  \frac{\widehat{p} }{1-\widehat{p} }  = e^{\widehat{\eta}} = e^{2.592} e^{- 0.089x}$$

---

## Ajuste en R

$$\widehat{p} = \widehat{P} \left(Y = 1 | X = x\right) = \frac{1}{1 + e^{-\left(2.592 - 0.089 X \right)}}, \quad \widehat{odds} (Y) =  \frac{\widehat{p} }{1-\widehat{p} }  = e^{\widehat{\eta}} = e^{2.592} e^{- 0.089x}$$

Si calculamos la exponencial de los coeficientes

```{r}
exp(coef(ajuste_logit))
```

* Es **13 veces más probable que una persona de 0 años asista a la manifestación a que no lo haga** (estimación poco útil ya que $X=0$ no está dentro del rango)

* Por **cada año que cumpla la persona**, la [**probabilidad de asistir frente a no asistir**]{.hl-yellow} se MULTIPLICA por 0.9146, es decir, se reduce un 8.54%.

---

## Evaluación: deviance


```{r}
ajuste_logit
```

En los modelos de clasificación [**no podemos calcular métricas como el error cuadrático medio, var residual o $R^2$**]{.hl-red} ya que no tenemos una objetivo continua.


Como vemos en la salida se nos proporciona dos medidas: **Null Deviance** y **Residual Deviance**

---

## Evaluación: deviance


```{r}
ajuste_logit
```

* [**Null Deviance**]{.hl-yellow}: al igual que en la bondad de ajuste de un modelo lineal se usa como referencia la SST, la [**anomalía nula o null deviance**]{.hl-yellow} es la «diferencia» al comparar la log-verosimilitud del [**modelo perfectamente sobreajustado vs un modelo sin parámetros (solo intercepto)**]{.hl-yellow}

$$D_0 = -2(\mathcal{L}_{\hat{\beta}_0} - \mathcal{L}_{saturado}) \geq 0, \quad \mathcal{L}_{modelo} = ln(P(observado | modelo))$$

---

## Evaluación: deviance

```{r}
ajuste_logit
```

* [**Residual Deviance**]{.hl-yellow}: la [**anomalía residual o anomalía del modelo o residual deviance**]{.hl-yellow} es la «diferencia» al comparar la log-verosimilitud del [**modelo perfectamente sobreajustado vs nuestro modelo**]{.hl-yellow}

$$D = -2(\mathcal{L}_{\widehat{\beta}} - \mathcal{L}_{saturado}) \geq 0$$

---

## Evaluación: deviance


Dado que $D_0$ mide la máxima distancia posible al modelo «perfecto» (sobreajustado) y $D$ mide la distancia de nuestro modelo, una forma de [**evaluar una regresión logística**]{.hl-yellow} es con el conocido como [**pseudo-$R^2$ o coeficiente de McFadden**]{.hl-yellow} (de todo lo mal que lo podía hacer, ¿cuánto lo he hecho?)

$$pseudo-R^2 = \frac{D_0 - D}{D_0} = 1 - \frac{D}{D_0}$$

. . .


El coeficiente de McFadden podemos extraerlo con la función `pR2()` del paquete `{pscl}`.

```{r}
library(pscl)
pR2(ajuste_logit)
1 - ajuste_logit$deviance/ajuste_logit$null.deviance
```


---


## Predicción logística

Una vez realizado el ajuste podemos usar como es habitual `predict(..., type = "response")` para obtener las predicciones de un conjunto de datos (en este caso lo vamos a hacer en train), teniendo claro que nos devuelve la [**probabilidad de ser 1**]{.hl-yellow}

```{r}
train |>
  mutate(prob_1 = predict(ajuste_logit, train, type = "response"),
         prob_0 = 1 - prob_1)
```

---

## Predicción logística

Con dichas probabilidades podemos construir un **dataset donde proporcionemos las cuotas, los log-odds y la clasificación de la clase propiamente dicha** (fijando un umbral a partir del cual una observación es clasificada como 1)

```{r}
umbral <- 0.5
pred_train <-
  train |> 
  mutate(prob_1 = predict(ajuste_logit, train, type = "response"), prob_0 = 1 - prob_1,
         odds = prob_1 / prob_0, log.odds = log(odds),
         pred_class = if_else(prob_1 > umbral, 1, 0))
pred_train
```

---

## Matriz de confusión

Una de las formas más habituales de [**evaluar un modelo de clasificación**]{.hl-yellow} es con la [**matriz de confusión**]{.hl-yellow}: ¿cuántos 1's reales son clasificados como 1? ¿cuántos 0's? ¿cuántos 1's reales son erroneamente clasificados como 0?

Para ello usamos `conf_mat()` del paquete `{yardstick}` (paquete de `{tidymodels}`), indicándole la columna con la clase real (`truth = ...`) y la columna con la clase predicha (`estimate = ...`), ambas como factor.

```{r}
library(yardstick)
conf_mat_train <-
  pred_train |>
  mutate(man = as_factor(man), pred_class = as_factor(pred_class)) |> 
  conf_mat(truth = man, estimate = pred_class)
conf_mat_train
```


El **modelo ha acertado 59 de las 60 observaciones etiquetadas con 0** y ha **acertado solo 9 de las 19 observaciones etiquetadas con 1**


---

## Métricas de evaluación

```{r}
conf_mat_train
```

Esa matriz de confusión puede ser analizada de diversas formas en función de la **pregunta que queramos contestar**. 

&nbsp;

De ahora en adelante llamaremos [**verdadero positivo/negativo (VP/VN)**]{.hl-yellow} a las observaciones 1's/0's que han sido clasificadas correctamente como 1's/0's, respectivamente. En nuestro ejemplo tenemos 59 VN y 9 VP.

Llamaremos  [**falsos positivo/negativo (FP/FN)**]{.hl-yellow} a las observaciones 0's/1's que han sido erroneamente clasificadas como 1's/0's, respectivamente. En nuestro caso tenemos 10 FN y 1 FP.


---

## Métricas de evaluación

```{r}
conf_mat_train
```

* ¿Qué [**proporción de observaciones han sido bien clasificadas**]{.hl-yellow}

&nbsp;

Dicha métrica se conoce como [**accuracy**]{.hl-yellow} (la suma de la diagonal de la matriz de confusión entre el total de observaciones, en nuestro caso un $0.86076$)

$$accuracy = \frac{VP + VN}{total}$$

---

## Métricas de evaluación

```{r}
conf_mat_train
```

* ¿Qué [**proporción de positivos reales han sido bien clasificados**]{.hl-yellow}?

&nbsp;

Dicha métrica se conoce como [**sensibilidad (sensitivity)**]{.hl-yellow} y aproxima la probabilidad de que una observación positiva sea clasificada como tal por el modelo (la probabilidad de que una PCR dé positiva en una persona que tiene realmente el COVID), en nuestro caso un $0.47368$

$$sensitivity = \frac{VP}{VP + FN}$$

---

## Métricas de evaluación

```{r}
conf_mat_train
```

* ¿Qué [**proporción de negativos reales han sido bien clasificados**]{.hl-yellow}?

&nbsp;

Dicha métrica se conoce como [**especificidad (specificity)**]{.hl-yellow} y aproxima la probabilidad de que una observación negativa sea clasificada como tal por el modelo (la probabilidad de que una PCR dé negativa en una persona que no tiene COVID), en nuestro caso un $0.98333$

$$specificity = \frac{VN}{VN + FP}$$

---

## Métricas de evaluación

```{r}
conf_mat_train
```

* ¿Qué [**concordancia (no aleatoria) ahí entre la realidad y la predicción**]{.hl-yellow}?

Dicha métrica se conoce como [**coeficiente kappa (de Cohen)**]{.hl-yellow} y aproxima la probabilidad de que la clase real y la clase predicha coincidan más allá de por motivos de azar (asumiendo independencia)

$$\kappa = \frac{P(a) - P(e)}{1-P(e)} = \frac{acc - P(e)}{1-P(e)}$$

donde $P(a)$ es la probabilidad empírica de acuerdo (es decir, el accuracy) y $P(e)$ la probabilidad empírica de acuerdo espúreo (acuerdo por azar)

---


## Métricas de evaluación

```{r}
conf_mat_train
```


$$\kappa = \frac{P(a) - P(e)}{1-P(e)} = \frac{acc - P(e)}{1-P(e)}$$

$$\begin{eqnarray}P(e) &=& P(real = 0, pred = 0) + P(real = 1, pred = 1) \nonumber \\ &=&  \frac{VN + FN}{total}* \frac{VN + FP}{total} + \frac{FP + VP}{total}* \frac{FN + VP}{total}\end{eqnarray}$$

En este caso $P(e) = 0.6937991$ y $\kappa = 0.54605$.

---

## Métricas de evaluación

```{r}
conf_mat_train
```

* ¿Qué [**proporción de los clasificados como positivos son realmente verdaderos positivos**]{.hl-yellow}

&nbsp;

Dicha métrica se conoce como [**precision**]{.hl-yellow} y aproxima la probabilidad de que una observación clasificada como positiva fuese realmente positiva (la probabilidad de que estés con covid si una PCR te da positiva), en nuestro caso un $0.9$

$$precision = \frac{VP}{VP + FP}$$

---

## Métricas de evaluación

```{r}
conf_mat_train
```

* ¿Qué [**proporción de los clasificados como positivos son mal clasificados como negativos**]{.hl-yellow}

&nbsp;

Dicha métrica se conoce como [**tasa de descubrimiento falso o false discovery rate (FDR)**]{.hl-yellow} (tasa de error de tipo I) y aproxima la probabilidad de que una observación clasificada como positiva no fuese realmente positiva (la probabilidad de que estés sano a pesar de que una PCR te da positiva), en nuestro caso un $0.1$

$$FDR = \frac{FP}{VP + FP} = 1 - precision$$

---


## Métricas de evaluación

Las métricas mencionadas, amén de otras muchas, pueden ser obtenidas haciendo `summary()` de la matriz de confusión (indicando que el evento de interés es el segundo nivel del factor, los 1's)

```{r}
conf_mat_train |> summary(event_level = "second")
```

---


## Métricas de evaluación


```{r}
conf_mat_train |>
  summary(event_level = "second") |> 
  slice(1:4)
```

Si nos fijamos en las primeras

* a priori parece que el [**modelo funciona bastante bien**]{.hl-yellow} ya que acierta el 86% de las veces

. . .

* de hecho la [**probabilidad de acierto en el caso de personas que no asisten**]{.hl-green} a la manifestación (0's) es del 98.3% pero ...

. . .

* la [**probabilidad de acierto en el caso de los asistentes**]{.hl-red} (1's) es solo del 47.4%

---

## Métricas de evaluación


```{r}
#| code-fold: true
conf_mat_train |>
  summary(event_level = "second") |> 
  slice(1:4)
```

Si pensamos nuestro modelo como una PCR

* la prueba [**acierta el 86% de las veces**]{.hl-yellow} 

* de hecho la [**probabilidad de que la PCR dé negativa estando sano**]{.hl-green} es del 98.3% (no hay apenas falsos positivos) pero ...

* la [**probabilidad de que la PCR dé positiva estando enfermo**]{.hl-red} (1's) es solo del 47.4% (hay muchísimos falsos negativos)


. . .

Si el objetivo fuese, por ejemplo [**evitar contagios...no parece muy buen método**]{.hl-red}

---

## Métricas de evaluación



Esto puede suceder por muchos motivos pero el principal es que, en nuestro caso, la [**muestra está desbalanceada**]{.hl-yellow}: el modelo ha podido aprender muy bien de los 0's pero mucho menos de los 1's (ya que solo hay un 25%).

```{r}
train |> count(man)
```

Por eso es **importante no quedarse solo con el accuracy** en la evaluación del modelo. De hecho, según $\kappa$, la probabilidad de concordancia entre realidad y predicción, excluyendo el azar, es solo de $0.545$.


---

## Curva ROC

Todas las [**métricas anteriores se basan en una decisión**]{.hl-yellow}: todo depende del [**umbral (threshold)**]{.hl-yellow} que hayamos usado para decidir que una observación es clasificada como 1 o como 0 (ya que, recuerda, el ajuste nos devuelve solo la probabilidad estimada de serlo $\widehat{p}$)

. . .

¿Qué pasaría si ese [**umbral fuese variando**]{.hl-yellow}, desde 0 hasta 1? ¿Cómo de bueno sería nuestro método si movemos ese umbral?

. . .


* Si el [**umbral tiende a cero**]{.hl-yellow} --> casi todas las observaciones serán predichas como 1's --> no habrá falsos negativos (todo positivo será clasificado como positivo) pero ningún negativo real será clasificado como tal --> [**sensibilidad tiende a 1, especificidad tiende a 0**]{.hl-yellow}

* Si el [**umbral tiende a 1**]{.hl-yellow} --> casi todas las observaciones serán predichas como 0's --> no habrá falsos positivos pero ningún positivo real será clasificado como tal --> [**sensibilidad tiende a 0, especificidad tiende a 1**]{.hl-yellow}


---

## Curva ROC

Podemos **calcular cada valor de sensibilidad y especificidad** con `roc_curve()` indicando la clase real a predecir (como factor), indicando el evento de interés (los 1's) y, en este caso, **necesitamos proporcionar la probabilidad estimada de ser 1** (en lugar de la clase predicha en sí como antes)

Lo que nos devuelve es la [**sensibilidad y especificidad para cada umbral**]{.hl-yellow}

```{r}
roc_data <- 
  pred_train |>
  mutate(man = as_factor(man)) |> 
  roc_curve(man, prob_1, event_level = "second")
roc_data
```

---

## Curva ROC

Podemos visualizar dicho dataset añadiendo `autoplot()` al dataset, lo que nos proporciona la conocida como [**curva ROC**]{.hl-yellow}: una curva que nos permite visualizar de manera global la [**calidad de nuestro clasificador para cada posible decisión**]{.hl-yellow}

```{r}
roc_data |> autoplot()
```

---

## Curva ROC


```{r}
#| echo: false
roc_data |> autoplot()
```


Fíjate que en el **eje X se visualiza 1 - especificidad**, es decir, la probabilidad empírica de que el método clasifique mal los 0's: la [**curva ROC visualiza la probabilidad de falso positivo vs verdadero positivo**]{.hl-yellow}

Cuando el **umbral es muy bajo**, dado que todo será clasificado como positivo, clasifica perfecto los verdaderos positivos (sensibilidad  = 1) pero todos los negativos son falsos positivos (1-especificidad = 1). Y al contrario.

---

## Curva ROC

```{r}
#| echo: false
roc_data |> autoplot()
```


Lo importante de esta curva es lo que se conoce como [**AUC o área debajo de la curva**]{.hl-yellow}, que podemos obtener con `roc_auc()`: buscamos un clasificador cuya área sea lo más próxima a 1.

```{r}
pred_train |>
  mutate(man = as_factor(man)) |> 
  roc_auc(man, prob_1, event_level = "second")
```

---


## Curva ROC

![](img/roc-auc.jpg)

---

## Sobre/bajomuestreo

Como hemos visto, tener la [**muestra desbalanceada**]{.hl-yellow} puede ser un problema. Para resolverlo, podemos (antes de realizar el ajuste) lo que se conoce como [**sobre/bajo muestreo**]{.hl-yellow}

. . .

La única información que vamos a usar para ello (para no hacer «trampas») es el **conjunto de entrenamiento**

```{r}
train |>
  count(man) |> 
  mutate(porc = 100*n/sum(n))
```

En él observamos que contamos con un **75.9% de 0's y 19% de 1's**. ¿Cómo [**equilibrar los porcentajes**]{.hl-yellow}? Parece obvio que hay dos formas: subir el número de 1's o bajar el número de 0's.

---

## Sobre/bajomuestreo

Las dos ideas buscan [**equilibrar dichos porcentajes**]{.hl-yellow}

* [**Bajomuestreo**]{.hl-yellow}: dejamos [**fija la cantidad de la clase minoritaria**]{.hl-yellow} (los 1's en este caso) y [**reducimos la cantidad de la clase mayoritaria**]{.hl-yellow} (los 0's) para que haya algo parecido a un equilibrio.

. . .

Para ello **filtramos los individuos de la clase mayoritaria**, **seleccionamos un % aleatorio** de ellas (en este caso un 33% ya que basta con tener 20 de las 60 para ese equilibrio) y **unimos la clase minoritaria**.

```{r}
set.seed(12345)
downsampling_data <-
  train |> filter(man == 0) |> 
  slice_sample(prop = 0.33, replace = FALSE) |> 
  bind_rows(train |> filter(man == 1))

downsampling_data |>
  count(man) |> mutate(porc = 100*n/sum(n))
```


---

## Sobre/bajomuestreo

* [**Sobremuestreo**]{.hl-yellow}: dejamos [**fija la cantidad de la clase mayoritaria**]{.hl-yellow} (los 0's en este caso) y [**aumentamos la cantidad de la clase minoritaria**]{.hl-yellow} (los 1's) para que haya algo parecido a un equilibrio.

. . .

Para ello **filtramos los individuos de la clase minoritaria**, **seleccionamos un % aleatorio** de ellas (en este caso un 300% ya pasar de 20 a 60, x3, para ese equilibrio) y **unimos la clase mayoritaria**.

```{r}
set.seed(12345)
oversampling_data <-
  train |> filter(man == 1) |> 
  slice_sample(prop = 3, replace = TRUE) |> 
  bind_rows(train |> filter(man == 0))

oversampling_data|>
  count(man) |>  mutate(porc = 100*n/sum(n))
```

---

## Sobre/bajomuestreo

:::: columns
::: {.column width="50%"}

```{r}
set.seed(12345)
downsampling_data <-
  train |> filter(man == 0) |> 
  slice_sample(prop = 0.33, replace = FALSE) |> 
  bind_rows(train |> filter(man == 1))
```

:::

::: {.column width="50%"}

```{r}
set.seed(12345)
oversampling_data <-
  train |> filter(man == 1) |> 
  slice_sample(prop = 3, replace = TRUE) |> 
  bind_rows(train |> filter(man == 0))
```

:::
::::

* [**Bajomuestreo**]{.hl-yellow}: la principal [**ventaja**]{.hl-green} es que los nuevos datos son una submuestra real de los datos originales; la [**desventaja**]{.hl-red} es que se reduce considerablemente el tamaño muestral.

* [**Sobremuestreo**]{.hl-yellow}: la principal [**ventaja**]{.hl-green} es que no reducimos el tamaño muestral; la [**desventaja**]{.hl-red} es que los datos nuevos son generados artificialmente (en este caso, son observaciones repetidas, de ahí el `replace = TRUE`)

Existen otras técnicas (SMOTE, ROSE, etc) que lo que hacen es [**simular datos sintéticos**]{.hl-yellow}, bien usando la distribución de cada variable o bien tomando los datos originales y simulando nuevos perturbando con ruido.

---

## Ajuste con bajomuestreo

```{r}
#| code-fold: true
ajuste_logit_down <-
  glm(data = downsampling_data, formula = man ~ ., family = binomial(link = "logit"))
pred_train_down <-
  downsampling_data |> 
  mutate(prob_1 = predict(ajuste_logit_down, downsampling_data, type = "response"),
         prob_0 = 1 - prob_1, odds = prob_1 / prob_0, log.odds = log(odds),
         pred_class = if_else(prob_1 > 0.5, 1, 0))

conf_mat_train_down <-
  pred_train_down |>
  mutate(man = as_factor(man), pred_class = as_factor(pred_class)) |> 
  conf_mat(truth = man, estimate = pred_class)

pred_train_down |>
  mutate(man = as_factor(man)) |> roc_auc(man, prob_1, event_level = "second")
conf_mat_train_down |> summary(event_level = "second")
```

[**Empeora el accuracy y un poco la especificidad**]{.hl-red} pero [**mejora la sensibilidad**]{.hl-green}

---

## Ajuste con sobremuestreo

```{r}
#| code-fold: true
ajuste_logit_over <- 
  glm(data = oversampling_data, formula = man ~ ., family = binomial(link = "logit"))
pred_train_over <-
  oversampling_data |> 
  mutate(prob_1 = predict(ajuste_logit_over, oversampling_data, type = "response"),
         prob_0 = 1 - prob_1, odds = prob_1 / prob_0, log.odds = log(odds),
         pred_class = if_else(prob_1 > 0.5, 1, 0))

conf_mat_train_over <-
  pred_train_over |>
  mutate(man = as_factor(man), pred_class = as_factor(pred_class)) |> 
  conf_mat(truth = man, estimate = pred_class)

pred_train_over |>
  mutate(man = as_factor(man)) |> roc_auc(man, prob_1, event_level = "second")
conf_mat_train_over |> summary(event_level = "second")
```

[**Empeora el accuracy y mucho la especificidad**]{.hl-red} pero [**mejora la sensibilidad**]{.hl-green}


---

## Caso real: challenger

Práctica con el dataset `challenger.txt` subido al campus. El objetivo será obtener la probabilidad de accidente en distintos lanzamientos espaciales, donde `fail.field` es **nuestra variable objetivo (si hubo o no accidente debido a un fallo en las juntas)** en función de si hubo o no fallo en los inyectores, la temperatura exterior, la presión de las juntas y la presión de los inyectores

```{r}
datos <-
  read_delim(file = "./datos/challenger.txt")
datos <- 
  datos |> select(-contains("nfails"))
datos
```

---

## Pues...ha sido un placer

![](img/me.jpeg)

[**Espero que hayáis aprendido algo <3**]{style="color:#444442;"}


