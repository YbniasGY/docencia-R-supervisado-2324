---
title: "Aprendizaje Supervisado I"
subtitle: "M√©todos de predicci√≥n lineal"
title-slide-attributes:
  data-background-image: img/data-science-2.jpeg
  data-background-size: cover
  data-background-opacity: "0.2"
author: "Grado en Ciencia de Datos Aplicada ‚Ä¢ curso 2023-2024"
affiliation: Facultad de Estudios Estad√≠sticos (UCM)
lang: es
language: custom_lang.yml
format: 
  revealjs:
    theme: [default, style.scss]
    chalkboard: true
    multiplex: true
    menu:
      side: left
      width: normal
    footer: "[<strong>Javier √Ålvarez Li√©bana</strong>](...) ‚Ä¢ Grado en Ciencia de Datos Aplicada (UCM) ‚Ä¢ curso 2023-2024"
    slide-number: c/t
execute:
  echo: true
---

# Empieza lo bueno...empieza la modelizaci√≥n

[**Vamos a juntar las piezas del puzzle para hacer ¬´magia¬ª**]{style="color:#444442;"}

```{r}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
```

---

## ¬°Buenas!

[**Correo**]{.hl-green}: **<javalv09@ucm.es>**. [**Despacho**]{.hl-green}: 722 (3¬™ planta). [**Tutor√≠as**]{.hl-green}: lunes (14:30-16:00), martes (13:00-15:00) y viernes (13:00-14:00).

::: columns
::: {.column width="30%"}
![](img/me.jpeg)
:::

::: {.column width="70%"}
::: incremental
-   [**Javier √Ålvarez Li√©bana**]{.hl-yellow}, de Carabanchel (Bajo).

-   Licenciado en Matem√°ticas (UCM). [**Doctorado en estad√≠stica**]{.hl-yellow} (UGR).

-   Encargado de la [**visualizaci√≥n y an√°lisis de datos covid**]{.hl-yellow} del Principado de Asturias (2021-2022).

-   Miembro de la [**Sociedad Espa√±ola de Estad√≠stica e IO**]{.hl-yellow} y la [**Real Sociedad Matem√°tica Espa√±ola**]{.hl-yellow}.
:::
:::
:::

. . .

Actualmente, [**investigador y docente en la Facultad de Estad√≠stica de la UCM**]{.hl-yellow}. Divulgando por [**Twitter**](https://twitter.com/dadosdelaplace) e [**Instagram**](https://instagram.com/javieralvarezliebana)


---

## Objetivos

::: columns
::: {.column width="37%"}
![](https://assets-global.website-files.com/6092cb6b4ac959f39728dd26/6188a97fa499b5fbfe410417_target%20(1).png)
:::

::: {.column width="63%"}
::: incremental
- Empezar a [**relacionar asignaturas**]{.hl-yellow} como matem√°ticas, inferencia y R.

- Aprender los fundamentos del [**aprendizaje estad√≠stico**]{.hl-yellow} (ahora llamado Machine Learning o data science)

- Pasar de los descriptivo a lo predictivo: [**construir nuestros primeros modelos**]{.hl-yellow}

- Entender en profundidad el contexto de la [**predicci√≥n lineal**]{.hl-yellow}.


:::
:::
:::

---

## Evaluaci√≥n

-   [**Asistencia**]{.hl-yellow}. Se [**valorar√° muy positivamente**]{.hl-purple} la participaci√≥n. Si se [**restar√°n puntos si eres expulsado de clase**]{.hl-purple}: -0.3 la 1¬™ vez, -0.6 la 2¬™, -1.2 la 3¬™...

. . .

- [**Evaluaci√≥n continua**]{.hl-yellow}: 1 **examen a papel** (25%), **2 entregas R en clase** (20%-30%) y **entrega grupal √∫ltimo d√≠a** (3-6p obligatoriamente, 25%)
  
. . .

- [**Examen final**]{.hl-yellow}:
  - [**Menos de un 6 (sin la grupal)**]{.hl-purple} -> examen de R en lugar de la grupal.
  - [**M√°s de un 6.5 (con la grupal)**]{.hl-purple} -> podr√°s decidir **peso del final entre un 20% y un 100%**.
  - [**Entre 5 y 6.5 (con la grupal)**]{.hl-purple} -> podr√°s decidir **peso del final entre un 40% y un 100%**.
  - [**Por debajo de 5 (con la grupal)**]{.hl-purple} -> podr√°s decidir **peso del final entre un 70% y un 100%**.
  

---

## Planificaci√≥n


* [**Entrega I (20%)**]{.hl-yellow}: semana 27-29 de febrero de 2024

* [**Entrega II (30%)**]{.hl-yellow}: semana 2-4 de abril de 2024

* [**Parcial con papel y boli (25%)**]{.hl-yellow}: 16-18 de abril de 2024 

* [**Entrega grupal (25%)**]{.hl-yellow}: 9 de mayo de 2024 (incluyendo lo visto hasta el 30 de abril)

&nbsp;

(se admiten propuestas)

. . . 

[**Clase a recuperar otro d√≠a**]{.hl-yellow}: clase del 21 de marzo.

---

## Materiales

* [**Diapositivas**]{.hl-yellow}: las diapositivas que usaremos en el aula a lo largo del curso, estructuradas por clases, estar√°n disponibles y actualizadas en **<https://javieralvarezliebana.es/docencia-R-supervisado-2324/diapos>** 

En el men√∫ de las diapositivas (abajo a la izquierda) tienes una [**opci√≥n para descargarlas en pdf**]{.hl-yellow} en `Tools` (consejo: no lo hagas hasta el final del curso ya que ir√°n modific√°ndose)
  
&nbsp;

* [**Material**]{.hl-yellow}: [**scripts de cada tema**](https://github.com/dadosdelaplace/docencia-R-supervisado-2324/tree/main/material) y materiales extras

* [**Res√∫menes de paquetes**]{.hl-yellow}: [**chuletas de los paquetes**](https://github.com/dadosdelaplace/docencia-R-supervisado-2324/tree/main/fichas%20paquetes) en formato .pdf

---


## Requisitos

Para el curso los √∫nicos requisitos ser√°n:

1.  [**Conexi√≥n a internet**]{.hl-yellow} (para la descarga de algunos datos y paquetes).

2.  [**Instalar R y RStudio**]{.hl-yellow}: la descarga la haremos (gratuitamente) desde <https://cran.r-project.org/> y  <https://posit.co/download/rstudio-desktop/>

3. Se dar√°n por asumido conocimientos aprendidos de [**R base, tidyverse y ggplot**]{.hl-yellow}

4. Se dar√°n por asumido conocimientos aprendidos de [**Quarto, diapositivas en Quarto y Github**]{.hl-yellow}. Para las entregas [**SOLO SE VALORAR√Å**]{.hl-purple} la salida html correspodiente.

5. [**Recomendable**]{.hl-yellow}: saber usar la calculadora en modo estad√≠stico.


---

## Listado de clases

* [**Clase 1**](#clase-1): repaso de descriptiva
* [**Clase 2**](#clase-2): importancia de la visualizaci√≥n
* [**Clase 3**](#clase-3): intro al aprendizaje estad√≠stico. Sesgo vs varianza. Supervisado vs no supervisado. Correlaci√≥n vs dependencia vs causalidad.


# Clase 1: repaso {#clase-1}

[**Objetivo de la predicci√≥n lineal. Concepto de linealidad. Repaso de estad√≠stica descriptiva**]{style="color:#444442;"}

---

## ¬øQu√© es predecir?

Como veremos m√°s adelante, en esta asignatura vamos a tratar principalmente lo que se conoce el aprendizaje estad√≠stico como [**predicci√≥n (continua)**]{.hl-yellow}

. . .

Dada una [**variable objetivo (variable dependiente)**]{.hl-yellow}, y con la informaci√≥n aportada por un conjunto de [**variables predictoras (covariables)**]{.hl-yellow}, el objetivo ser√° construir un modelo que consiga dar una estimaci√≥n/predicci√≥n lo ¬´mejor posible¬ª

. . .

Es importante que - de momento - distingamos dos conceptos:

* [**Estimaci√≥n**]{.hl-yellow}: el modelo aprende de unos datos e intenta estimar dichos valores que ha usado.
* [**Predicci√≥n**]{.hl-yellow}: el modelo aprende de unos datos e intenta estimar valores que el **modelo no conoce**.

M√°s adelante los llamaremos ¬´predicci√≥n en train¬ª y ¬´predicci√≥n en test¬ª

---

## ¬øQu√© es la linealidad?

En matem√°ticas decimos que una funci√≥n $f(x)$ es [**lineal**]{.hl-yellow} cuando se cumple:

* [**Propiedad aditiva**]{.hl-purple}: $f(x + y) = f(x) + f(y)$

* [**Propiedad homog√©nea**]{.hl-purple}: $f(k*x) = k*f(x)$ (donde $k$ es una constante en $\mathbb{R}$).

Ambas se pueden resumir en $f(a*x + b*y) = a*f(x) + b*f(y)$

. . .

En estad√≠stica llamamos [**modelo de predicci√≥n lineal**]{.hl-yellow} a un modelo que usa la informaci√≥n de covariables $X_1, X_2, \ldots, X_p$, de manera que su informaci√≥n siempre [**se relacionen entre s√≠ con sumas y restas**]{.hl-yellow}.

- [**Ejemplos lineales**]{.hl-green}: $y = 2*x_1 - 3$ o  $y = 4 - \frac{x_1}{2} + 3*x_2$

- [**Ejemplos no lineales**]{.hl-red}: $y = 2*\frac{1}{x_1}$ o  $y = 4 - x_{1}^{2} - x_2$ o $y = ln(x_1) + cos(x_2)$

En general son modelos que se pueden representar con rectas (una sola predictora), planos (dos predictoras), etc.

---

## Repaso: continua vs discreta

* **Caracteres**: cada una de las caracter√≠sticas o cualidades que se podr√≠an medir o analizar para cada individuo de la poblaci√≥n (y de los que disponemos el valor para cada individuo de la muestra).

* **Modalidades**: diferentes valores que puede adoptar una caracter√≠stica o variable.

&nbsp;

Como veremos m√°s adelante, en el √°mbito del aprendizaje estad√≠stico va a ser fundamental tener clara la diferencia entre una [**variable cualitativa y cuantitativa**]{.hl-purple} y entre [**variable continua y otra discreta**]{.hl-yellow}. ¬øCu√°l es la diferencia?


---

## Repaso: continua vs discreta

Imagina las siguientes variables:

- ¬øTienes hermanos? (si/no)
- Color de zapatillas
- Nivel de estudios
- N√∫mero de hermanos
- N√∫mero de pelos en la cabeza
- Resultado de un dado
- Temperatura ¬∫C
- Estatura o peso

&nbsp;

[**¬øCu√°l es la diferencia entre ellas**]{.hl-yellow}


---

## Repaso: continua vs discreta



* [**Cualitativas**]{.hl-yellow}: **cualidades o categor√≠as**. Ejemplos: sexo, estado civil, estudios, etc.
  * [**Ordinales**]{.hl-purple}: admiten relaci√≥n jerarquica (suspenso-aprobado-bien, sano-leve-grave).
  * [**Nominales**]{.hl-purple}: no tienen asociada una jerarqu√≠a (sexo, religi√≥n, color de zapatillas, etc).

. . .

* [**Cuantitativas**]{.hl-yellow}: cuantificaci√≥n num√©rica.
  * [**Discretas**]{.hl-purple} (barras): se pueden contar y enumerar (aunque sean infinitos) (granos de arena, n¬∫ hermanos, etc) ‚Üí se **suman**
  * [**Continuas**]{.hl-purple}: toman infinitos valores y entre dos valores cualesquiera hay a su vez infinitas opciones (estatura, peso, etc) ‚Üí se **integran**

---

## Repaso: continua vs discreta

![](img/vitro-fuego-discretas.jpg)


---

## Repaso: poblaci√≥n vs muestra

En estad√≠stica llamaremos [**poblaci√≥n**]{.hl-yellow} al universo te√≥rico, al colectivo de individuos a estudiar, o de posibles elementos/eventos de los podr√≠amos tener observaciones (ejemplo: 47 millones de espa√±oles).

. . .

Cada uno de esos elementos o eventos se llaman **individuos**.

. . .

Aunque nuestro objetivo ser√° conocer algunas de las propiedades de la poblaci√≥n, la [**poblaci√≥n suele ser inaccesible**]{.hl-red} en su totalidad ‚Üí [**SELECCI√ìN**]{.hl-green} de un conjunto de individuos

---

## Repaso: poblaci√≥n vs muestra

Para ello en estad√≠stica usamos lo que se conoce como [**muestra**]{.hl-yellow}: un subconjunto finito, de tama√±o $n$, ¬´representativo¬ª de la poblaci√≥n (en estudio estad√≠stico realizado sobre la totalidad de una poblaci√≥n se denomina **censo**).

. . .

Para ello existe una rama conocida como [**muestreo**]{.hl-yellow}

* [**Probabil√≠stico**]{.hl-purple}: todos los individuos tienen oportunidad de ser seleccionados, y la probabilidad de que suceda puede ser modelizada (al azar, por estratos, etc).

* [**No probabil√≠stico**]{.hl-purple}: algunos elementos de la poblaci√≥n no tienen posibilidad de selecci√≥n (sesgo de exclusi√≥n), o su probabilidad no puede ser conocida.

. . .

ü§î ¬øSer√≠a adecuado hacer una encuesta sobre el streamer favorito de los j√≥venes a trav√©s de una encuesta realizada por tel√©fono fijo?


---

## Sesgos en el muestreo

![](img/sampling-bias.jpg)

. . .

[**Sesgo de selecci√≥n**]{.hl-yellow}: aparece cuando no se tiene en cuenta la forma en la que se han recogido los datos.

--- 


## Sesgos en el muestreo


![](img/dewey.jpg)

El ejemplo m√°s famoso es el caso [**¬´Dewey defeats Truman¬ª (Dewer derrota a Truman)**]{.hl-yellow}, el titular con el que abri√≥ el Chicago Tribune en 1948, el mismo d√≠a en el que Truman gan√≥ al rep√∫blicano Dewer en las elecciones de 1948: sin esperar a los resultados, se basaron en una encuesta telef√≥nica (sin contar con el sesgo que, en aquella √©poca, solo la clase alta ten√≠a tel√©fono).

---



## Sesgos en el muestreo

![](img/superviviente.jpg)

¬øD√≥nde reforzar√≠as los aviones?

---

## Sesgos en el muestreo

![](img/superviviente.jpg)



El [**sesgo del superviviente**]{.hl-yellow} (un tipo de sesgo de selecci√≥n) aparece cuando se toma una muestra de un fen√≥meno ignorando si los individuos elegidos tienen las mismas opciones respecto al mismo.


---

## Repaso: medidas de centralizaci√≥n

* [**Media**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la media muestral $\overline{x}$ se define como la **suma de todos los valores dividida por el tama√±o muestral**

$$\overline{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$

. . .

[**Geom√©tricamente**]{.hl-purple}: es el **valor ¬´m√°s cercano¬ª de todos los datos a la vez** (minimiza las distancias al cuadrado)

---

## Media muestral


[**VENTAJAS**]{.hl-green}

* F√°cil de calcular y entender
* F√°cil y eficiente de programar
* Siempre existe (para cuantitativas)

. . .

[**DESVENTAJAS**]{.hl-red}

* No es un valor de los datos (la media de {1, 2, 3, 4} es 2.5)
* **Poco robusta** (valores at√≠picos le afectan mucho)
* Solo se puede definir para variables cuantitativas


---

## Repaso: medidas de centralizaci√≥n

* [**Mediana**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la mediana muestral se define como el **valor que es mayor o igual que al menos el 50%**, y menor igual que al menos el 50% de los datos

$$Me_{x} = \arg \min_{x_i} \left\lbrace F_i > 0.5 \right\rbrace, \quad Me_x = e_{i-1} + \frac{0.5 - F_{i-1}}{F_i - F_{i-1} }a_i$$

La mediana es el [**valor de en medio**]{.hl-purple} si ordenamos los datos (y si se pueden ordenar...)

---

## Mediana muestral


[**VENTAJAS**]{.hl-green}

* Suele ser un valor de la muestra
* Un poco m√°s robusta que la media


. . .

[**DESVENTAJAS**]{.hl-red}

* Muy ineficiente (requiere un algoritmo de ordenaci√≥n)
* Solo definida para  cuantitativas o cualitativas ordinales

---


## Repaso: medidas de centralizaci√≥n

* [**Moda**]{.hl-yellow}: dada una muestra $\boldsymbol{x} =\left(x_1, \ldots, x_n \right)$, la moda muestral se define como el **valor o valores m√°s repetidos** (en caso de que existan).

$$Mo_x = \arg \max_{x_i} f_i, \quad Mo_x = e_{i-1} + \frac{d_i - d_{i-1}}{\left(d_i - d_{i-1} \right) + \left(d_i - d_{i+1} \right)}a_i$$


. . .

[**Gr√°ficamente**]{.hl-purple}: representa el ¬´pico¬ª de un diagrama de barras o un histograma

---

## Moda muestral


[**VENTAJAS**]{.hl-green}

* Es un valor de la muestra
* Muy robusta
* **Se puede calcular para cualquier** cuanti o cuali



. . .

[**DESVENTAJAS**]{.hl-red}

* No siempre existe (amodal) y pueden existir varias (bimodal, trimodal, etc)
* Poco usada en inferencia


---

## Repaso: medidas de centralizaci√≥n

![](img/ine-salarios-oculto.jpg)

**¬øCu√°l es la mediana, la media y la moda?**

---

## Repaso: medidas de centralizaci√≥n

![](img/ine-salarios.jpg)

---

## Repaso: medidas de dispersi√≥n

![](img/iker-jimenez.jpg)

¬øQu√© tiene que ver la imagen con la dispersi√≥n?


---

## Repaso: medidas de dispersi√≥n

![](img/extremos.jpg)

El cambio clim√°tico no solo es porque aumente la [**temperatura media (centralizaci√≥n)**]{.hl-yellow} sino por la aparici√≥n cada vez m√°s frecuente de fen√≥menos extremos 


Aumento de la [**variabilidad**]{.hl-yellow} ‚Üí aumento de la [**DISPERSI√ìN**]{.hl-yellow}

---

## Repaso: medidas de dispersi√≥n

[**¬øC√≥mo medir lo que se alejan los datos de la media?**]{.hl-yellow}

. . .

![](img/primera-idea-varianza.jpg)

Una **primera idea** podr√≠a ser [**medir la distancia de cada dato al centro**]{.hl-yellow}, es decir, restar cada dato de la media, y despu√©s realizar su promedio.

---

![](img/wait-for-it.jpg)

---

## Repaso: medidas de dispersi√≥n

Imagina que tenemos la siguiente muestra $X = \left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$.

**¬øCu√°nto vale la media?**

. . .

La media vale 0 y la distancia a ella es...la propia muestra $\left\lbrace -5, -3, -1, 0, 1, 3, 5 \right\rbrace$. **¬øCu√°l es el promedio de dichas distancias?**

. . .

Pues...de nuevo vale 0.

. . .

Si la dispersi√≥n es 0...[**¬øno hay dispersi√≥n?**]{.hl-red} ¬øNo deber√≠a de dar 0 solo cuando los datos sean constantes?

---

## Repaso: medidas de dispersi√≥n

Para **evitar que se cancelen** los signos lo que haremos ser√° calcular el [**promedio PERO de las distancias al cuadrado**]{.hl-yellow}, la conocida como [**varianza**]{.hl-yellow}

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 $$

. . .


::: callout-warning
# Cuidado

Tomar el valor absoluto (para evitar que se cancelen los signos) suele ser una mala idea en matem√°ticas (no es derivable como funci√≥n).
:::

---

## Repaso: medidas de dispersi√≥n


[**Problema**]{.hl-red}: si los datos est√°n en metros, la varianza estar√° en...metros cuadrados


. . .

:::: columns
::: {.column width="50%"}

¬øTiene sentido medir la dispersi√≥n de nuestra estatura en baldosas?

:::

::: {.column width="50%"}

![](img/albert-rivera.jpg)
:::

::::

---

## Repaso: medidas de dispersi√≥n

Para tener una [**medida de dispersi√≥n en las unidades de los datos**]{.hl-yellow} calcularemos la [**desviaci√≥n t√≠pica**]{.hl-yellow}, como la ra√≠z cuadrada de la varianza

$$s_{x} = \sqrt{s_{x}^{2}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2} = \sqrt{\overline{x^2} - \overline{x}^2}$$


---

## Repaso: medidas de dispersi√≥n

Todav√≠a tenemos un peque√±o problema.

Imagina que queremos **comparar la dispersi√≥n de dos conjuntos** de datos, estaturas de personas y di√°metros de n√∫cleos de c√©lulas. Y Supongamos que las medias son 170 cm y 5 micr√≥metros, y la desviaci√≥n t√≠pica de 1 cm y 1.5 micr√≥metros.

[**¬øQu√© conjunto de datos es m√°s disperso?**]{.hl-yellow}

. . .

Para tener una  **medida de dispersi√≥n adimensional** definiremos el [**coeficiente de variaci√≥n**]{.hl-yellow} 

$$CV_{x} = \frac{s_{x}}{\left| \overline{x} \right|}$$

---

## Repaso: medidas de localizaci√≥n

Las [**medidas de posici√≥n o localizaci√≥n**]{.hl-yellow} nos localizan los datos, siendo valores que **nos dividen un conjunto ordenado** en subconjuntos del mismo tama√±o (ejemplo: mediana es percentil 50).

* **Percentil**: valores $P_{\alpha}$ del conjunto ordenado que dejan por debajo, al menos, el $\alpha$% de datos y $\alpha$% por encima.       
* **Decil**: valores $D_{\alpha}$ que dividen los datos en 10 partes iguales.

* **Cuartil**: valores $C_{\alpha}$ o $q_{\alpha}$ que dividen los datos en 4 partes iguales.

---

## Repaso: covarianza y correlaci√≥n

[**¬øQu√© es en realidad la varianza?**]{.hl-yellow}

. . .

La [**varianza es el promedio de las desviaciones al cuadrado**]{.hl-yellow} (respecto a la media), apareciendo dos veces dicha desviaci√≥n: puede ser entendida como una [**medida que cuantifica la relaci√≥n de una variable CONSIGO MISMA**]{.hl-yellow}

. . .

[**¬øY si qui√©semos medir la relaci√≥n de una variable X respecto a otra variable Y (en lugar de consigo misma)?**]{.hl-purple}


---

## Repaso: covarianza y correlaci√≥n

$$s_{x}^{2} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)^2 = \overline{x^2} - \overline{x}^2 \quad \text{(varianza)}$$


La idea detr√°s de la [**covarianza**]{.hl-yellow} es justo esa: sustituir una de esas desviaciones de la X por la desviaci√≥n de la Y.


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

---

## Repaso: covarianza y correlaci√≥n

Es importante entender algunas [**propiedades de la covarianza**]{.hl-yellow}

. . .

* [**Signo**]{.hl-purple}: la covarianza puede ser tanto positiva como negativa como 0: al eliminar el cuadrado de la varianza, ya no es necesario que sea positiva


. . .

* [**¬øQu√© cuantifica?**]{.hl-purple} La covarianza mide la [**relaci√≥n LINEAL**]{.hl-red} (en torno a una recta) entre dos variables

. . .

* [**¬øQu√© dice su signo?**]{.hl-purple} El signo de la covarianza nos indicar√° la [**direcci√≥n de la dependencia lineal**]{.hl-yellow}: si es positiva, la relaci√≥n ser√° creciente (cuando X crece, Y crece); si es negativa, la relaci√≥n ser√° decreciente (cuando X crece, Y decrece)


---

## Repaso: covarianza y correlaci√≥n

Al igual que pasaba con la varianza, la [**covarianza depende de las unidades y magnitudes**]{.hl-yellow} de los datos, as√≠ que lo que haremos ser√° [**estandarizar la covarianza**]{.hl-yellow}. Definiremos la [**coeficiente correlaci√≥n lineal (de Pearson)**]{.hl-yellow} como la covarianza dividida entre el producto de las desviaciones t√≠picas (adimensional)


$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

. . .

Tiene el [**mismo signo que la covarianza**]{.hl-yellow} (el denominador es siempre positivo) y sus [**valores siempre est√°n entre -1 y 1**]{.hl-yellow}

* m√°s cerca de -1 o 1 ‚Üí relaci√≥n lineal m√°s fuerte
* m√°s cerca de 0 ‚Üí ausencia de relaci√≥n **LINEAL**

---

## Repaso: covarianza y correlaci√≥n

![](img/correlaciones.jpg)


# Clase 2: importancia del dataviz {#clase-2}

[**¬øBasta con calcular la correlaci√≥n para cuantificar si un ajuste es adecuado?**]{style="color:#444442;"}


---

## Conjunto anscombe

En el paquete `{datasets}` se encuentra el dataset conocido como [**cuarteto de Anscombe**]{.hl-yellow}, un dataset que cuenta con 4 conjuntos de datos.

```{r}
anscombe_tb <- as_tibble(datasets::anscombe)
anscombe_tb
```

. . .

[**Primera tarea: convierte a tidydata**]{.hl-yellow}

---

## Conjunto anscombe

```{r}
anscombe_x <- 
  anscombe_tb |>
  pivot_longer(cols = x1:x4, names_to = "dataset", values_to = "x") |> 
  select(-contains("y")) |> 
  mutate(dataset = str_remove_all(dataset, "x"))

anscombe_y <- 
  anscombe_tb |>
  pivot_longer(cols = y1:y4, names_to = "dataset", values_to = "y") |> 
  select(-contains("x"), -dataset)

anscombe_tidy <-
  anscombe_x |> mutate("y" = anscombe_y$y)
anscombe_tidy
```

---

## Conjunto anscombe

[**¬øQu√© caracter√≠sticas muestrales tienen?**]{.hl-yellow} Calcula la media, varianza, desv. t√≠pica, covarianza y correlaci√≥n en cada dataset

. . .

```{r}
anscombe_tidy |> 
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

. . .

Si te fijas [**todos los datasets tienen los mismos momentos**]{.hl-yellow} muestrales, incluso tendr√≠an el mismo ajuste de regresi√≥n...¬øser√°n el mismo dataset desordenado?

---

## Conjunto anscombe

[**Visualiza los 4 datasets**]{.hl-yellow}

. . .

```{r}
#| code-fold: true
ggplot(anscombe_tidy) +
  geom_point(aes(x = x, y = y, color = dataset), size = 3, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~dataset) + theme_minimal()
```

---

## Conjunto anscombe

```{r}
#| echo: false
ggplot(anscombe_tidy) +
  geom_point(aes(x = x, y = y, color = dataset), size = 3, alpha = 0.7) +
  ggthemes::scale_color_colorblind() +
  facet_wrap(~dataset) + theme_minimal()
```


Por suerte o por desgracia [**no todo son matem√°ticas**]{.hl-red}: antes de pensar que modelo es mejor para nuestros datos, es important√≠simo realizar un [**an√°lisis exploratorio**]{.hl-yellow} de los datos (incluyendo visualizaci√≥n)

---

## Datasaurus

Podemos visualizarlo de manera a√∫n m√°s extrema con el dataset `datasaurus_dozen` del paquete `{datasauRus}` (ver m√°s en <https://www.research.autodesk.com/publications/same-stats-different-graphs/>)

```{r}
library(datasauRus)
datasaurus_dozen |>
  summarise(mean_x = mean(x), mean_y = mean(y),
            var_x = var(x), var_y = var(y),
            sd_x = sd(x), sd_y = sd(y),
            cov = cov(x, y), cor = cor(x, y), .by = dataset)
```

---

## Datasaurus

```{r}
#| code-fold: true
ggplot(datasaurus_dozen |> filter(dataset != "wide_lines"),
       aes(x = x, y = y, color = dataset)) +
  geom_point(size = 2.5, alpha = 0.75) +
  geom_smooth(method = "lm", se = FALSE)  +
  facet_wrap(~dataset, ncol = 3) +
  theme_minimal()
```

---

# Clase 3: profundizando en la correlaci√≥n {#clase-3}

[**Matrices de correlaci√≥n y covarianza. Correlaci√≥n vs causalidad vs dependencia**]{style="color:#444442;"}


---

## Correlaci√≥n lineal: sin agrupar


Como dec√≠amos, la idea detr√°s de la [**covarianza**]{.hl-yellow} es una "varianza" entre dos variales (la varianza es una covarianza de una variable consigo misma), midiendo el **promedio de lo que se desv√≠a cada una respecto a su media**

$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

. . .

Para calcularla en `R` basta con usar la funci√≥n `cov()`

```{r}
starwars |> 
  drop_na(mass, height) |> 
  summarise(cov(mass, height))
```

---

## Correlaci√≥n lineal: sin agrupar

Vamos a practicar una vez m√°s como [**hacerlo a mano con el siguiente ejercicio**]{.hl-yellow}.

---

## Cor lineal: sin agrupar

En la tabla inferior se han recopilado (del 2013 al 2022) la **temperatura media en el mes de abril en Madrid (variable X, en ¬∫C)** y el **n√∫mero de d√≠as (variable Y) en el que el nivel de ozono super√≥ las 0.20 ppm (partes por mill√≥n)**

* ¬øCu√°l fue media de d√≠as en los que se super√≥ umbral de ozono de 0.20 ppm?
* ¬øCu√°l fue media de d√≠as en los que se super√≥ umbral de ozono en los a√±os que la temperatura media en marzo super√≥ los 17.4¬∫C?
* ¬øCu√°l es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)

---


## Correlaci√≥n lineal: sin agrupar

Repite el ejercicio con pocas l√≠neas de c√≥digo `R`

* ¬øCu√°l fue la media de d√≠as en los que se super√≥ el umbral de ozono de 0.20 ppm?
* ¬øCu√°l fue la media de d√≠as en los que se super√≥ el umbral de ozono en los a√±os que la temperatura media en marzo super√≥ los 17.4¬∫C?
* ¬øCu√°l es su covarianza?

![](img/tabla-ej-covarianza-sin-agrupar.png)


---


## Correlaci√≥n lineal: sin agrupar

Realiza lo que consideres tanto a mano como en `R`

* ¬øExiste alguna **relaci√≥n de dependencia entre las variables**? ¬øDe qu√© tipo? ¬øC√≥mo de fuerte o d√©bil es dicha relaci√≥n? ¬øEn qu√© direcci√≥n es dicha relaci√≥n?


$$s_{xy} = \frac{1}{n} \sum_{i=1}^{n} \left(x_i - \overline{x} \right)\left(y_i - \overline{y} \right) = \overline{x*y} - \overline{x}*\overline{y}$$

$$r_{xy} = \rho_{xy} = \frac{s_{xy}}{s_x s_y}$$

---

## Correlaci√≥n lineal: sin agrupar

No s√© si te has fijado qu√© sucede cuando intentamos [**calcular la covarianza/correlaci√≥n de varias variables**]{.hl-yellow}, por ejemplo vamos a calcular la (cuasi)covarianza de todas las variables num√©ricas de starwars.

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

. . .

Podemos usar la funci√≥n `cov()` sin m√°s, fuera de un resumen, obteniendo lo que se conoce como [**matriz de (cuasi)covarianzas**]{.hl-yellow} y que tendr√° un papel fundamental en estad√≠stica ya que contiene la informaci√≥n (= varianza) del dataset.

---

## Matriz de covarianzas


```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cov()
```

Adem√°s de ser [**sim√©trica**]{.hl-yellow}...¬øqu√© tenemos en la [**diagonal**]{.hl-yellow}?

. . .

La [**matriz de (cuasi)covarianzas**]{.hl-yellow} se denota como $\Sigma$ y sus elementos se define como $\Sigma_{ii} = s_{x_i}^{2}$ para la diagonal y $\Sigma_{ij} = \Sigma_{ji} = s_{x_i x_j}$ fuera de ella.

. . .

::: callout-important
## Importante

Recuerda que los softwares estad√≠sticos nos devuelven siempre la [**cuasi covarianza**]{.hl-purple}, dividido entre $n-1$ y no entre $n$. La cuasivarianza y la cuasicovarianza son los [**mejores estimadores muestrales (insesgados)**]{.hl-yellow} de los respectivos par√°metros poblaciones

:::

---

## Matriz de correlaciones

De la misma manera con `cor()` podemos [**calcular la matriz de correlaciones**]{.hl-yellow} (en este caso sin el `cuasi` ya que se cancelan denominadores)

. . .

```{r}
starwars |> 
  select(where(is.numeric)) |> 
  drop_na() |>
  cor()
```

. . .

La [**matriz de correlaciones**]{.hl-yellow} se denota como $R$ y sus elementos se define como $r_{ii} = 1$ para la diagonal y $r_{ij} = r_{x_ix_j}$ fuera de ella, y nos proporciona la dependencia lineal entre variables ya de manera **estandarizada**.

---

## Matriz de correlaciones

[**¬øSe te ocurre alguna manera de calcular la matriz de correlaciones a partir de la de covarianzas?**]{.hl-yellow}

---

## Correlaci√≥n lineal: datos agrupados

Imagina ahora que queremos calcular la covarianza/correlaci√≥n de los siguientes datos que representan 24 tiradas de dados

![](img/dados.png)

¬øC√≥mo calcular la covarianza/correlaci√≥n agrupando los datos?

---

## Correlaci√≥n lineal: datos agrupados

![](img/tabla-bi-freq.png)
---

## Correlaci√≥n lineal: datos agrupados

---

## Correlaci√≥n vs dependencia


Podemos tener [**variables incorreladas**]{.hl-red}, con correlaci√≥n nula, pero que [**exista dependencia entre ellas**]{.hl-green}: la covarianza/correlaci√≥n [**SOLO CAPTURA relaciones lineales**]{.hl-yellow}, nada m√°s.

. . .

Veamos un ejemplo sencillo con $X = \left\lbrace -1, 0, 1 \right\rbrace$ y $Y = X^2 =  \left\lbrace 1, 0, 1 \right\rbrace$. 

* La media de ambas es nula
* La media del producto es la media de $XY = \left\lbrace -1, 0, 1 \right\rbrace$, que es de nuevo nula
* As√≠ la covarianza $\overline{x*y} - \overline{x}*\overline{y}$ es nula a pesar de tener la mayor dependencia posible (dependencia funcional)

---

## Correlaci√≥n vs dependencia

![](img/covarianza-no-lineal.png)

En relaciones no lineales como la de la imagen, la **correlaci√≥n estar√° cercana a cero** (ya que no hay relaci√≥n lineal) pero existe una [**dependencia**]{.hl-yellow}. Diremos que [**dos variables son dependientes entre s√≠**]{.hl-yellow} cuando existe un **patr√≥n num√©rico que las relaciona**

. . .

* [**Independencia implica incorrelaci√≥n**]{.hl-green}
* [**Incorrelaci√≥n NO implica independencia**]{.hl-red}

---

## Correlaci√≥n vs dependencia

![](img/escenarios-covarianza.png)

---

## Dependencia vs causalidad

![](img/helados-moreno.jpg)

Si $X$ e $Y$ fuesen variables dependientes (lineales o no)...[**¬øimplicar√≠a que el ascenso/descenso de una provoca el ascenso/descenso de la otra?**]{.hl-yellow}

. . .

Imagina por ejemplo dos variables: nivel de bronceado (X) y consumo de helados (Y). ¬øSon dependientes? Aparentemente s√≠ ya que su comportamiento es similar. **¬øUna causa la otra?**

---

## Dependencia vs causalidad

![](img/helados-moreno.jpg)

Diremos que dos variables tienen una [**relaci√≥n causal o de causalidad**]{.hl-yellow} cuando una variable es la CAUSA del comportamiento de la otra, algo que no sabremos solo con estad√≠stica (sino con conocimiento experto, en este caso de nutricionistas y m√©dicos)

. . .

[**Correlaci√≥n NO IMPLICA causalidad**]{.hl-red} (al menos no tiene por qu√©)

---

## Dependencia vs causalidad


![](img/chocolate-correlacion.jpg)


Este fen√≥meno es conocido como [**correlaciones esp√∫reas**]{.hl-yellow}, y aparece cuando dos variables presentan una [**relaci√≥n matem√°tica**]{.hl-green} pero sin [**ning√∫n tipo de relaci√≥n causal o l√≥gica**]{.hl-red}. Puedes ver m√°s en <https://www.tylervigen.com/spurious-correlations>

---

## Dependencia vs causalidad

![](img/confounder.jpg)

Dicho patr√≥n matem√°tico puede deberse al mero azar o la existencia de lo que se conoce como [**variables de confusi√≥n**]{.hl-yellow} (en el caso del helado, el verano es el que causa ambas).

La rama que se dedica al [**estudio de la causalidad**]{.hl-yellow}, combinando las ramas de la **estad√≠stica, filosof√≠a, sociolog√≠a y psicolog√≠a** se le conoce como [**inferencia causal**]{.hl-yellow} y es fundamental en un an√°lisis m√°s profunde de las relaciones entre las variables (sobre todo en campos como la econom√≠a o la sociolog√≠a)


# Clase 4: primeras regresiones {#clase-4}

[**Historia regresi√≥n. Aprendizaje supervisado. Regresi√≥n lineal univariante**]{style="color:#444442;"}

---

## Historia de la reg

> Hay una regla universal: cualquier pariente tuyo es probablemente m√°s mediocre que t√∫¬ª

La [**historia de regresi√≥n**]{.hl-yellow} se remonta a **Francis Galton**, primo de Charles Darwin, que adem√°s de estad√≠stico fue psic√≥logo, ge√≥grafo y, por desgracia, el primer eugen√©sico (de hecho acu√±√≥ el termino)

. . .

Tambi√©n fue el primero en proponer m√©todos de clasificaci√≥n de huellas en medicina forense e incluso se le atribuye el primer mapa meteorol√≥gico de la historia

---

## Regresi√≥n y Darwin

Galton mostr√≥ fascinaci√≥n por ¬´El origen de la especies¬ª de su primo. Sin embargo, [**Galton no se centraba en los mejor adaptados sino en los que √©l llama mediocres**]{.hl-yellow}

. . .

Seg√∫n Galton, las sociedades estaban fomentando la mediocridad, interfiriendo en la selecci√≥n natural, as√≠ que empez√≥ a **estudiar si el talento era o no hereditario**.


. . .

¬øSu conclusi√≥n? El [**talento se disipaba**]{.hl-yellow} a lo largo de las generaciones

---

## Regresi√≥n a la mediocridad

En 1886 public√≥ ¬´Regression towards mediocrity in hereditary stature¬ª, un art√≠culo que cambiar√≠a la estad√≠stica: fue el [**primer uso documentado de lo que hoy conocemos como recta de regresi√≥n**]{.hl-yellow} 

. . .

:::: columns
::: {.column width="38%"}

![](img/reg_dalton.jpg)

:::

::: {.column width="62%"}

Galton analiz√≥ la estatura de 205 hijos y padres, observando que, de nuevo, los valores extremos se disipaban: a lo largo de las generaciones hab√≠a una [**regresi√≥n (un retroceso) a la mediocridad (entendida como la media)**]{.hl-yellow} 


:::
::::

Hijos de altos eran algo m√°s bajitos, e hijos de bajitos eran algo m√°s altos.

---

## Regresi√≥n a la mediocridad

:::: columns
::: {.column width="60%"}

![](img/reg-galton-coef.jpg)

:::

::: {.column width="40%"}

Galton no solo observ√≥ que las [**estaturas ¬´regresaban¬ª a un valor medio sino que lo hac√≠an con un patr√≥n**]{.hl-yellow}, con un factor constante de $2/3$: si los padres se desviaban $+3$ por encima de la media, los hijos se desviaban solo $(2/3)*3 = +2$ por encima de la media.

:::
::::

---

## Aprendizaje estad√≠stico

La regresi√≥n lineal es el modelo m√°s simple de lo que se conoce como [**aprendizaje estad√≠stico (Machine Learning)**]{.hl-yellow}, en concreto del conocido como [**aprendizaje supervisado**]{.hl-yellow}

![](img/ml.jpg)


---

## Ciencia de Datos


La [**ciencia de datos**]{.hl-yellow} es precisamente la rama que integra las matem√°ticas, la estad√≠stica, la probabilidad, el Machine Learning e incluso el Big Data

![](img/stats-ml.jpg)

---

## Aprendizaje ¬øsupervisado?

![](img/unsupervised-learning.jpg)

---

## Aprendizaje ¬øsupervisado?

En el campo del Machine Learning hay principalmente dos tipos de metodolog√≠as:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje supervisado**]{.hl-purple}: tendremos dos tipos de variables, la [**variable dependiente (output/target)**]{.hl-yellow} que se quiere predecir/clasificar, normalmente denotada como $Y$, y las [**variables independientes (inputs) o explicativas o predictoras**]{.hl-yellow}, que contienen la informaci√≥n disponible. Ejemplos: regresi√≥n, knn, √°rboles, etc.
 

:::

::: {.column width="50%"}

![](img/supervised.jpg)
:::
::::

---

## Aprendizaje ¬øsupervisado?

En el campo del Machine Learning hay principalmente dos tipos de metodolog√≠as:

:::: columns
::: {.column width="50%"}

* [**Aprendizaje no supervisado**]{.hl-purple}: no existe la distinci√≥n entre target y variables explicativas ya que [**no tenemos etiquetados los datos**]{.hl-yellow}, no sabemos a priori la respuesta correcta. El aprendizaje no supervisado [**buscar√° patrones**]{.hl-yellow} basados en similitudes/diferencias. Ejemplos: PCA, clustering, redes neuronales, etc.

:::


::: {.column width="50%"}

![](img/unsupervised.jpg)

:::
::::

---

## Clasificaci√≥n vs predicci√≥n

Como hemos comentado, la [**regresi√≥n lineal**]{.hl-yellow} se enmarca dentro del [**predicci√≥n supervisada**]{.hl-yellow}

* [**Predicci√≥n**]{.hl-purple}: la [**variable objetivo es una variable cuantitativa continua**]{.hl-yellow} (por ejemplo, precio, glucosa, peso, etc).

* [**Clasificaci√≥n**]{.hl-purple}: la [**variable objetivo es una variable cualitativa**]{.hl-yellow} (por ejemplo, especie de flor, ausencia/presencia de enfermedad, si/no, etc) o **cuantitativa discreta** (por ejemplo, n√∫mero de accidentes). La etiqueta tomar√° un valor dentro del conjunto de modalidades permitidas, pudiendo ser **binaria** (si/no) o **multiclase** (A, B, C, D).

&nbsp;

üìö Ver ¬´The elements of Statistical Learning¬ª (Hastie et al., 2008)

# Clase 5: ajuste de regresi√≥n {#clase-5}

[**Interpretaci√≥n de coeficientes. M√©todo m√≠nimos cuadrados**]{style="color:#444442;"}

---

## Modelo predictivo

Dentro del marco de la [**predicci√≥n supervisada**]{.hl-yellow} un modelo tendr√° siempre la siguiente forma:

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad {\rm E} \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$

* $X$ ser√°n los [**datos**]{.hl-yellow}

* $f(\cdot)$ ser√° nuestro [**modelo**]{.hl-yellow}, es decir, el [**valor esperado de $Y$**]{.hl-yellow} con la informaci√≥n que tenemos.

* $\mathbf{X} = \left(X_1, \ldots, X_p \right)$ ser√°n nuestras [**predictoras o variables independientes**]{.hl-yellow}

* $\varepsilon$ ser√° el [**error o ruido**]{.hl-yellow}, una [**variable aleatoria de media 0**]{.hl-yellow} ${\rm E} \left[\varepsilon | \boldsymbol{X} = x \right] = 0$ (el error deber√≠a ser reducido a **algo aleatorio (irreducible)**, aunque en estad√≠stica SIEMPRE nos vamos a equivocar).

---

## Modelo predictivo

$$Y = f(\mathbf{X}) + \varepsilon = f\left(X_1, \ldots, X_p \right) + \varepsilon, \quad {\rm E} \left[Y | \boldsymbol{X} = x \right] =f\left(X_1, \ldots, X_p \right) $$


El objetivo es intentar [**estimar dicha variable objetivo**]{.hl-yellow} minimizando al m√°ximo el error cometido (la parte que podemos evitar).

. . .

El [**modelo estimado**]{.hl-yellow} se definir√° como

$$\widehat{Y} := \widehat{{\rm E} \left[Y | \boldsymbol{X} = x \right]}  = \widehat{f}\left(X_1, \ldots, X_p \right), \quad \widehat{\varepsilon} = \widehat{Y} - Y$$

* $\widehat{Y}$ ser√°n las [**estimaciones**]{.hl-yellow}, definidas como la estimaci√≥n del [**valor esperado de $Y$**]{.hl-yellow} con la informaci√≥n que tenemos.

* $\widehat{f}$ ser√° el [**modelo estimado**]{.hl-yellow}

* $\widehat{\varepsilon}$ los [**errores estimados**]{.hl-yellow} (con media muestral igual a cero).


---

## Reg. lineal univariante

En el caso concreto de la [**regresi√≥n lineal**]{.hl-yellow} nuestro modelo ser√° un **hiperplano lineal** (en el caso de una variable, una simple recta):

$${\rm E} \left[Y | \boldsymbol{X} = x \right]  = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p$$

. . .

Su estimaci√≥n ser√° por tanto

$$\widehat{Y} := \widehat{{\rm E} \left[Y | \boldsymbol{X} = x \right]} = \widehat{f}(\mathbf{X}) = \widehat{\beta_0} + \widehat{\beta_1} X_1 + \ldots + \widehat{\beta_p} X_p $$

* El modelo $\widehat{f}(\cdot)$ ser√° una [**recta/hiperplano lineal**]{.hl-yellow}

* En el caso de la regresi√≥n lineal [**univariante**]{.hl-yellow} tendremos por tanto ${\rm E} \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1X$ ($p = 1$)

* El objetivo ser√° obtener la estimaci√≥n de los $\widehat{\beta}$ tal que **minimicemos el error**

---

## Varianza residual

Como ya hemos comentado, una manera de cuantificar lo que nuestro modelo se equivoca es lo que llamamos [**varianza residual o error cuadr√°tico medio**]{.hl-yellow} (varianza del error), que podemos estimar como sigue (ya que la [**media muestral de los errores ser√° cero**]{.hl-purple})

$$s_{r}^{2} = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \left( \widehat{\varepsilon}_{i} - \overline{\widehat{\varepsilon}}_{i}\right)^2 = \frac{1}{n} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^{2} $$
¬øC√≥mo quedar√≠a la f√≥rmula si **desarrollo lo que vale los errores estimados** $\widehat{\varepsilon}$?

. . .


$$s_{r}^{2} := s_{r}^{2}\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)= \frac{1}{n} \sum_{i=1}^{n} \left(Y_i - \widehat{Y}_i \right)^2 = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\widehat{\beta}_0 + \widehat{\beta}_1X_i \right)\right]^2$$

---

## M√©todo m√≠nimos cuadrados

El [**m√©todo de los m√≠nimos cuadrados**]{.hl-yellow}, originariamente planteado a la vez por Legendre y Gauss, consiste en establecer que los [**par√°metros √≥ptimos**]{.hl-yellow} ser√°n aquellos $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$ que minimicen dicha suma de cuadrados (que minimicen la varianza residual)

$$\widehat{\boldsymbol{\beta}} = \left(\widehat{\beta}_0, \widehat{\beta}_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} s_{r}^{2}\left(\beta_0, \beta_1 \right) = \arg \min_{\left(\beta_0, \beta_1 \right) \in \mathbb{R}^2} n*s_{r}^{2}\left(\beta_0, \beta_1 \right)$$

Vamos a desarrollar ese cuadrado...

--- 


## M√©todo m√≠nimos cuadrados

$$s_{r}^{2}\left(\beta_0, \beta_1 \right) = \frac{1}{n} \sum_{i=1}^{n} \left[Y_i - \left(\beta_0 + \beta_1 X_i \right)\right]^2$$ 
Vamos a desarrollar ese cuadrado...

. . .

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$




---

## M√©todo m√≠nimos cuadrados

$$\begin{eqnarray} s_{r}^{2}\left(\beta_0, \beta_1 \right) &=& \frac{1}{n} \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_0 + \beta_1 X_i \right)^2 - 2Y_{i}\left(\beta_0 + \beta_1 X_i \right)\right] \nonumber \\  &=& \sum_{i=1}^{n} \left[Y_{i}^{2} + \left(\beta_{0}^{2} + \beta_{1}^{2}X_{i}^{2} + 2\beta_{0}\beta_{1} X_i \right) - 2Y_{i}\beta_{0} - 2Y_{i}\beta_{1} X_i  \right] \end{eqnarray}$$

¬øC√≥mo encontrar el m√≠nimo de una funci√≥n?

. . .

Efectivamente, derivando.

$$\frac{\partial s_{r}^{2}}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{0} + 2 \beta_1 X_i - 2Y_i \right] = 2 \left( \beta_{0} + \beta_{1} \overline{x} - \overline{y} \right)$$ 

$$\frac{\partial s_{r}^{2}}{\partial \beta_1} = \frac{1}{n} \sum_{i=1}^{n} \left[ 2\beta_{1}X_{i}^{2} + 2 \beta_0 X_i - 2Y_iX_i \right] = 2 \left( \beta_{1} \overline{x^2}+\beta_{0}\overline{x} - \overline{xy} \right)$$ 

---

## M√©todo m√≠nimos cuadrados


Si lo agrupamos en un sistema y lo [**igualamos a cero**]{.hl-yellow} para encontrar el √≥ptimo tenemos

$$\left\{\beta_{0} + \beta_{1} \overline{x}   = \overline{y} \atop \overline{x} \beta_{0} + \overline{x^2} \beta_{1} = \overline{xy} \right.$$ 

¬øC√≥mo resolver un sistema?

. . .

Haciendo la regla de Cramer tenemos que

$$\widehat{\beta}_1 = \frac{\overline{xy} - \overline{x}*\overline{y}}{\overline{x^2} - \overline{x}^2} = \frac{s_{xy}}{s_{x}^{2}}$$

$$\widehat{\beta}_0 = \frac{\overline{y}*\overline{x^2} - \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2}  = \overline{y} + \frac{\overline{y}*\overline{x}^2- \overline{x}*\overline{xy}}{\overline{x^2} - \overline{x}^2} = \overline{y} - \widehat{\beta}_1 \overline{x}$$


---

## Estimaci√≥n reg. univariante


Los [**estimadores de los par√°metros de una regresi√≥n lineal univariante**]{.hl-yellow} ser√°n por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

. . .

Es importante [**distinguir lo poblaci√≥n de lo muestral**]{.hl-yellow}

* Los par√°metros $\left( \beta_0, \beta_1 \right)$ son desconocidos, los [**par√°metros poblacionales**]{.hl-yellow}

* Los par√°metros $\left( \widehat{\beta}_0, \widehat{\beta}_1 \right)$ son estimados a partir de los datos, son [**variables aleatorias**]{.hl-yellow} ya que han sido calculados en funci√≥n de una muestra aleatoria $\left\lbrace \left(x_i, y_i \right) \right\rbrace_{i=1}^{n}$ de la poblaci√≥n $\left(X, Y \right) $

---

## Estimaci√≥n reg. univariante

Los [**estimadores de los par√°metros de una regresi√≥n lineal univariante**]{.hl-yellow} ser√°n por tanto $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$ ([**pendiente de la recta**]{.hl-purple}) y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$ ([**ordenada en el origen**]{.hl-purple})

¬øCu√°l es su [**interpretaci√≥n**]{.hl-yellow}?

. . .

* [**Ordenada en el origen**]{.hl-yellow}: tambi√©n llamado **intercepto**, y denotado como $\beta_0$ su valor real, es el valor de $Y$ cuando $X=0$. Es decir, $\widehat{\beta}_0$ se puede interpretar como la estimaci√≥n $\widehat{Y}$ cuando $X = 0$ (cuando dicha estimaci√≥n tenga sentido).


. . .

* [**Pendiente**]{.hl-yellow}: denotado com $\beta_1$ su valor real, cuantifica el incremento de $Y$ cuando $X$ aumenta una unidad. Es decir, $\widehat{\beta}_1$ se puede interpretar como la variaci√≥n de la **estimaci√≥n $\widehat{Y}$ cuando $X$ tiene un incremento unitario**.

---

## Regresi√≥n en R

Para hacer un [**ajuste de regresi√≥n lineal univarainte en R**]{.hl-yellow} solo necesitamos utilizar la funci√≥n `lm()` (de linear model) con dos argumentos:

* `data = ...`: los datos de los que queremos sacar el ajuste (sin ausentes de momento)
* `formula = ...`: la f√≥rmula del ajuste (y frente a x, expresado como `y ~ x`)

```{r}
datos <- 
  starwars |>
  drop_na(mass, height) 
ajuste_lineal <- lm(data = datos, formula = mass ~ height)
```

---

## Regresi√≥n en R

Una vez realizado el ajuste podemos [**resumirlo**]{.hl-yellow} con `summary()`

```{r}
ajuste_lineal |> summary()
```

. . .

¬øQu√© representa cada bloque de la salida?

---

## Regresi√≥n en R

```{r}
ajuste_lineal |> summary()
```

* `Call: ...`: la orden que hemos ejecutado

* `Residuals: ...`: un resumen en forma de [**cuartiles de los residuales**]{.hl-yellow}, es decir, de los [**errores**]{.hl-yellow} (f√≠jate que aunque la media de los errores siempre es 0, no tiene porque serlo la mediana, de hecho que no lo sea ya nos indica que normales no van a ser seguramente).


---

## Regresi√≥n en R

```{r}
ajuste_lineal |> summary()
```


* `Coefficients: ...`: de momento solo nos interesa la columna [**Estimate**]{.hl-yellow} que nos da las estimaciones de los par√°metros. En la fila `Intercept` siempre ir√° $\widehat{\beta}_0$, y el resto de filas tendr√° el nombre de la variable predictora a la que multiplica el par√°metro (en este caso la fila `height` corresponde a la estimaci√≥n $\widehat{\beta}_1$).

---


## Regresi√≥n en R

```{r}
ajuste_lineal |> summary()
```

En nuestro caso particular, el [**ajuste de regresi√≥n**]{.hl-yellow} viene dado por $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1 X = -11.4868 + 0.6240*X$, donde $X$ es la estatura: por cada cm extra que mida un personaje, el modelo estima que el peso aumenta 0.6240 kg, y la estimaci√≥n de peso para un persona que mida 0 cm es de -11.4868 kg (obviamente esta estimaci√≥n mucho sentido no tiene)

---

## Regresi√≥n en R

```{r}
ajuste_lineal |> summary()
```

* `Multiple R-squared: ...` (de momento lo dem√°s lo ignoramos): es el conocido como $R^2$ o bondad de ajuste. Hablaremos de √©l en las pr√≥ximas clases, pero de momento, nos basta saber que es una [**m√©trica de calidad del ajuste**]{.hl-yellow} que va de 0 (peor) a 1 (mejor).

---

## Predicci√≥n en R

Una vez estimado el modelo (sus coeficientes) podemos usarlo, bien para  [**estimar valores que el modelo conoce**]{.hl-yellow} (de los que aprendi√≥) o para [**predecir nuevos valores**]{.hl-yellow} de los que el modelo no ha aprendido

. . .

Simplemente necesitamos la funci√≥n `predict(modelo, nuevos_datos)`, usando como argumentos el **modelo entrenado** y un [**nuevo dataset que contenga una/s columna/s llamadas igual que nuestras variables predictores**]{.hl-yellow} con los valores de la predictora para los que queremos predecir

```{r}
predicciones <- 
  tibble("height" = c(15, 81.3, 153.1, 189.3, 210.3),
         "mass_predict" = predict(ajuste_lineal, tibble("height" = height)))
predicciones
```

---

## Rango de fiabilidad

Aunque hablaremos en profundidad de la bondad de ajuste, supongamos que tenemos un **modelo extremadamente bueno**, con una bondad de ajuste $R^2 = 0.99$. ¬øSer√≠a fiable nuestro modelo anterior para predecir el peso de un personaje con $X = 15$ cm?

. . .

```{r}
predicciones
```

Si te fijas la [**¬°predicci√≥n del peso es negativa!**]{.hl-red}: por muy beb√© que sea, algo pesar√°. ¬øPor qu√© sucede esto?

---

## Rango de fiabilidad


![](img/rango_1.png)
![](img/rango_2.png)

Nuestro [**modelo solo puede aprender de los datos que conoce**]{.hl-yellow}: solo podremos predecir para valores nuevos de las predictoras [**dentro del rango de las predictoras con las que entrenamos**]{.hl-yellow} el modelo

Da igual lo bueno que sea el modelo: las [**predicciones fuera del rango de las predictoras de entrenamiento**]{.hl-red} no ser√°n fiables ya que el modelo no sabe lo que sucede fuera.

# Clase 6: diagnosis  {#clase-6}

[**Hip√≥tesis e inferencia del modelo**]{style="color:#444442;"}

---

## Repaso


* [**Modelo lineal**]{.hl-yellow}: descrito como $Y = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p X_p + \varepsilon$, buscamos estimar el **valor esperado**  ${\rm E} \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 X_1 + \ldots + \beta_p x_p$, donde $\boldsymbol{X}$ es la poblaci√≥n y $x$ la muestra.

. . .

* [**Caso univariante**]{.hl-yellow}: $p = 1$ tal que ${\rm E} \left[Y | \boldsymbol{X} = x \right] = \beta_0 +  \beta_1 x$.

. . .

* [**Estimaci√≥n**]{.hl-yellow}: $\widehat{Y}$ en funci√≥n de $\left(\widehat{\beta}_0, \widehat{\beta}_1 \right)$. De todas las rectas posibles, queremos la [**recta de regresi√≥n m√≠nimos-cuadrados**]{.hl-yellow}: **minimiza la varianza residual** (calculamos sus derivadas parciales, igualamos a cero y resolvemos el sistema). Esos estimadores ser√°n $\widehat{\beta}_1 = \frac{s_{xy}}{s_{x}^{2}}$  y $\widehat{\beta}_0 = \overline{y} - \widehat{\beta}_1 \overline{x}$.

. . .

* [**Predicci√≥n**]{.hl-yellow}: podemos estimar o predecir valores con la recta $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_0 X$, siempre y cuando $X = x$ est√© dentro del rango de valores de entrenamiento del modelo.

---

## Diagnosis

Hasta ahora no hemos pedido nada a la muestra $\left\lbrace \left( X_i, Y_i \right) \right\rbrace_{i=1}^{n}$, ¬øpara qu√© necesitar√≠amos [**hip√≥tesis**]{.hl-yellow} entonces?

. . .

La raz√≥n es que, hasta ahora, lo √∫nico que hemos podido realizar es una [**estimaci√≥n puntual**]{.hl-yellow} de los par√°metros, pero dado que dichos estimadores ser√°n variables aleatorias, necesitaremos realizar [**inferencia estad√≠stica**]{.hl-yellow} sobre ellos (recuerda: los par√°metros son simpleme estimaciones para esa muestra de la poblaci√≥n, de forma que dada otra muestra, la recta ser√° distinta).

. . .

Para poder cuantificar la [**variabilidad y precisi√≥n de nuestras estimaciones**]{.hl-yellow} necesitaremos que los datos cumplan ciertas [**hip√≥tesis probabil√≠sticas**]{.hl-purple}: lo interesante no es la estimaci√≥n puntual de los par√°metros a partir de la muestra sino lo que [**podamos inferir de ellos a la poblaci√≥n**]{.hl-green}

---

## Diagnosis

En el caso de la regresi√≥n lineal univariante pediremos [**4 hip√≥tesis**]{.hl-yellow}

. . .

1. [**Linealidad**]{.hl-green}: el valor esperado de $Y$ es ${\rm E} \left[Y | \boldsymbol{X} = x \right] = \beta_0 + \beta_1 x$

. . .

2. [**Homocedasticidad**]{.hl-green}: el modelo no podr√° explicar toda la informaci√≥n (a veces se equivocar√° por arriba y otras por abajo), pero necesitamos que la [**varianza del error sea finita y constante**]{.hl-yellow}, tal que $\sigma_{r}^2 = \sigma_{\varepsilon}^2 = {\rm Var} \left[\varepsilon | \boldsymbol{X} = x \right] = cte < \infty$, que no var√≠e seg√∫n aumenta o disminuye la $X$.

. . .

3. [**Normalidad**]{.hl-green}: pediremos que $\varepsilon \sim N \left(0, \sigma_{r}^2 \right)$

. . .

4. [**Independencia**]{.hl-green}: los errores $\left\lbrace \varepsilon_i \right\rbrace_{i=1}^{n}$ deben ser independientes entre s√≠ (el error en una observaci√≥n no depende de otras). En particular, ser√°n **incorrelados**

$${\rm Cor}_{\varepsilon_i \varepsilon_j} = {\rm E} \left[\varepsilon_i \varepsilon_j \right] - {\rm E} \left[\varepsilon_i \right] {\rm E} \left[\varepsilon_j \right] = {\rm E} \left[\varepsilon_i \varepsilon_j \right] = 0, \quad i \neq j$$

---


## Diagnosis


Las 4 hip√≥tesis se pueden [**resumir de manera te√≥rica**]{.hl-yellow} en

$$Y | \boldsymbol{X} = x \sim N \left(\beta_0 + \beta_1x, \sigma_{\varepsilon}^2 \right)$$

. . .

Su [**versi√≥n muestral**]{.hl-purple} ser√≠a simplemente

$$y_i | \boldsymbol{X} = x_i \sim N \left(\beta_0 + \beta_1x_i, \sigma_{\varepsilon}^2 \right), \quad i=1,\ldots,n$$

---

## Inferencia de los par√°metros

Las hip√≥tesis nos permiten decir (lo demostraremos m√°s adelante) que los [**par√°metros estimados siguen una distribuci√≥n (condicionada) normal**]{.hl-yellow} de [**media el par√°metro**]{.hl-purple}  a estimar y de [**varianza el conocido como standard error**]{.hl-purple}

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

. . .

Por las propiedades de la normal y el teorema central del l√≠mite 

$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)}| \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$

---

## Inferencia de los par√°metros


$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

Veremos m√°s adelante porqu√© pero de momento...estos son los $SE$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

¬øQu√© **propiedades** tienen estos estimadores?


---

## Inferencia de los par√°metros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


. . .

* [**Insesgados**]{.hl-yellow}: ambos son [**estimadores insesgados**]{.hl-green} ya que la esperanza de la estimaci√≥n es el valor a estimar. ${\rm E} \left[ \widehat{\beta}_j \right] = \beta_j$

. . .
 
* [**Precisi√≥n vs tama√±o muestral**]{.hl-yellow}: cuando $n \to \infty$, sus varianzas tienden a cero, es decir, $SE \left(\widehat{\beta}_j\right)^2 \to^{n \to \infty} 0$ ya que $n$ est√° dividiendo. Traducci√≥n: [**a m√°s datos, mayor precision**]{.hl-green} (menos varianza tendr√°n los estimadores si repetimos la toma de muestras)


---

## Inferencia de los par√°metros


$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$

. . .

* [**Precisi√≥n vs var residual**]{.hl-yellow}: cuanto m√°s grande sea la varianza del error $\sigma_{\varepsilon}^{2}$, $SE \left(\widehat{\beta}_j\right)^2$ crecer√° (es decir, [**m√°s ruido implicar√° m√°s imprecisi√≥n**]{.hl-red})

. . .

* [**Precisi√≥n vs varianza de X**]{.hl-yellow}: cuanto m√°s grande sea la varianza de la predictora $s_{x}^2$, $SE \left(\widehat{\beta}_j\right)^2$ decrecer√°, tal que $SE \left(\widehat{\beta}_j\right)^2 \to^{s_{x}^2 \to \infty} 0$. Dicho de otra forma: cu√°nta [**m√°s informaci√≥n (varianza) contenga nuestra tabla, mayor precisi√≥n**]{.hl-green}.

. . .

* [**Precisi√≥n vs media X**]{.hl-yellow}: solo afecta a la estimaci√≥n de $\beta_0$, cuya [**precisi√≥n decrece cuando la media aumenta**]{.hl-red}. Esto tiene sentido ya que cu√°nto m√°s grande en media sean los datos, menos fiable ser√° la predicci√≥n para $X=0$.

---

## Inferencia de los par√°metros

$$\widehat{\beta}_j | \left( X=x \right) \sim N \left(\beta_j, SE \left(\widehat{\beta}_j\right)^2 \right), \quad j=0,1$$

$$SE \left(\widehat{\beta}_0\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right], \quad SE \left(\widehat{\beta}_1\right)^2 = \frac{\sigma_{\varepsilon}^{2}}{n s_{x}^2}$$


Pero tenemos un problema: la [**varianza residual poblacional**]{.hl-yellow} (varianza poblaci√≥n del error) $\sigma_{\varepsilon}^{2}$ es [**desconocida**]{.hl-yellow}. Como suele pasar, la [**sustituiremos por un estimador (insesgado)**]{.hl-yellow} de la misma (ya demostaremos...), donde $p$ es el n√∫mero de variables (lo que R llama `std error` en la salida)

. . .

$$\widehat{\sigma_{\varepsilon}^{2}} = \frac{n}{n-p-1} s_{r}^{2} = \frac{1}{n-p-1} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2 =_{p=1} \frac{1}{n-2} \displaystyle \sum_{i=1}^{n} \widehat{\varepsilon}_{i}^2$$
---

## Inferencia de los par√°metros



$$\frac{\widehat{\beta}_j - \beta_j}{SE \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim N \left(0, 1 \right), \quad j=0,1$$


¬øQu√© suele suceder cuando en una normal **desconocemos la varianza real y la sustituimos por su estimador insesgado**?

. . .

Si [**sustituimos el error standard por su estimador**]{.hl-yellow} (sustituyendo la varianza residual por su estimador insesgado), obtenemos que sigue una **t-Student**

$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$
donde $\widehat{SE} \left(\widehat{\beta}_0\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n}\left[ 1+ \frac{\overline{x}^2}{s_{x}^2} \right]$ y $\widehat{SE} \left(\widehat{\beta}_1\right)^2 = \frac{\widehat{\sigma_{\varepsilon}^{2}}}{n s_{x}^2}$

---

## Inferencia de los par√°metros


```{r}
ajuste_lineal |> summary()
```

* `std error`: es lo que hemos denotado como $\widehat{SE} \left(\widehat{\beta}_j\right)^2$

. . .

* `t value`: es el estad√≠stico $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ que hemos dicho que sigue una t-Student (f√≠jate que hemos puesto $\beta_j = 0$)

## Inferencia de los par√°metros


```{r}
ajuste_lineal |> summary()
```

* `Pr(>|t|)`: representa un p-valor. ¬øDe qu√© contraste se te ocurre?

. . .

* `Residual standard error`: es el estimador insesgado de la varianza residual, que hemos denotado como $\widehat{\sigma_{\varepsilon}^{2}}$

---

## Inferencia de los par√°metros

:::: columns
::: {.column width="55%"}

$$\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE} \left(\widehat{\beta}_j\right)} | \left( X=x \right) \sim t_{n-p-1} , \quad j=0,1$$

:::

::: {.column width="45%"}

![](img/tablas-intervalos.png)

:::
::::

Si te acuerdas de inferencia, esto implica que los [**intervalos de confianza de nuestros par√°metros**]{.hl-yellow} son

$$IC \left(\alpha, \widehat{\beta}_j \right) = \left[\widehat{\beta}_j - t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right), \widehat{\beta}_j + t_{n-p-1; \alpha/2} \widehat{SE} \left(\widehat{\beta}_j\right) \right]$$
donde $t_{n-p-1; \alpha/2}$ es el cuantil $1-\alpha/2$ (deja a la izquierda una probabilidad de $1-\alpha/2$).

---


## Par√©ntesis: intervalos de confianza
confint(
---


## Inferencia de los par√°metros

Y si tenemos inferencia, tenemos contrastes: ¬øte acuerdas de los p-valores que devuelve la tabla para cada par√°metro?

. . .

Para cada par√°metro se realiza un [**contraste de significancia**]{.hl-yellow}: ¬øcu√°nta evidencia hay en mis datos para poder decir que el [**valor estimado de mi par√°metro es distinto de 0**]{.hl-yellow}?

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

. . .

El estad√≠stico usado es el [**t value**]{.hl-yellow} que devuelve `R` definido como $\frac{\widehat{\beta}_j - 0}{\widehat{SE} \left(\widehat{\beta}_j\right)}$ y que, [**BAJO EL SUPUESTO DE QUE LA HIP√ìTESIS NULA SEA CIERTA**]{.hl-purple}, sigue una t-Student $t_{n-p-1}$

---

## Inferencia de los par√°metros

$$H_0:~\widehat{\beta}_j = 0 \quad vs \quad H_1:~\widehat{\beta}_j \neq 0$$

Este contraste significancia en realidad **contesta a la siguiente pregunta**

¬øTiene mi variable $X_j$ un [**efecto lineal SIGNIFICATIVO**]{.hl-yellow} en mi variable $Y$?

. . .

&nbsp;

Esa pregunta puede ser respondida desde un punto de vista **frecuentista** haciendo uso de los [**p-valores**]{.hl-yellow}: si $p-value < \alpha$, entonces rechazamos la nula (rechazamos que la variable no tenga efecto lineal significativo, es decir, aceptamos que s√≠ lo tiene); en caso contrario no podemos rechazar (que **no es lo mismo que aceptarla**, solo que no hay evidencias suficientes en los datos para decir que haya un efecto significativo)

---

## Par√©ntesis: p-valor

---

## Inferencia de los par√°metros



```{r}
ajuste_lineal |> summary()
```

Si te fijas ambos p-valores est√°n por encima de $\alpha = 0.05$ (valor adoptado habitualmente) los que nos dice que [**no hay evidencias en los datos para afirmar que haya efectos lineales significativos**]{.hl-yellow}

. . .

¬øY si probamos a quitar $\beta_0$ (es decir, la respuesta est√° centrada)?

---

## Inferencia de los par√°metros

Para ello basta a√±adir un `-1` antes de las variables predictoras.

```{r}
lm(data = datos, formula = mass ~ -1 + height) |> summary()
```

F√≠jate que ahora, am√©n que [**hay un efecto lineal significativo de la altura**]{.hl-yellow} y de tener $R^2$ mayor, $\beta_1$ tiene una precisi√≥n mayor. En este caso [**solo pod√≠a quitar uno**]{.hl-yellow} (perder√≠amos $X$), pero veremos m√°s adelante c√≥mo decidir cu√°l quitar si tuvi√©semos varias variables.

---

siguiente clase 13/02: caso pr√°ctico

* An√°lisis exploratorio (num√©rico y visualizaci√≥n, corrplot)

* Estimaci√≥n

* Interpretaci√≥n coeficientes

* Diagnosis (herramientas en R)


---

siguiente clase 15/02:

* otro caso real

* evaluaci√≥n (m√©tricas, etc), significado R2

* intervalos predicci√≥n (se)

---

siguiente clase 20/02:


ANOVA

caso con todo

---

siguiente clase 22/02:


caso con todo a mano y en R


---

siguiente clase 27/02:

entrega

---

siguiente clase 29/02:

Regresi√≥n lineal multivariante

---

# El mundo Github

[**Trabajar ordenados, publicar resultados, replicabilidad de lo realizado**]{style="color:#444442;"}

---

## ¬øQu√© es Github?

[**GitHub**]{.hl-yellow} es la plataforma colaborativa m√°s conocida basada en el [**sistema de control de versiones Git**]{.hl-yellow}

. . .

-   [**¬øQu√© es Git?**]{.hl-purple} Git es un sistema de [**control de versiones**]{.hl-yellow}: una especie de [**Dropbox**]{.hl-yellow} para facilitar la [**programaci√≥n colaborativa**]{.hl-yellow} entre un grupo de personas, permitiendo llevar la [**trazabilidad de los cambios**]{.hl-yellow} realizados.

. . .

-   [**¬øQu√© es Github?**]{.hl-purple} Nuestra [**plataforma/interfaz**]{.hl-yellow} para ejecutar el control de versiones: nos servir√° no solo para trabajar colaborativamente sino para [**hacer transparente**]{.hl-yellow} el proceso de construcci√≥n de nuestros proyectos de c√≥digo.

. . .

::: callout-important
## Importante

Desde el 4 de junio de 2018 Github es de Microsoft (ergo el c√≥digo que subas tambi√©n)
:::

---

## Visi√≥n general

Tras hacernos una cuenta en Github, [**arriba a la derecha**]{.hl-purple} tendremos un c√≠rculo, y haciendo click en [**Your Profile**]{.hl-purple}, veremos algo similar a esto

::: columns
::: {.column width="55%"}
![](img/github_1.png)
:::

::: {.column width="45%"}
-   [**Edit profile**]{.hl-purple}: nos permite a√±adir una [**descripci√≥n y foto de perfil**]{.hl-yellow}.

-   [**Overview**]{.hl-purple}: en ese panel de cuadrados se [**visualizar√° nuestra actividad**]{.hl-yellow} a lo largo del tiempo.

-   [**Repositories**]{.hl-purple}: el c√≥dugo ser√° subido a [**repositorios**]{.hl-yellow}, el equivalente a nuestras carpetas compartidas en Dropbox.
:::
:::

---

## Primer uso: consumidor

Antes de aprender como crear repositorios, Github tambi√©n nos servir√° para

-   [**Acceder a c√≥digo**]{.hl-purple} ajeno
-   [**Proponer mejoras**]{.hl-purple} a otros usuarios, e incluso proponer [**correcciones de error que detectemos**]{.hl-yellow} de software que usemos

. . .

-   [**Instalar paquetes de R**]{.hl-purple}. En muchas ocasiones los desarrolladores de paquetes suben las actualizaciones a CRAN cada cierto tiempo, y en otras el software no es suficientemente ¬´amplio¬ª para poder ser subido como paquete.

El c√≥digo de paquetes que no tengamos subido en CRAN podremos [**instalarlo como c√≥digo desde Github**]{.hl-yellow}

---

## Instalar desde Github

Por ejemplo, vamos a instalar un paquete llamado [`{peRReo}`](https://github.com/jbgb13/peRReo), cuya √∫nica funci√≥n es darnos [**paletas de colores**]{.hl-yellow} basadas en portadas de [**√°lbumes de m√∫sica urbana**]{.hl-yellow}

::: columns
::: {.column width="50%"}
![](img/perrreo.png)
:::

::: {.column width="50%"}
![](img/rosalia.png){width="360"}
:::
:::

. . .

Para ello antes tendremos que instalar un [**conjunto de paquetes para desarrolladores**]{.hl-yellow} llamado `{devtools}`, que nos permitir√° la instalaci√≥n desde Github

```{r}
#| eval: false
install.packages("devtools")
```

---

## Instalar desde Github

Las [**instrucciones de instalaci√≥n**]{.hl-yellow} suelen venir detalladas en la portada del repositorio

::: columns
::: {.column width="50%"}
![](img/install_perreo.png)
:::

::: {.column width="50%"}
![](img/rosalia.png){width="310"}
:::
:::

. . .

En la mayor√≠a de casos bastar√° con la funci√≥n `install_github()` (del paquete que acabamos de instalar), pas√°ndole como argumento la [**ruta del repositorio**]{.hl-yellow} (sin "github.com/").

```{r}
#| eval: false
devtools::install_github("jbgb13/peRReo")
```

Ya puedes perrear con ggplot ;)

---

## Descargar desde Github

La mayor√≠a de veces lo que subamos no ser√° un paquete de R como tal sino que [**subiremos un c√≥digo m√°s o menos organizado**]{.hl-yellow} y comentado. En ese caso podremos [**descargar el repo entero**]{.hl-yellow} haciendo click [**Code**]{.hl-green} y luego Download ZIP.

Por ejemplo, vamos a descargarnos los scripts de dataviz que han subido desde el [Centre d'Estudis d'Opini√≥](https://github.com/ceopinio/bop-grafics)

![](img/ceo_github.png)

---

## Ideal

![](img/abogados_simpson.jpeg){width="600"}

[**¬øLo ideal en caso de RTVE?**]{.hl-purple} Tener dos tipos de repositorios

-   Una [**colecci√≥n de repositorios p√∫blicos (producci√≥n)**]{.hl-yellow} donde hacer transparente el c√≥digo y los datos ([**ya validados**]{.hl-purple}), coordinado por un n¬∫ reducido de personas.

-   Una [**colecci√≥n de repositorios privados (desarrollo)**]{.hl-yellow} donde est√© todo el equipo colaborando y donde se haga el [**trabajo del d√≠a**]{.hl-purple}, con trazabilidad interna.

---

## Nuestro primer repositorio

Vamos a [**crear nuestro primero repositorio**]{.hl-yellow} que servir√° adem√°s como [**carta de presentaci√≥n**]{.hl-yellow} de nuestro perfil en Github.

1.  [**Repositories**]{.hl-purple}: hacemos click en las pesta√±a de Repositories.

2.  [**New**]{.hl-purple}: hacemos click en el [**bot√≥n verde New**]{.hl-green} para crear un nuevo repositorio

![](img/new_repo.png)

---

## Nuestro primer repositorio

-   [**Repository name**]{.hl-purple}: el [**nombre del repositorio**]{.hl-yellow}. En este caso vamos a crear un repositorio muy concreto: el nombre debe [**coincidir exactamente con tu nombre de usuario**]{.hl-yellow}

-   [**Description**]{.hl-purple}: descripci√≥n de tu repositorio. En este caso ser√° un repo de presentaci√≥n.

![](img/repo_init_1.png)

---

## Nuestro primer repositorio

-   [**Public vs private**]{.hl-purple}: con cada repositorio tendremos la opci√≥n de hacer el repositorio

    -   [**p√∫blico**]{.hl-purple}: todos los usuarios podr√°n ver el c√≥digo as√≠ c√≥mo la trazabilidad de su desarrollo (qu√© se a√±ade y cu√°ndo). Es para m√≠ la opci√≥n m√°s recomendable cuando quieres darle [**visibilidad y transparencia**]{.hl-yellow} a tu trabajo
    -   [**privado**]{.hl-purple}: solo tendr√°n acceso al repositorio aquellos usuarios a los que se lo permitas. No se podr√° visualizar ni instalar nada de √©l fuera de Github.

![](img/repo_init_2.png)

En este caso concreto, dado que ser√° un repositorio de presentaci√≥n, lo [**haremos p√∫blico**]{.hl-yellow}.

---

## Nuestro primer repositorio

-   [**Add a README file**]{.hl-purple}: un README file ser√° el archivo donde incluiremos las [**instrucciones y detalles de uso**]{.hl-yellow} a los dem√°s (en el caso de `{peRReo}` era el archivo que conten√≠a los detalles de instalaci√≥n)

![](img/repo_init_3.png)

De momento [**ignoraremos los dem√°s campos**]{.hl-red} para este primer repositorio.

---

## Nuestro primer repositorio

![](img/repo_init_1.png)

Por defecto Github asume que este repositorio, con el [**mismo nombre que nuestro usuario**]{.hl-yellow} ser√° el repositorio que querremos que se presente de inicio cuando alguien entra en nuestro perfil, y ser√° el repositorio donde \[**incluir en el README.md**\] una presentaci√≥n de nosotros y un √≠ndice de tu trabajo (si quieres).

---

## Nuestro primer repositorio

![](img/profile_github_md.png)

F√≠jate que ahora en nuestra [**portada tenemos dicho README.md**]{.hl-yellow} que podemos personalizar a nuestro gusto haciendo uso de [**html y markdown**]{.hl-yellow}.

Aqu√≠ puedes ver [**algunos ejemplos de README.MD**](https://github.com/matiassingers/awesome-readme)

---

## Repo de c√≥digo

Una vez que tenemos nuestro README de presentaci√≥n (recuerda que puedes [**personalizar a tu gusto con html y markdown**]{.hl-yellow}) vamos a crear un [**repositorio de c√≥digo**]{.hl-yellow}.

. . .

Si ya era importante [**trabajar con proyectos**]{.hl-yellow} en `RStudio`, cuando lo combinamos con Github es a√∫n m√°s crucial que creemos un proyecto antes de subir el c√≥digo, as√≠ que vamos a crear uno de prueba que se llame `repo-github-1`.

. . .

En dicho proyecto vamos a [**crear un script**]{.hl-yellow} (en mi caso llamado **codigo.R**) en el que deber√°s hacer los siguientes pasos:

---

## Repo de c√≥digo

1.  [**Carga**]{.hl-yellow} directamente desde la p√°gina del [ISCIII](https://cnecovid.isciii.es/covid19/resources) el archivo llamado `casos_hosp_uci_def_sexo_edad_provres.csv`

```{r}
#| eval: false
#| code-fold: true
# Carga de datos desde ISCIII
datos_covid <- read_csv(file = "https://cnecovid.isciii.es/covid19/resources/casos_hosp_uci_def_sexo_edad_provres.csv")
```

. . .

2.  [**Filtra**]{.hl-yellow} datos de Madrid (`"M"`), de 2020 y con sexo conocido (hombre/mujer). Tras ello qu√©date con las columnas `fecha`, `sexo`, `grupo_edad`, `num_casos` (ese orden). Por √∫ltimo obt√©n la suma de casos diarios por fecha y sexo.

```{r}
#| eval: false
#| code-fold: true
# Depuraci√≥n
datos_madrid <-
  datos_covid |>
  # Filtrado por Madrid y fecha
  filter(provincia_iso == "M" & fecha <= "2020-12-31" & sexo != "NC") |> 
  # Selecci√≥n de columnas
  select(provincia_iso:fecha, num_casos) |> 
  # Resumen de casos diarios por fecha y sexo
  summarise(num_casos = sum(num_casos), .by = c(fecha, sexo))
  
```

---

## Repo de c√≥digo

3.  [**Exporta el dataset a un csv**]{.hl-yellow} en una carpeta que se llame `exportado`

```{r}
#| eval: false
#| code-fold: true
# Exportamos datos
write_csv(datos_madrid, file = "./exportado/datos_madrid.csv")
```

. . .

::: columns
::: {.column width="40%"}
4.  Crea una [**gr√°fica de l√≠neas**]{.hl-yellow} que tenga en el eje x fecha, en el eje y casos, con una curva por sexo (gr√°ficas separadas).

```{r}
#| eval: false
#| code-fold: true
# Gr√°fica
ggplot(datos_madrid) +
  geom_line(aes(x = fecha, y = num_casos, color = sexo),
            alpha = 0.6, linewidth = 0.7) +
  scale_color_manual(values = c("#85519D", "#278862")) +
  facet_wrap(~sexo) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
:::

::: {.column width="60%"}
![](./img/ggplot.png){width="380"}
:::
:::

. . .

5.  Tras ello [**exportamos la gr√°fica**]{.hl-yellow}

```{r}
#| eval: false
ggsave(filename = "./exportado/ggplot.png", plot = last_plot(),
       bg = "white", width = 12, height = 8)
```

---

## Repo de c√≥digo

¬øC√≥mo [**subimos el proyecto**]{.hl-yellow}? Vamos de nuevo a [**crear un proyecto de cero**]{.hl-yellow}. Antes no hemos hablado de dos campos importantes:

::: columns
::: {.column width="50%"}
![](./img/repo-1.png){width="460"}
:::

::: {.column width="50%"}
-   `Add .gitignore` nos permitir√° [**seleccionar el lenguaje**]{.hl-yellow} en el que estar√° nuestro proyecto para que Github lo entienda al sincronizar (y no actualice cosas que no deba).

-   `Choose a license` nos permitir√° [**seleccionar la licencia**]{.hl-yellow} que determinar√° las condiciones en las que otros podr√°n reusar tu c√≥digo.
:::
:::

---

## Repo de c√≥digo

::: columns
::: {.column width="50%"}
![](./img/repo-1-1.png)
:::

::: {.column width="50%"}
Si te fijas traer crearlo tenemos solo 3 archivos: el de licencia, el .gitignore y el readme.md (donde deber√≠amos escribir una gu√≠a de uso de lo que hayamos subido)
:::
:::

Para subir los archivos vamos a clickar en [**Add file \< Upload File**]{.hl-purple} y [**arrastraremos TODOS los archivos**]{.hl-yellow} de la carpeta de nuestro proyecto.

---

## Repo de c√≥digo

Tras la subida de archivos tendremos un cuadro llamado [**Commit changes**]{.hl-purple}

![](./img/commit-1.png)

Un [**commit**]{.hl-purple} es una [**modificaci√≥n del repositorio**]{.hl-yellow} con algo que se a√±ade/elimine/modifique, y dicho cuadro es recomendable usarlo para [**resumir en qu√© consiste la modificaci√≥n**]{.hl-yellow}, de manera que quede trazado el cambio.

---

## Repo de c√≥digo

Haciendo click en el reloj donde indica el [**n√∫mero de commits**]{.hl-yellow} accedemos al [**hist√≥rico de commits (cambios)**]{.hl-yellow} con hora, d√≠a, autor, comentarios, etc.

![](./img/commit-reloj.png)

---

## Repo de c√≥digo

Vamos a realizar un [**cambio en nuestro c√≥digo**]{.hl-yellow}: en tu c√≥digo local (local --\> tu ordenador), en lugar de filtrar por Madrid haz el [**filtro por Barcelona**]{.hl-yellow}, guarda el c√≥digo y sube en el repositorio el nuevo archivo (con el mismo nombre, Github har√° la sobrescritura)

```{r}
#| eval: false
#| code-line-numbers: "3"
datos_bcn <-
  datos_covid |>
  filter(provincia_iso == "B" & fecha <= "2020-12-31" & sexo != "NC") |> 
  select(fecha, sexo, grupo_edad, num_casos) |> 
  summarise(num_casos = sum(num_casos), .by = c(fecha, sexo))
```

---

## Consulta de commits

![](./img/barcelona_covid.png){width="550"}

::: columns
::: {.column width="40%"}
Si ahora [**consultamos el commit**]{.hl-yellow}, al lado hay un n√∫mero que lo identifica, y clickando en √©l nos resume los cambios: no solo [**almacena todas las versiones pasadas**]{.hl-yellow} sino que adem√°s nos [**muestra las diferencias entre los archivos cambiados**]{.hl-yellow}
:::

::: {.column width="60%"}
![](./img/commit_cambio.png)
:::
:::

---

## Trazabilidad de cambios

::: columns
::: {.column width="72%"}
![](./img/commit_split.png) ![](./img/commit_unified.png)
:::

::: {.column width="28%"}
Tenemos [**dos modos de visualizaci√≥n**]{.hl-yellow} de los cambios: el modo split nos muestra el antiguo y el nuevo, con las inclusiones en verde y lo que ya no est√° en rojo; y el modo unified nos muestra todo en un mismo documento.
:::
:::

---

## Recuperaci√≥n de commits

![](./img/browse-repo.png)

Github nos permite incluso [**recuperar una versi√≥n del pasado**]{.hl-yellow} de nuestro repositorio, haciendo click en el tercer icono del commit.

---

## Recuperaci√≥n de commits

![](./img/branch-commit.png)

Si te fijas ahora al lado de `1 branch` tenemos un [**men√∫ desplegable**]{.hl-yellow} en el que antes pon√≠a `main` y ahora un n√∫mero identificador del commit. Ya hablaremos de la idea de [**rama (branch)**]{.hl-yellow}

---

## Repo con rmd/qmd

::: columns
::: {.column width="60%"}
Vamos a poner en pr√°ctica lo aprendido:

1.  Crea un nuevo repositorio en Github (llamado `repo-github-2`) donde habr√° alojado con proyecto de R.

2.  Crea un proyecto en `RStudio` que se llame (por ejemplo) `proyecto-qmd`

3.  Una vez dentro del proyecto en `RStudio` haz click en `File < New File < Quarto Document`
:::

::: {.column width="40%"}
![](./img/primer-qmd.png)
:::
:::

Deber√°s tener un documento similar a este: un [**quarto markdown (.qmd)**]{.hl-yellow}, un documento que nos permitir√° incluir [**markdown + c√≥digo**]{.hl-yellow} (puede ser `R` o puede ser `Observable`, `D3`, etc).

---

## Repo con rmd/qmd

Este formato es ideal para:

-   [**Trabajar en equipo**]{.hl-yellow} construyendo el borrador de una pieza.
-   Tomar [**apuntes o informes**]{.hl-yellow} para uno mismo.
-   [**Presentar**]{.hl-yellow} tu trabajo a tus compa√±eros.

::: columns
::: {.column width="50%"}
![](./img/prueba-qmd-html.png)
:::

::: {.column width="50%"}
Si te fijas ahora nuestro repositorio tiene un archivo con formato `.html`...es decir...

[**¬°Es una web!**]{.hl-yellow}
:::
:::

---

## Github pages

¬øC√≥mo [**convertir nuestro repositorio en una web**]{.hl-yellow}?

![](./img/github-pages.png)

1.  Haz click en `Settings`
2.  Ve al apartado `Pages`
3.  En el subapartado `branch` selecciona la √∫nica rama que tenemos ahora (`main`)
4.  Selecciona la carpeta donde tengas el `.html` (en web complejas estar√° como en cualquier web en `docs`, en algo simple estar√° en la ruta raiz del repositorio)
5.  Haz click en `Save`

---

## Github pages

Si te fijas en la [**parte superior del repositorio**]{.hl-yellow} ahora tenemos un icono naranja, que nos indica que la [**web est√° en proceso de ser desplegada (deploy)**]{.hl-orange}

![](./img/github-naranja.png)

---

## Github pages

Pasados unos segundos (dependiendo del tama√±o de la web y tu conexi√≥n a internet) ese [**icono pasar√° a ser un check verde**]{.hl-green}: habemus web

![](./img/github-verde.png)

El [**link de la web por defecto**]{.hl-yellow} ser√° `{nombre_usuario}.github.io/{nombre_repo}`

---

## Github pages

![](./img/github-pages-deploy.png)

¬°Un momento! Ahora mismo nuestra web [**no nos est√° mostrando nuestro .qmd**]{.hl-red}, sino por defecto el [**README.md**]{.hl-yellow}.

. . .

Para que Github entienda que queremos visualizar ese `.html` que hemos generado a partir del `.qmd` vamos en nuestro proyecto local a [**borrar**]{.hl-yellow} todo lo que no sea nuestro archivo `.Rproj` y nuestro archivo `.qmd`, y vamos a [**cambiar el nombre**]{.hl-yellow} a este √∫ltimo llam√°ndolo `index.qmd`, y volvemos a compilarlo para [**generar un index.html**]{.hl-yellow}

---

## Github pages

Vamos a [**subir a Github ese nuevo proyecto**]{.hl-yellow} con el cambio de nombre (llamado `repo-github-3`) para ver luego las diferencias entre uno y otro

![](./img/repo-index-html.png)

---

## Github pages

Si [**repetimos el proceso para hacer una Page**]{.hl-yellow} y esperamos al tick verde...

::: columns
::: {.column width="50%"}
![](./img/index-html-qmd.png)
:::

::: {.column width="50%"}
Si a tu `.qmd` ya le llamas de inicio `index.qmd`, autom√°ticamente, al detectar Github un `index.html`, interpreta que ese [**archivo index.html**]{.hl-yellow} es el que define la web (y puedes personalizar a√±adiendo un archivo `css` de estilos)

[**Habemus web**]{.hl-green} simplemente clickando en Pages :)
:::
:::

---

## Repo con diapositivas

Vamos a crear el √∫ltimo repositorio que se llamar√° `repo-diapos`, y crear un proyecto en `RStudio` del mismo nombre (por ejemplo). Una vez creado le daremos a `File < New File < Quarto Presentation`.

::: columns
::: {.column width="45%"}
![](img/quarto-slides.png)
:::

::: {.column width="55%"}
La forma de escribir ser√° igual que un `.qmd` normal solo que ahora [**cada diapositiva la separaremos**]{.hl-yellow} con un `---` (usando archivos de estilos podemos personalizar lo que queramos)

Llama al archivo directamente `index.qmd`, s√∫belo a Github y con un click en Pages tienes una [**web con tus diapositivas**]{.hl-yellow}
:::
:::

---

## Uso de Gitkraken

La forma m√°s [**sencilla para trabajar de manera colaborativa**]{.hl-yellow} en Github, y tenerlo sincronizado con nuestro local, es hacer uso de [Gitkraken](https://www.gitkraken.com/download)

::: columns
::: {.column width="50%"}
![](img/gitkraken-repo.png)
:::

::: {.column width="50%"}
Una vez dentro clickamos en el icono de la carpeta (`Repo Management`) y si ya tenemos el repositorio en Github seleccionamos `Clone`, indicando donde queremos clonar (en nuestro local) y que [**repositorio de Github queremos clonar**]{.hl-yellow}.
:::
:::

---

## Uso de Gitkraken

::: columns
::: {.column width="50%"}
![](img/view-change.png)
:::

::: {.column width="50%"}
Una vez clonado, la idea es que cada [**cambio que hagamos en local nos aparecer√° en Gitkraken**]{.hl-yellow} como `View changes`.
:::
:::

---

## Uso de Gitkraken

Cuando tengas suficientes cambios como para [**actualizar el repositorio**]{.hl-yellow} (tampoco tiene sentido actualizar con cada edici√≥n), ver√°s algo similar a esto con todos los [**commits realizados**]{.hl-yellow}

::: columns
::: {.column width="50%"}
![](img/stage-all-changes.png)
:::

::: {.column width="50%"}
Podr√°s decidir cu√°les de los [**commits locales quieres incluir en remoto**]{.hl-yellow}, bien uno a uno o en `Stage all changes` (para todos)
:::
:::

---

## Uso de Gitkraken

Tras incluir los commits deber√°s incluir un [**t√≠tulo y descripci√≥n del commit**]{.hl-yellow}

![](img/titulo-commit.png)

---

## Uso de Gitkraken

Tras hacerlo ver√°s que ahora tenemos [**dos iconos separados en una especie de √°rbol**]{.hl-yellow} (¬øte acuerdas de la `branch` o rama?):

-   [**Ordenador**]{.hl-purple}: la versi√≥n del repositorio que tienes en tu [**ordenador**]{.hl-yellow}.

-   [**Logo**]{.hl-purple}: la versi√≥n del repositorio que tienes [**subida en remoto**]{.hl-yellow}

![](img/split-gitkraken.png)

---

## Uso de Gitkraken

Mientras eso suceda solo tendr√°s sincronizado tu ordenador con Gitkraken, pero no con Github. Para ello haremos [**click en Push**]{.hl-yellow} (con `Pull` podr√°s forzar a tener en local lo mismo que en remoto).

![](img/push-gitkraken.png)

---

## Branchs

Como hemos mencionado ya en varias ocasiones, hay un elefante en la habitaci√≥n que a√∫n no hemos mentado: las [**ramas o branchs**]{.hl-yellow} de un repositorio.

. . .

Imagina que est√°is trabajando varios en un proyecto y tene√≠s una versi√≥n que funciona pero que quer√©is [**modificar en paralelo a partir del estado actual**]{.hl-yellow} del repositorio.

. . .

Las [**ramas**]{.hl-yellow} nos permiten partir de una versi√≥n com√∫n del repositorio y hacer cambios que [**no afecten a los dem√°s**]{.hl-yellow}

---

## Branchs

Para [**crear una rama**]{.hl-yellow} a partir del estado actual de repositorio haremos click en `Branch` y le pondremos un nombre

![](img/branch-button.png)

Una vez creada ver√°s [**dos iconos**]{.hl-yellow} y un [**men√∫ desplegable**]{.hl-yellow} con las distintas ramas en las que quieres hacer el commit. Imagina que realizas un cambio pero [**no quieres a√±adirlo a la rama principal**]{.hl-yellow}: puedes hacer el [**commit en tu rama propia en LOCAL**]{.hl-yellow} (lo har√°s en la rama activa de tu men√∫ de branchs).

---

## Branchs

La primera vez te pedir√° que escribas la [**rama en REMOTO**]{.hl-yellow} con la quieres sincronizar tu rama en local. [**Consejo**]{.hl-green}: ponle el mismo nombre en remoto que en local.

![](img/name-branch.png)

---

## Branchs

F√≠jate que ahora tenemos el ordenador y el logo en el mismo sitio. Esto no significa que tengas ambas ramas en tu local, solo que [**Gitkraken tiene ambas sincronizadas**]{.hl-yellow}: clickando en cualquiera de ellas, tus archivos en tu ordenador cambiar√°n.

![](img/both-branchs.png)

---

## Pull request

Lo m√°s recomendable es que [**solo se incorpore de una rama secundaria**]{.hl-yellow} a la rama principal aquello que est√° [**validado por un/a coordinador/a**]{.hl-yellow} del repositorio, asegur√°ndose que todo funciona correctamente.

Cuando queramos incluirlo haremos [**click con bot√≥n derecho**]{.hl-yellow} en el icono de la rama secundaria y seleccionamos `Start a pull request to origin from...`

![](img/pull-request-menu.png)

. . .

Una [**pull request**]{.hl-yellow} ser√° una [**petici√≥n al responsable de la rama principal**]{.hl-yellow} para incluir los cambios

---

## Pull request

::: columns
::: {.column width="60%"}
![](img/create-pull-request.png)
:::

::: {.column width="40%"}
En el cuadro que no se abre deberemos escribir:

-   La [**rama**]{.hl-yellow} a la que hacer el `merge` (normalmente la `main`)
-   T√≠tulo y resumen de los cambios
-   Puedes incluso asignar un [**revisor**]{.hl-yellow} entre los colaboradores del repo.
-   Puedes asignar [**etiquetas**]{.hl-yellow}
:::
:::

---

## Pull request

Mientras no se acepte aparecer√° un [**icono de rama**]{.hl-yellow} y un +1 en Pull Requests

![](img/pending-branch.png){width="500"}

. . .

Si somos al mantenedor del repositorio, haciendo click en el men√∫ nos saldr√°n las ramas que nos quieren hacer hacer `merge`

![](img/menu-pr.png){width="500"}

---

## Pull request

Al hacer click se abrir√° un [**cuadro de Pull Request**]{.hl-yellow} para decidir si

-   [**Revisar**]{.hl-yellow} los cambios
-   [**Aprobar**]{.hl-yellow} el `merge`
-   [**A√±adir comentarios**]{.hl-yellow} al que ha solicitado el `merge` por si queremos solicitar alg√∫n cambio [**antes de ser aprobado**]{.hl-yellow}

## ![](img/menu-pr-2.png)

## Pull request

Tras revisar todo y aprobarlo clickaremos en `Confirm merge`, y tras ello podremos decidir si esa rama que era paralela a la principal la queremos [**eliminar**]{.hl-yellow} o dejar visible a todos (consejo: dejar visible para tene [**trazabilidad**]{.hl-yellow} del proyecto de trabajo)

::: columns
::: {.column width="50%"}
![](img/merge-branch.png)
:::

::: {.column width="50%"}
![](img/delete-branch.png)
:::
:::

